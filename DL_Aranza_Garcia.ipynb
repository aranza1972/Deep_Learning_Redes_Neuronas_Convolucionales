{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "DL_Aranza_Garcia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wis86uqVOpkz"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VNCgkwAtqyZ"
      },
      "source": [
        "## Redes de Neuronas Convolucionales\n",
        "Resolver mediante redes de neuronas convoluciones el problema de clasificación SIGNS en el cual se pretende predecir el número que se muestra mediante signos con la mano. \n",
        "\n",
        "\n",
        "\n",
        "Image(filename=\"/content/signs.png\")\n",
        "\n",
        "\n",
        "Implementa:\n",
        "* Un modelo desarrollado desde cero.\n",
        "* Un modelo que haga uso de transfer learning. Se puede utilizar los modelos de keras u otros modelos/pesos que se obtengan de internet. En ese caso, referenciar.\n",
        "\n",
        "Identificar el modelo óptimo. Será necesario presentar pruebas que verifiquen la adecuación del modelo elegido.\n",
        "\n",
        "Explicar de forma detallada cada uno de los pasos realizados para el análisis, preparación y resolución del problema así como el motivo de su aplicación.\n",
        "\n",
        "#### IMPORTANTE: Modifica el nombre del fichero con tu nombre y apellido"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Det-GnMqvRSC"
      },
      "source": [
        "# Solo en google colab\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LN2abb-OvLD"
      },
      "source": [
        "###1. Importamos las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ-0KTiVvWub"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb6fpIrq7iwy"
      },
      "source": [
        "###2. Cargamos los datos, escalamos las imagenes y realizamos la separación de conjunto de las imagenes para realizar el entrenamiento así como conjunto de imajenes para test y validación: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XZWf7QRtqyb"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset():\n",
        "    train_dataset = h5py.File('train_signs.h5', \"r\")\n",
        "    X_train = np.array(train_dataset[\"train_set_x\"][:])/255\n",
        "    y_train = np.array(train_dataset[\"train_set_y\"][:])\n",
        "\n",
        "    test_dataset = h5py.File('test_signs.h5', \"r\")\n",
        "    X_test = np.array(test_dataset[\"test_set_x\"][:])/255\n",
        "    y_test = np.array(test_dataset[\"test_set_y\"][:])\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH1u7e10tqyb"
      },
      "source": [
        "X_train, y_train, X_test, y_test, classes = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUDkbJTY_iCa",
        "outputId": "2a8e885c-e14c-4f43-9865-f1fc1d142f47"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 2, ..., 2, 4, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxQJCm8k8MRY"
      },
      "source": [
        "###3. Mostramos una imagen aleatoría así como el tamaño de cada una da las variables creadas para el analisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "JG0rWcyHtqyb",
        "outputId": "38e09f3d-696f-48a3-92e2-a42aeab43c74"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "_ = plt.imshow(X_train[np.random.randint(X_train.shape[0])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4haIxXOtqyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e32578-2771-4997-f18e-5143d93e10b4"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1080, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgYqVSsaoUm_",
        "outputId": "58745647-05bb-4296-96d4-dfab3c17e6a6"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1080,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQtIdDMyocqT",
        "outputId": "67446fe4-466e-4772-c9a3-aad1766d69af"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHgkHp73odco",
        "outputId": "45e7cd46-d2d0-42ce-f7a8-7eb9945d094b"
      },
      "source": [
        "classes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm624--1odNT",
        "outputId": "cd55f854-ae12-41ec-fa20-f889b1c737ea"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW_QbVmk8p6Q"
      },
      "source": [
        "###4. Realizamos OneHotEnconding de las variables de salida para posteriormente en los modelos creados poder utilizar la función de perdidas categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flw2a8KhEuo6"
      },
      "source": [
        "# OneHotEncoding para AUC\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "le = LabelBinarizer().fit(classes)\n",
        "y_train= le.transform(y_train)\n",
        "y_test= le.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYXENGGqWyPE",
        "outputId": "20258a05-0a9f-4957-9a43-66e24400ec5b"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUcSLQ5L3RPd"
      },
      "source": [
        "###5. Creamos los filtros primero de 7x7 despues de 3x3 y aplicamos a las imagenes para la detección de bordes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_6EibX1pCe3",
        "outputId": "41b24ae4-97d3-4560-df71-a69210069c05"
      },
      "source": [
        "batch_size, height, width, channels = X_train.shape\n",
        "# Creamos un filtro vertical y otro horizonatl\n",
        "filters_line = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32) \n",
        "filters_line[:, 3, :, 0] = 1 # vertical line\n",
        "filters_line[3, :, :, 1] = 1 # horizontal line\n",
        "print(\"Filtro (linea) vertical\\n\", filters_line[:,:,1,0])\n",
        "print(\"Filtro (linea) Horizontal\\n\", filters_line[:,:,1,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtro (linea) vertical\n",
            " [[0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]]\n",
            "Filtro (linea) Horizontal\n",
            " [[0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjofx9Si3bvt"
      },
      "source": [
        "Crear un filtro vertical y otro horizontal de dimensión 3x3 como los vistos en clase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyYy3fOb2Dgu",
        "outputId": "a8513f05-2cdb-4bfa-97d3-3e2f2cf92aa8"
      },
      "source": [
        "filters = np.zeros(shape=(3, 3, channels, 2), dtype=np.float32) \n",
        "filters[0, :, :, 0] = 1 # horizontal filter\n",
        "filters[2, :, :, 0] = -1 # horizontal filter\n",
        "\n",
        "filters[:, 0, :, 1] = 1 # vertical filter\n",
        "filters[:, 2, :, 1] = -1 # vertical filter\n",
        "print(\"Filtro horizontal\\n\", filters[:,:,1,0])\n",
        "print(\"Filtro vertical\\n\", filters[:,:,1,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtro horizontal\n",
            " [[ 1.  1.  1.]\n",
            " [ 0.  0.  0.]\n",
            " [-1. -1. -1.]]\n",
            "Filtro vertical\n",
            " [[ 1.  0. -1.]\n",
            " [ 1.  0. -1.]\n",
            " [ 1.  0. -1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxXxA-6x3miT"
      },
      "source": [
        "#### Realizamos una convolución con los filtros\n",
        "Utilizar `tf.nn.conv2d` para aplicar los filtros creados a las imagenes. \n",
        "El tamaño de la imagen resultante tiene que ser el mismo que el de las imagenes de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDohzNeL2SvQ",
        "outputId": "5d79574b-0746-4c75-d49c-b027a9ed2ba3"
      },
      "source": [
        "print(X_train.shape)\n",
        "outputs = tf.nn.conv2d(X_train, filters, strides=1, padding=\"SAME\")\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1080, 64, 64, 3)\n",
            "(1080, 64, 64, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVLX5CgW3r9R"
      },
      "source": [
        "¿Cuántos canales tienen las imagenes antes y después de aplicar los filtros?\n",
        "\n",
        "SOLUCION: 3 canales de entrada, 2 de salida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s5N7o_t36hG"
      },
      "source": [
        "#### Filtro horizontal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "aRA06zK04N07",
        "outputId": "18132a4b-8f51-4fa1-917b-606b37760c49"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "_ = plt.imshow(outputs[0, :, :, 0], cmap=\"gray\") #"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1bpsKaw4dab"
      },
      "source": [
        "#### Filtro vertical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "iY212YBt4jEa",
        "outputId": "cff0d252-fff5-42da-8565-eed942be6c82"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "_ = plt.imshow(outputs[1, :, :, 1], cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkkTkF6t5XLP"
      },
      "source": [
        "###6. Pooling\n",
        "\n",
        "#### Max Pooling con $f=3, s=3$ para altura y anchura\n",
        "El resultado es una imagen de menor resolución con los rasgos más importantes. Se puede ver que los bordes están más diferenciados.\n",
        "\n",
        "Utilizar `tf.nn.max_pool` para aplicar max pooling espacial (anchura y altura) con un padding válido\n",
        "\n",
        "Kernel Size y strides esperan una tupla con tamaño/stride  `( entre instancias, altura, anchura, canales)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC8VYiWD5ZVT"
      },
      "source": [
        "#Sobre la imagen original\n",
        "#output = tf.nn.max_pool(X_train, ksize=(1, 3, 3, 1), strides=(1, 3, 3, 1), padding=\"VALID\")\n",
        "#Sobre el resultado de la convolución\n",
        "output = tf.nn.max_pool(outputs, ksize=(1, 3, 3, 1), strides=(1, 3, 3, 1), padding=\"VALID\")\n",
        "#Repetir operación para resaltar mas los resultados\n",
        "#output = tf.nn.max_pool(output, ksize=(1, 3, 3, 1), strides=(1, 3, 3, 1), padding=\"VALID\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "2QmRXnqx5yfv",
        "outputId": "965d2e88-4664-46f5-a538-e90804c1e8bb"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "_ = plt.imshow(output[0, :, :, 1], cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVjFQveH6eiS"
      },
      "source": [
        "###7. Max Pooling con $f=2, s=2$ para los canales\n",
        "El resultado es una imagen en escala de grises. Al utilizar un pool de tipo máximo, de nuevo los rasgos más importantes son realzados.\n",
        "\n",
        "Si lo aplicamos sobre los resultados de los filtros verticales y horizontales, tendremos una combinación de los bordes resaltados tanto verticales como horizontales.\n",
        "\n",
        "Utilizar tf.nn.max_pool para aplicar max pooling en profundidad (canales) con un padding válido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-7xCcRF6mWP"
      },
      "source": [
        "output = tf.nn.max_pool(outputs, ksize=(1, 1, 1, 2), strides=(1, 1, 1, 2), padding=\"VALID\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "95ylmqE06xBV",
        "outputId": "f8330294-492a-410e-9f7d-bae65586a878"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "_ = plt.imshow(output[0, :, :, 0], cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwcRN3I4iTag"
      },
      "source": [
        "# Redes de Neuronas Convolucionales realizadas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5rYOwnV7Ahn"
      },
      "source": [
        "# 1.Mi primera CNN: Fashion Mnist\n",
        "Cambiamos el entorno a GPU.\n",
        "\n",
        "Partiendo de la CNN realizada en clase realizo la siguiente Red:\n",
        "\n",
        "### Creo una CNN utilizando padding de tipo samme con:\n",
        "* 1 capa con 64 filtros de dimensión 7 y max pool de dimensión 2 **(Cambio la entrada)**\n",
        "* 1 capa con convolucion de 128 filtros de dimensión 3 y max pool de dimensión 2\n",
        "* 1 capa con convolucion de 256 filtros de dimensión 3 y max pool de dimensión 2\n",
        "* 1 capa de tipo Dense de dimensión 128\n",
        "* Dropout a 0.5\n",
        "* 1 capa de tipo Dense de dimensión 64\n",
        "* **Dropout a 0.5 (Esta la elimino)**\n",
        "* 1 capa de tipo de Dense de dimensión 6 **(Cambio la salida)**.\n",
        "\n",
        "También al haber realizado el OneHOtEncoding de las variables de salida, cambio la función de pedidas **loss=categorical_crossentropy**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Iq8Xbz47DPr",
        "outputId": "61e1161f-2635-4ef3-8c84-bd5484b5e7c2"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=7, activation='relu', input_shape=[64, 64, 3]),\n",
        "\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "    keras.layers.Conv2D(128,3,activation=\"relu\", padding=\"same\"),\n",
        "\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "    keras.layers.Conv2D(256,3,activation=\"relu\", padding=\"same\"),\n",
        "\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "\n",
        "    keras.layers.Dense(units=6, activation='softmax'),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='nadam',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy']\n",
        "\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train,y_train, validation_split = 0.2, epochs=25)\n",
        "\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_58 (Conv2D)           (None, 58, 58, 64)        9472      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 29, 29, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 29, 29, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 14, 14, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               1605760   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 1,992,902\n",
            "Trainable params: 1,992,902\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "27/27 [==============================] - 2s 23ms/step - loss: 1.8147 - accuracy: 0.1601 - val_loss: 1.7911 - val_accuracy: 0.1528\n",
            "Epoch 2/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7961 - accuracy: 0.1733 - val_loss: 1.7925 - val_accuracy: 0.1528\n",
            "Epoch 3/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7928 - accuracy: 0.1598 - val_loss: 1.7879 - val_accuracy: 0.1759\n",
            "Epoch 4/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7917 - accuracy: 0.1527 - val_loss: 1.7806 - val_accuracy: 0.1296\n",
            "Epoch 5/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7883 - accuracy: 0.2156 - val_loss: 1.7392 - val_accuracy: 0.2639\n",
            "Epoch 6/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7445 - accuracy: 0.2585 - val_loss: 1.5188 - val_accuracy: 0.3287\n",
            "Epoch 7/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.4946 - accuracy: 0.3997 - val_loss: 1.3074 - val_accuracy: 0.5093\n",
            "Epoch 8/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.3697 - accuracy: 0.4359 - val_loss: 1.0998 - val_accuracy: 0.5694\n",
            "Epoch 9/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.0815 - accuracy: 0.5707 - val_loss: 0.9058 - val_accuracy: 0.6713\n",
            "Epoch 10/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.9197 - accuracy: 0.6216 - val_loss: 0.6918 - val_accuracy: 0.7269\n",
            "Epoch 11/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.8560 - accuracy: 0.6593 - val_loss: 0.6197 - val_accuracy: 0.7593\n",
            "Epoch 12/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.6329 - accuracy: 0.7500 - val_loss: 0.5693 - val_accuracy: 0.7870\n",
            "Epoch 13/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5721 - accuracy: 0.7692 - val_loss: 0.4953 - val_accuracy: 0.8241\n",
            "Epoch 14/25\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.4638 - accuracy: 0.8282 - val_loss: 0.4532 - val_accuracy: 0.8287\n",
            "Epoch 15/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3957 - accuracy: 0.8427 - val_loss: 0.4525 - val_accuracy: 0.8380\n",
            "Epoch 16/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3434 - accuracy: 0.8807 - val_loss: 0.3685 - val_accuracy: 0.8657\n",
            "Epoch 17/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2618 - accuracy: 0.9030 - val_loss: 0.4448 - val_accuracy: 0.8657\n",
            "Epoch 18/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2899 - accuracy: 0.9027 - val_loss: 0.4420 - val_accuracy: 0.8843\n",
            "Epoch 19/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2330 - accuracy: 0.9116 - val_loss: 0.3732 - val_accuracy: 0.8843\n",
            "Epoch 20/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.1788 - accuracy: 0.9325 - val_loss: 0.3846 - val_accuracy: 0.8796\n",
            "Epoch 21/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.1465 - accuracy: 0.9405 - val_loss: 0.3599 - val_accuracy: 0.8796\n",
            "Epoch 22/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.1637 - accuracy: 0.9378 - val_loss: 0.4880 - val_accuracy: 0.8611\n",
            "Epoch 23/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2124 - accuracy: 0.9147 - val_loss: 0.3928 - val_accuracy: 0.8704\n",
            "Epoch 24/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.1693 - accuracy: 0.9319 - val_loss: 0.3715 - val_accuracy: 0.9120\n",
            "Epoch 25/25\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.1279 - accuracy: 0.9511 - val_loss: 0.3790 - val_accuracy: 0.8935\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2722 - accuracy: 0.9417\n",
            "CNN básica:  [0.27220821380615234, 0.9416666626930237]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7sb1PLTkBFr"
      },
      "source": [
        "### Con mi primera CNN alcanzo un accuracy con datos de test de **94,17%** y veo que no ha habido sobreaprendizaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y480Nf-iG9m"
      },
      "source": [
        "###Tras hablar contigo te indiqué que pensaba que si la entrenaba más a lo mejor podría mejorar el resultado para ello tal como me indicaste voy ha hacerlo pero utilizado **EarlyStopping de Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIKSkIdvmo73",
        "outputId": "5891b8fd-c847-4bec-a278-be908af231b5"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=7, activation='relu', input_shape=[64, 64, 3]),\n",
        "\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "    keras.layers.Conv2D(128,3,activation=\"relu\", padding=\"same\"),\n",
        "\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "    keras.layers.Conv2D(256,3,activation=\"relu\", padding=\"same\"),\n",
        "\n",
        "    keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "\n",
        "    keras.layers.Dense(units=6, activation='softmax'),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='nadam',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy']\n",
        "\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train,y_train, validation_split = 0.2, epochs=50,callbacks=[callback],verbose=1)\n",
        "\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 58, 58, 64)        9472      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 29, 29, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 29, 29, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               1605760   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 1,992,902\n",
            "Trainable params: 1,992,902\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "27/27 [==============================] - 2s 44ms/step - loss: 1.9306 - accuracy: 0.1832 - val_loss: 1.7904 - val_accuracy: 0.2407\n",
            "Epoch 2/50\n",
            "27/27 [==============================] - 1s 33ms/step - loss: 1.7970 - accuracy: 0.1705 - val_loss: 1.8018 - val_accuracy: 0.1713\n",
            "Epoch 3/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7954 - accuracy: 0.2099 - val_loss: 1.7887 - val_accuracy: 0.2130\n",
            "Epoch 4/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7897 - accuracy: 0.2123 - val_loss: 1.7874 - val_accuracy: 0.2685\n",
            "Epoch 5/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.8031 - accuracy: 0.1557 - val_loss: 1.7897 - val_accuracy: 0.1713\n",
            "Epoch 6/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7868 - accuracy: 0.1940 - val_loss: 1.7889 - val_accuracy: 0.2037\n",
            "Epoch 7/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.8548 - accuracy: 0.2160 - val_loss: 1.7931 - val_accuracy: 0.1481\n",
            "Epoch 8/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7910 - accuracy: 0.1902 - val_loss: 1.7933 - val_accuracy: 0.1528\n",
            "Epoch 9/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7917 - accuracy: 0.1786 - val_loss: 1.7928 - val_accuracy: 0.1713\n",
            "Epoch 10/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7926 - accuracy: 0.1802 - val_loss: 1.7924 - val_accuracy: 0.1806\n",
            "Epoch 11/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7900 - accuracy: 0.2060 - val_loss: 1.7927 - val_accuracy: 0.1296\n",
            "Epoch 12/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7918 - accuracy: 0.1973 - val_loss: 1.7802 - val_accuracy: 0.1944\n",
            "Epoch 13/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.8137 - accuracy: 0.1915 - val_loss: 1.7908 - val_accuracy: 0.1296\n",
            "Epoch 14/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7898 - accuracy: 0.1823 - val_loss: 1.7824 - val_accuracy: 0.2269\n",
            "Epoch 15/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7803 - accuracy: 0.2042 - val_loss: 1.7573 - val_accuracy: 0.2407\n",
            "Epoch 16/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.7420 - accuracy: 0.2606 - val_loss: 1.6001 - val_accuracy: 0.3750\n",
            "Epoch 17/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.6671 - accuracy: 0.3152 - val_loss: 1.5494 - val_accuracy: 0.4815\n",
            "Epoch 18/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.4230 - accuracy: 0.4334 - val_loss: 1.2445 - val_accuracy: 0.5000\n",
            "Epoch 19/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.2402 - accuracy: 0.5363 - val_loss: 1.1385 - val_accuracy: 0.5648\n",
            "Epoch 20/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.0932 - accuracy: 0.5495 - val_loss: 0.9651 - val_accuracy: 0.6435\n",
            "Epoch 21/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.9824 - accuracy: 0.5816 - val_loss: 0.9109 - val_accuracy: 0.6343\n",
            "Epoch 22/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.9271 - accuracy: 0.6437 - val_loss: 0.8621 - val_accuracy: 0.6713\n",
            "Epoch 23/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.7399 - accuracy: 0.7182 - val_loss: 0.7343 - val_accuracy: 0.7639\n",
            "Epoch 24/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.6845 - accuracy: 0.7340 - val_loss: 0.6408 - val_accuracy: 0.8009\n",
            "Epoch 25/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.5819 - accuracy: 0.7797 - val_loss: 0.5971 - val_accuracy: 0.8380\n",
            "Epoch 26/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.5413 - accuracy: 0.7855 - val_loss: 0.5081 - val_accuracy: 0.8380\n",
            "Epoch 27/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.4989 - accuracy: 0.8060 - val_loss: 0.5989 - val_accuracy: 0.8148\n",
            "Epoch 28/50\n",
            "27/27 [==============================] - 1s 33ms/step - loss: 0.4807 - accuracy: 0.8263 - val_loss: 0.5808 - val_accuracy: 0.8287\n",
            "Epoch 29/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.4177 - accuracy: 0.8474 - val_loss: 0.5725 - val_accuracy: 0.7963\n",
            "Epoch 30/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.3612 - accuracy: 0.8633 - val_loss: 0.5876 - val_accuracy: 0.8565\n",
            "Epoch 31/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.3487 - accuracy: 0.8664 - val_loss: 0.5128 - val_accuracy: 0.8611\n",
            "Epoch 32/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.3219 - accuracy: 0.8899 - val_loss: 0.5505 - val_accuracy: 0.8750\n",
            "Epoch 33/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.2890 - accuracy: 0.8930 - val_loss: 0.4918 - val_accuracy: 0.8611\n",
            "Epoch 34/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.2738 - accuracy: 0.9077 - val_loss: 0.5647 - val_accuracy: 0.8565\n",
            "Epoch 35/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.2906 - accuracy: 0.8797 - val_loss: 0.4818 - val_accuracy: 0.8796\n",
            "Epoch 36/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.2217 - accuracy: 0.9233 - val_loss: 0.4107 - val_accuracy: 0.8889\n",
            "Epoch 37/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.1929 - accuracy: 0.9326 - val_loss: 0.4310 - val_accuracy: 0.8796\n",
            "Epoch 38/50\n",
            "27/27 [==============================] - 1s 31ms/step - loss: 0.2488 - accuracy: 0.9102 - val_loss: 0.4992 - val_accuracy: 0.8796\n",
            "Epoch 39/50\n",
            "27/27 [==============================] - 1s 33ms/step - loss: 0.1902 - accuracy: 0.9293 - val_loss: 0.5811 - val_accuracy: 0.8565\n",
            "Epoch 40/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.2514 - accuracy: 0.8995 - val_loss: 0.4278 - val_accuracy: 0.8935\n",
            "Epoch 41/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.2154 - accuracy: 0.9194 - val_loss: 0.4755 - val_accuracy: 0.8796\n",
            "Epoch 42/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.1972 - accuracy: 0.9155 - val_loss: 0.5241 - val_accuracy: 0.8889\n",
            "Epoch 43/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.1740 - accuracy: 0.9381 - val_loss: 0.4451 - val_accuracy: 0.8889\n",
            "Epoch 44/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.1089 - accuracy: 0.9596 - val_loss: 0.5358 - val_accuracy: 0.8704\n",
            "Epoch 45/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.1468 - accuracy: 0.9427 - val_loss: 0.4186 - val_accuracy: 0.9028\n",
            "Epoch 46/50\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 0.1449 - accuracy: 0.9407 - val_loss: 0.4810 - val_accuracy: 0.8981\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3237 - accuracy: 0.9417\n",
            "CNN básica:  [0.32372933626174927, 0.9416666626930237]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cod_HBJWpBWm"
      },
      "source": [
        "###He podido comprobar que aunque lo he entrenado 46 epochs (21 epochs más) no he mejorado el accuracy, mantengo el mismo resultado **94,17%**. Se ha detenido antes de llegar al epoch 50 para evitar sobreaprendizaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDe9VCH9CgN0"
      },
      "source": [
        "# 2. ResNet\n",
        "\n",
        "Parto de la CNN creada en clase y sólo cambio:\n",
        "  - input_shape: 64,64,3.\n",
        "  - salida:6.\n",
        "  - loss:categorical_crossentropy\n",
        "  \n",
        "Creamos el Residual Block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq4urYgDln6z"
      },
      "source": [
        "#SOLUCION\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            #Convolución: filters filtros de dimension 3, padding=samme y strides=strides sin bías\n",
        "            keras.layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False),\n",
        "            #Aplicar batch normalization\n",
        "            keras.layers.BatchNormalization(), \n",
        "            #Aplicamos función de activación\n",
        "            self.activation, \n",
        "            #Convolución: filters filtros de dimension 3, padding=samme y strides=1 sin bías\n",
        "            keras.layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False),\n",
        "            #Aplicar batch normalization\n",
        "            keras.layers.BatchNormalization()\n",
        "        ]\n",
        "        self.skip_layers = [] \n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                #Convolución: filters filtros de dimension 1, padding=samme y strides=strides sin bías\n",
        "                keras.layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=False), \n",
        "                #Aplicar batch normalization\n",
        "                keras.layers.BatchNormalization()\n",
        "            ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers: \n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otg_icp7Cw0k"
      },
      "source": [
        "Ahora creamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_kU4yFCuls"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "# Convolución 64 filtros de dimension 7, padding=samme y strides=2 sin bías\n",
        "# Preparar el modelo para trabajar con Fashion Mnist\n",
        "model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[64, 64, 3], padding=\"same\", use_bias=False))\n",
        "#Original para trabajar con imagenes de Image Net\n",
        "# model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3], padding=\"same\", use_bias=False))\n",
        "# Batch Normalization\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "#Aplicamos función de activación relu\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "# Max pool de dimension 3 con stride 2 y padding same\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")) \n",
        "prev_filters = 64\n",
        "# Añadimos 3 residual blocks de 64 filtros, 4 de 128, 6 de 265 y 3 de 512\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2 \n",
        "    model.add(ResidualBlock(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "# Aplicamos GlobalAvgPool2D (Maximo de cada canal)\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "# Convertimos las entradas en un vector para poder pasarlo a una capa Dense\n",
        "model.add(keras.layers.Flatten())\n",
        "# Creamos la capa de salida para clasificación con 10 posibles clases mutuamente excluyentes\n",
        "model.add(keras.layers.Dense(6, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTlrENNNDSp7"
      },
      "source": [
        "Realizo el modelo con Residual Blocks para resolver Fashion Mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpYdPtS-DQqk",
        "outputId": "4ce0ba5e-13a3-4ccc-b18a-f83c72d448c3"
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_split = 0.2, epochs=20)\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_61 (Conv2D)           (None, 32, 32, 64)        9408      \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "residual_block_16 (ResidualB (None, 16, 16, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_block_17 (ResidualB (None, 16, 16, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_block_18 (ResidualB (None, 16, 16, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_block_19 (ResidualB (None, 8, 8, 128)         230912    \n",
            "_________________________________________________________________\n",
            "residual_block_20 (ResidualB (None, 8, 8, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_block_21 (ResidualB (None, 8, 8, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_block_22 (ResidualB (None, 8, 8, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_block_23 (ResidualB (None, 4, 4, 256)         920576    \n",
            "_________________________________________________________________\n",
            "residual_block_24 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_25 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_26 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_27 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_28 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_29 (ResidualB (None, 2, 2, 512)         3676160   \n",
            "_________________________________________________________________\n",
            "residual_block_30 (ResidualB (None, 2, 2, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "residual_block_31 (ResidualB (None, 2, 2, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 21,304,774\n",
            "Trainable params: 21,287,750\n",
            "Non-trainable params: 17,024\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 10s 85ms/step - loss: 0.6326 - accuracy: 0.8616 - val_loss: 22.6014 - val_accuracy: 0.2685\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.1223 - accuracy: 0.9595 - val_loss: 158.6252 - val_accuracy: 0.1528\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 0.1828 - accuracy: 0.9401 - val_loss: 14.5313 - val_accuracy: 0.1528\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.2145 - accuracy: 0.9411 - val_loss: 103.9377 - val_accuracy: 0.2315\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 0.0471 - accuracy: 0.9877 - val_loss: 24.3518 - val_accuracy: 0.1806\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.1230 - accuracy: 0.9611 - val_loss: 22.5082 - val_accuracy: 0.1806\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 2s 63ms/step - loss: 0.1223 - accuracy: 0.9572 - val_loss: 21.5925 - val_accuracy: 0.1944\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0521 - accuracy: 0.9880 - val_loss: 5.6272 - val_accuracy: 0.2870\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0759 - accuracy: 0.9808 - val_loss: 12.3972 - val_accuracy: 0.2731\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0658 - accuracy: 0.9814 - val_loss: 14.4257 - val_accuracy: 0.2222\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 2s 64ms/step - loss: 0.0709 - accuracy: 0.9823 - val_loss: 4.6789 - val_accuracy: 0.3889\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 2.4729 - val_accuracy: 0.4815\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0910 - accuracy: 0.9799 - val_loss: 1.7365 - val_accuracy: 0.6713\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0354 - accuracy: 0.9892 - val_loss: 2.2845 - val_accuracy: 0.5417\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0748 - accuracy: 0.9780 - val_loss: 4.6572 - val_accuracy: 0.4769\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 2s 66ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.9127 - val_accuracy: 0.7778\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.8935\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0165 - accuracy: 0.9971 - val_loss: 0.2437 - val_accuracy: 0.9398\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1942 - val_accuracy: 0.9630\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2091 - val_accuracy: 0.9676\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.2106 - accuracy: 0.9500\n",
            "CNN básica:  [0.2105908989906311, 0.949999988079071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vhjFVJsNKz"
      },
      "source": [
        "### Con **Residual Block** alcanzo un accuracy con datos de test de **95%** y veo que no ha habido sobreaprendizaje, por lo que mejoro con respecto a la primera CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klJPfuBTwnca"
      },
      "source": [
        "###Al igual que en la anterior CNN voy a intentar entrenarla más y para ello voy ha hacerlo pero utilizado **EarlyStopping de Keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpFwdYQptcUH",
        "outputId": "80b0b4a8-5d91-4127-8e03-0bdf2a27efa0"
      },
      "source": [
        "class ResidualBlock(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            #Convolución: filters filtros de dimension 3, padding=samme y strides=strides sin bías\n",
        "            keras.layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False),\n",
        "            #Aplicar batch normalization\n",
        "            keras.layers.BatchNormalization(), \n",
        "            #Aplicamos función de activación\n",
        "            self.activation, \n",
        "            #Convolución: filters filtros de dimension 3, padding=samme y strides=1 sin bías\n",
        "            keras.layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False),\n",
        "            #Aplicar batch normalization\n",
        "            keras.layers.BatchNormalization()\n",
        "        ]\n",
        "        self.skip_layers = [] \n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                #Convolución: filters filtros de dimension 1, padding=samme y strides=strides sin bías\n",
        "                keras.layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=False), \n",
        "                #Aplicar batch normalization\n",
        "                keras.layers.BatchNormalization()\n",
        "            ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers: \n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "# Convolución 64 filtros de dimension 7, padding=samme y strides=2 sin bías\n",
        "# Preparar el modelo para trabajar con Fashion Mnist\n",
        "model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[64, 64, 3], padding=\"same\", use_bias=False))\n",
        "#Original para trabajar con imagenes de Image Net\n",
        "# model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3], padding=\"same\", use_bias=False))\n",
        "# Batch Normalization\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "#Aplicamos función de activación relu\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "# Max pool de dimension 3 con stride 2 y padding same\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")) \n",
        "prev_filters = 64\n",
        "# Añadimos 3 residual blocks de 64 filtros, 4 de 128, 6 de 265 y 3 de 512\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2 \n",
        "    model.add(ResidualBlock(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "# Aplicamos GlobalAvgPool2D (Maximo de cada canal)\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "# Convertimos las entradas en un vector para poder pasarlo a una capa Dense\n",
        "model.add(keras.layers.Flatten())\n",
        "# Creamos la capa de salida para clasificación con 10 posibles clases mutuamente excluyentes\n",
        "model.add(keras.layers.Dense(6, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(X_train,y_train, validation_split = 0.2, epochs=50,callbacks=[callback],verbose=1)\n",
        "\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 64)        9408      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "residual_block (ResidualBloc (None, 16, 16, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_block_1 (ResidualBl (None, 16, 16, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_block_2 (ResidualBl (None, 16, 16, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_block_3 (ResidualBl (None, 8, 8, 128)         230912    \n",
            "_________________________________________________________________\n",
            "residual_block_4 (ResidualBl (None, 8, 8, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_block_5 (ResidualBl (None, 8, 8, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_block_6 (ResidualBl (None, 8, 8, 128)         295936    \n",
            "_________________________________________________________________\n",
            "residual_block_7 (ResidualBl (None, 4, 4, 256)         920576    \n",
            "_________________________________________________________________\n",
            "residual_block_8 (ResidualBl (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_9 (ResidualBl (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_10 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_11 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_12 (ResidualB (None, 4, 4, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_block_13 (ResidualB (None, 2, 2, 512)         3676160   \n",
            "_________________________________________________________________\n",
            "residual_block_14 (ResidualB (None, 2, 2, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "residual_block_15 (ResidualB (None, 2, 2, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 21,304,774\n",
            "Trainable params: 21,287,750\n",
            "Non-trainable params: 17,024\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "27/27 [==============================] - 16s 225ms/step - loss: 2.3357 - accuracy: 0.3404 - val_loss: 10.7910 - val_accuracy: 0.1528\n",
            "Epoch 2/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.5042 - accuracy: 0.8155 - val_loss: 8.9934 - val_accuracy: 0.1528\n",
            "Epoch 3/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.2120 - accuracy: 0.9342 - val_loss: 14.4789 - val_accuracy: 0.1528\n",
            "Epoch 4/50\n",
            "27/27 [==============================] - 5s 190ms/step - loss: 0.2852 - accuracy: 0.9064 - val_loss: 11.8909 - val_accuracy: 0.1528\n",
            "Epoch 5/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.1944 - accuracy: 0.9433 - val_loss: 18.2377 - val_accuracy: 0.1528\n",
            "Epoch 6/50\n",
            "27/27 [==============================] - 5s 190ms/step - loss: 0.1504 - accuracy: 0.9553 - val_loss: 40.0838 - val_accuracy: 0.1528\n",
            "Epoch 7/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 16.8306 - val_accuracy: 0.1528\n",
            "Epoch 8/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.0572 - accuracy: 0.9811 - val_loss: 34.6470 - val_accuracy: 0.1528\n",
            "Epoch 9/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.1135 - accuracy: 0.9748 - val_loss: 45.3611 - val_accuracy: 0.1528\n",
            "Epoch 10/50\n",
            "27/27 [==============================] - 5s 190ms/step - loss: 0.1080 - accuracy: 0.9735 - val_loss: 13.4467 - val_accuracy: 0.1574\n",
            "Epoch 11/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 0.0940 - accuracy: 0.9666 - val_loss: 4.6309 - val_accuracy: 0.3843\n",
            "Epoch 12/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 6.2485 - val_accuracy: 0.3241\n",
            "Epoch 13/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.0969 - accuracy: 0.9721 - val_loss: 3.2089 - val_accuracy: 0.5370\n",
            "Epoch 14/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 0.0145 - accuracy: 0.9945 - val_loss: 2.9773 - val_accuracy: 0.5741\n",
            "Epoch 15/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.3600 - val_accuracy: 0.6389\n",
            "Epoch 16/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 8.0434e-04 - accuracy: 1.0000 - val_loss: 1.9390 - val_accuracy: 0.6944\n",
            "Epoch 17/50\n",
            "27/27 [==============================] - 5s 190ms/step - loss: 6.1355e-04 - accuracy: 1.0000 - val_loss: 1.5182 - val_accuracy: 0.7269\n",
            "Epoch 18/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 4.2753e-04 - accuracy: 1.0000 - val_loss: 1.0562 - val_accuracy: 0.7824\n",
            "Epoch 19/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 3.1701e-04 - accuracy: 1.0000 - val_loss: 0.6911 - val_accuracy: 0.8519\n",
            "Epoch 20/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 2.7838e-04 - accuracy: 1.0000 - val_loss: 0.4400 - val_accuracy: 0.9074\n",
            "Epoch 21/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 2.0534e-04 - accuracy: 1.0000 - val_loss: 0.2920 - val_accuracy: 0.9398\n",
            "Epoch 22/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 2.8502e-04 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 0.9444\n",
            "Epoch 23/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 1.7474e-04 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9537\n",
            "Epoch 24/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 1.6028e-04 - accuracy: 1.0000 - val_loss: 0.1820 - val_accuracy: 0.9537\n",
            "Epoch 25/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 1.9649e-04 - accuracy: 1.0000 - val_loss: 0.1717 - val_accuracy: 0.9537\n",
            "Epoch 26/50\n",
            "27/27 [==============================] - 5s 193ms/step - loss: 1.0224e-04 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9537\n",
            "Epoch 27/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 1.0308e-04 - accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 0.9583\n",
            "Epoch 28/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 7.2347e-05 - accuracy: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.9583\n",
            "Epoch 29/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 1.0449e-04 - accuracy: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.9583\n",
            "Epoch 30/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 9.1979e-05 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 0.9676\n",
            "Epoch 31/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 1.1120e-04 - accuracy: 1.0000 - val_loss: 0.1339 - val_accuracy: 0.9676\n",
            "Epoch 32/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 7.3970e-05 - accuracy: 1.0000 - val_loss: 0.1336 - val_accuracy: 0.9676\n",
            "Epoch 33/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 6.8444e-05 - accuracy: 1.0000 - val_loss: 0.1352 - val_accuracy: 0.9676\n",
            "Epoch 34/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 1.0302e-04 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9676\n",
            "Epoch 35/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 5.1195e-05 - accuracy: 1.0000 - val_loss: 0.1351 - val_accuracy: 0.9676\n",
            "Epoch 36/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 7.8947e-05 - accuracy: 1.0000 - val_loss: 0.1353 - val_accuracy: 0.9676\n",
            "Epoch 37/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 3.3915e-05 - accuracy: 1.0000 - val_loss: 0.1361 - val_accuracy: 0.9676\n",
            "Epoch 38/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 5.5544e-05 - accuracy: 1.0000 - val_loss: 0.1329 - val_accuracy: 0.9676\n",
            "Epoch 39/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 8.6859e-05 - accuracy: 1.0000 - val_loss: 0.1339 - val_accuracy: 0.9676\n",
            "Epoch 40/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 4.1658e-05 - accuracy: 1.0000 - val_loss: 0.1363 - val_accuracy: 0.9722\n",
            "Epoch 41/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 4.3551e-05 - accuracy: 1.0000 - val_loss: 0.1363 - val_accuracy: 0.9722\n",
            "Epoch 42/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 3.7930e-05 - accuracy: 1.0000 - val_loss: 0.1354 - val_accuracy: 0.9722\n",
            "Epoch 43/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 7.1937e-05 - accuracy: 1.0000 - val_loss: 0.1350 - val_accuracy: 0.9722\n",
            "Epoch 44/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 5.1389e-05 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9722\n",
            "Epoch 45/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 1.1308e-04 - accuracy: 1.0000 - val_loss: 0.1276 - val_accuracy: 0.9676\n",
            "Epoch 46/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 4.4816e-05 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9676\n",
            "Epoch 47/50\n",
            "27/27 [==============================] - 5s 192ms/step - loss: 3.3059e-05 - accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9676\n",
            "Epoch 48/50\n",
            "27/27 [==============================] - 5s 193ms/step - loss: 2.2405e-05 - accuracy: 1.0000 - val_loss: 0.1324 - val_accuracy: 0.9676\n",
            "Epoch 49/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 7.0898e-05 - accuracy: 1.0000 - val_loss: 0.1333 - val_accuracy: 0.9676\n",
            "Epoch 50/50\n",
            "27/27 [==============================] - 5s 191ms/step - loss: 3.1806e-05 - accuracy: 1.0000 - val_loss: 0.1344 - val_accuracy: 0.9676\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.0832 - accuracy: 0.9833\n",
            "CNN básica:  [0.08321157097816467, 0.9833333492279053]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2QRFH21xxi6"
      },
      "source": [
        "### Pablo en este caso tengo dudas con el resultado:  porque creo que puede haber sobreaprendizaje si observo el accuracy de train del 100% pero también es cierto que nos indicaste que cuando sucede el sobreaprendizaje pasan cosas raras con los datos de test ( y en este caso no tengo subidas y bajadas bruscas con los datos de test) por lo que para mi no me queda del todo claro este resultado del **98,33%**.\n",
        "### Tambien si me fijo en el caso anterior de esta red tengo accuracy de train de 100% al final del entrenamiento y me indicaste que no había sobreaprendizaje ya que en los datos de test no había esos cambios bruscos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVuHyxPXCFfI"
      },
      "source": [
        "# 3. CNN con Transfer Learning - cargamos ResNet\n",
        "\n",
        "[Keras](https://keras.io/applications/) proporciona modelos de algunas de las redes convolucionales más famosas entrenadas para reconocer imagenes de [Image Net](http://www.image-net.org/)\n",
        "\n",
        "Esto nos simplifica bastante el entrenamiento cuando tenemos la posibilidad de hacer **transfer learning**.\n",
        "\n",
        "Cargo directamente una red ResNet \n",
        "\n",
        "**En este caso lo hago pero sé de antemano que no voy a obtener un buen resultado ya que el tipo de red seleccionada está entrenada para la detección de objetos y no es la más adecuada para este problema**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov5WQmNsDnp7"
      },
      "source": [
        "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdwBq-tIAVtd"
      },
      "source": [
        "Intentemos reconocer qué son las imagenes utilizando el modelo cargado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eTBuHfMD4rq",
        "outputId": "28f618bf-e3d2-43f8-d283-dfca8d5d3bd4"
      },
      "source": [
        "images_resized = tf.image.resize(X_train, [224, 224])\n",
        "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
        "Y_proba = model.predict(inputs)\n",
        "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3) \n",
        "for image_index in range(len(X_train)):\n",
        "    print(\"Image #{}\".format(image_index))\n",
        "    for class_id, name, y_proba in top_K[image_index]:\n",
        "        print(\" {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "Image #80\n",
            " n02786058 - Band_Aid     64.24%\n",
            " n04591157 - Windsor_tie  10.92%\n",
            " n03970156 - plunger      6.24%\n",
            "\n",
            "Image #81\n",
            " n03970156 - plunger      37.36%\n",
            " n03250847 - drumstick    10.69%\n",
            " n02786058 - Band_Aid     7.28%\n",
            "\n",
            "Image #82\n",
            " n02786058 - Band_Aid     28.58%\n",
            " n04357314 - sunscreen    12.15%\n",
            " n03314780 - face_powder  5.28%\n",
            "\n",
            "Image #83\n",
            " n02786058 - Band_Aid     19.85%\n",
            " n03250847 - drumstick    11.41%\n",
            " n04357314 - sunscreen    8.68%\n",
            "\n",
            "Image #84\n",
            " n02786058 - Band_Aid     46.06%\n",
            " n04591157 - Windsor_tie  25.52%\n",
            " n04350905 - suit         3.11%\n",
            "\n",
            "Image #85\n",
            " n04372370 - switch       43.26%\n",
            " n04591157 - Windsor_tie  12.59%\n",
            " n02786058 - Band_Aid     7.15%\n",
            "\n",
            "Image #86\n",
            " n03250847 - drumstick    40.78%\n",
            " n03970156 - plunger      19.85%\n",
            " n02786058 - Band_Aid     16.49%\n",
            "\n",
            "Image #87\n",
            " n03250847 - drumstick    19.46%\n",
            " n04372370 - switch       16.66%\n",
            " n04254120 - soap_dispenser 13.62%\n",
            "\n",
            "Image #88\n",
            " n04591157 - Windsor_tie  8.83%\n",
            " n02951585 - can_opener   6.92%\n",
            " n04254120 - soap_dispenser 5.77%\n",
            "\n",
            "Image #89\n",
            " n02786058 - Band_Aid     31.78%\n",
            " n03970156 - plunger      4.10%\n",
            " n04357314 - sunscreen    3.37%\n",
            "\n",
            "Image #90\n",
            " n04357314 - sunscreen    14.53%\n",
            " n04591157 - Windsor_tie  12.21%\n",
            " n02786058 - Band_Aid     7.29%\n",
            "\n",
            "Image #91\n",
            " n04591157 - Windsor_tie  47.10%\n",
            " n04372370 - switch       22.93%\n",
            " n02786058 - Band_Aid     5.32%\n",
            "\n",
            "Image #92\n",
            " n03970156 - plunger      17.36%\n",
            " n03720891 - maraca       16.93%\n",
            " n03041632 - cleaver      10.55%\n",
            "\n",
            "Image #93\n",
            " n02786058 - Band_Aid     28.33%\n",
            " n04591157 - Windsor_tie  4.37%\n",
            " n04209239 - shower_curtain 4.27%\n",
            "\n",
            "Image #94\n",
            " n03970156 - plunger      23.28%\n",
            " n03250847 - drumstick    15.11%\n",
            " n04591157 - Windsor_tie  9.67%\n",
            "\n",
            "Image #95\n",
            " n02786058 - Band_Aid     18.86%\n",
            " n04591157 - Windsor_tie  9.25%\n",
            " n03314780 - face_powder  9.06%\n",
            "\n",
            "Image #96\n",
            " n04591157 - Windsor_tie  26.55%\n",
            " n03970156 - plunger      20.18%\n",
            " n02786058 - Band_Aid     7.95%\n",
            "\n",
            "Image #97\n",
            " n02786058 - Band_Aid     39.71%\n",
            " n04357314 - sunscreen    29.46%\n",
            " n04371430 - swimming_trunks 2.56%\n",
            "\n",
            "Image #98\n",
            " n04357314 - sunscreen    16.29%\n",
            " n02786058 - Band_Aid     14.66%\n",
            " n04591157 - Windsor_tie  7.21%\n",
            "\n",
            "Image #99\n",
            " n04591157 - Windsor_tie  23.73%\n",
            " n02786058 - Band_Aid     7.54%\n",
            " n03970156 - plunger      7.45%\n",
            "\n",
            "Image #100\n",
            " n02786058 - Band_Aid     18.42%\n",
            " n04591157 - Windsor_tie  14.11%\n",
            " n02951585 - can_opener   10.32%\n",
            "\n",
            "Image #101\n",
            " n02786058 - Band_Aid     34.33%\n",
            " n04591157 - Windsor_tie  8.12%\n",
            " n04357314 - sunscreen    3.71%\n",
            "\n",
            "Image #102\n",
            " n04591157 - Windsor_tie  31.62%\n",
            " n04357314 - sunscreen    14.26%\n",
            " n02786058 - Band_Aid     4.29%\n",
            "\n",
            "Image #103\n",
            " n04591157 - Windsor_tie  14.65%\n",
            " n04372370 - switch       14.42%\n",
            " n02786058 - Band_Aid     12.41%\n",
            "\n",
            "Image #104\n",
            " n04591157 - Windsor_tie  13.09%\n",
            " n02786058 - Band_Aid     12.81%\n",
            " n04357314 - sunscreen    9.45%\n",
            "\n",
            "Image #105\n",
            " n03970156 - plunger      29.63%\n",
            " n02786058 - Band_Aid     18.29%\n",
            " n03250847 - drumstick    7.37%\n",
            "\n",
            "Image #106\n",
            " n03970156 - plunger      24.43%\n",
            " n04270147 - spatula      14.12%\n",
            " n03250847 - drumstick    12.24%\n",
            "\n",
            "Image #107\n",
            " n04591157 - Windsor_tie  24.68%\n",
            " n02786058 - Band_Aid     18.51%\n",
            " n04357314 - sunscreen    6.75%\n",
            "\n",
            "Image #108\n",
            " n02786058 - Band_Aid     16.47%\n",
            " n04357314 - sunscreen    14.91%\n",
            " n04591157 - Windsor_tie  6.68%\n",
            "\n",
            "Image #109\n",
            " n04357314 - sunscreen    25.91%\n",
            " n04591157 - Windsor_tie  12.72%\n",
            " n02786058 - Band_Aid     11.82%\n",
            "\n",
            "Image #110\n",
            " n04591157 - Windsor_tie  17.76%\n",
            " n04357314 - sunscreen    11.38%\n",
            " n02951585 - can_opener   3.84%\n",
            "\n",
            "Image #111\n",
            " n03250847 - drumstick    36.00%\n",
            " n02786058 - Band_Aid     21.18%\n",
            " n03970156 - plunger      9.69%\n",
            "\n",
            "Image #112\n",
            " n02786058 - Band_Aid     14.63%\n",
            " n04357314 - sunscreen    13.33%\n",
            " n04372370 - switch       5.62%\n",
            "\n",
            "Image #113\n",
            " n03250847 - drumstick    49.92%\n",
            " n04591157 - Windsor_tie  13.60%\n",
            " n03970156 - plunger      11.50%\n",
            "\n",
            "Image #114\n",
            " n04591157 - Windsor_tie  23.28%\n",
            " n04270147 - spatula      16.72%\n",
            " n02951585 - can_opener   7.70%\n",
            "\n",
            "Image #115\n",
            " n02786058 - Band_Aid     11.79%\n",
            " n04591157 - Windsor_tie  11.13%\n",
            " n03970156 - plunger      9.16%\n",
            "\n",
            "Image #116\n",
            " n03970156 - plunger      30.94%\n",
            " n03250847 - drumstick    26.53%\n",
            " n02786058 - Band_Aid     10.34%\n",
            "\n",
            "Image #117\n",
            " n03970156 - plunger      21.13%\n",
            " n03250847 - drumstick    20.25%\n",
            " n02786058 - Band_Aid     10.02%\n",
            "\n",
            "Image #118\n",
            " n02786058 - Band_Aid     17.69%\n",
            " n04357314 - sunscreen    12.87%\n",
            " n04591157 - Windsor_tie  6.98%\n",
            "\n",
            "Image #119\n",
            " n04357314 - sunscreen    17.31%\n",
            " n02786058 - Band_Aid     17.19%\n",
            " n04591157 - Windsor_tie  5.20%\n",
            "\n",
            "Image #120\n",
            " n04591157 - Windsor_tie  33.36%\n",
            " n04357314 - sunscreen    4.70%\n",
            " n04254120 - soap_dispenser 4.62%\n",
            "\n",
            "Image #121\n",
            " n04372370 - switch       17.61%\n",
            " n02786058 - Band_Aid     8.44%\n",
            " n04254120 - soap_dispenser 6.00%\n",
            "\n",
            "Image #122\n",
            " n02786058 - Band_Aid     34.36%\n",
            " n04357314 - sunscreen    20.24%\n",
            " n03250847 - drumstick    5.19%\n",
            "\n",
            "Image #123\n",
            " n04591157 - Windsor_tie  16.73%\n",
            " n03970156 - plunger      15.55%\n",
            " n03250847 - drumstick    13.24%\n",
            "\n",
            "Image #124\n",
            " n04372370 - switch       19.07%\n",
            " n03314780 - face_powder  16.94%\n",
            " n04591157 - Windsor_tie  6.86%\n",
            "\n",
            "Image #125\n",
            " n03970156 - plunger      19.97%\n",
            " n04591157 - Windsor_tie  11.13%\n",
            " n04357314 - sunscreen    5.47%\n",
            "\n",
            "Image #126\n",
            " n03970156 - plunger      42.83%\n",
            " n03250847 - drumstick    13.32%\n",
            " n04270147 - spatula      5.89%\n",
            "\n",
            "Image #127\n",
            " n03970156 - plunger      51.11%\n",
            " n02786058 - Band_Aid     11.31%\n",
            " n03250847 - drumstick    5.10%\n",
            "\n",
            "Image #128\n",
            " n03970156 - plunger      9.45%\n",
            " n04591157 - Windsor_tie  9.30%\n",
            " n02786058 - Band_Aid     8.74%\n",
            "\n",
            "Image #129\n",
            " n04591157 - Windsor_tie  45.17%\n",
            " n03970156 - plunger      8.63%\n",
            " n03720891 - maraca       4.83%\n",
            "\n",
            "Image #130\n",
            " n02786058 - Band_Aid     17.87%\n",
            " n04357314 - sunscreen    13.53%\n",
            " n02808440 - bathtub      5.96%\n",
            "\n",
            "Image #131\n",
            " n02786058 - Band_Aid     28.33%\n",
            " n03970156 - plunger      7.29%\n",
            " n03250847 - drumstick    6.92%\n",
            "\n",
            "Image #132\n",
            " n02786058 - Band_Aid     22.79%\n",
            " n04357314 - sunscreen    10.25%\n",
            " n03250847 - drumstick    8.63%\n",
            "\n",
            "Image #133\n",
            " n04372370 - switch       20.30%\n",
            " n02786058 - Band_Aid     10.15%\n",
            " n02951585 - can_opener   7.42%\n",
            "\n",
            "Image #134\n",
            " n04372370 - switch       51.77%\n",
            " n04591157 - Windsor_tie  12.83%\n",
            " n02786058 - Band_Aid     11.97%\n",
            "\n",
            "Image #135\n",
            " n04372370 - switch       18.64%\n",
            " n04591157 - Windsor_tie  15.85%\n",
            " n02951585 - can_opener   6.95%\n",
            "\n",
            "Image #136\n",
            " n04372370 - switch       22.49%\n",
            " n03970156 - plunger      9.62%\n",
            " n02786058 - Band_Aid     5.73%\n",
            "\n",
            "Image #137\n",
            " n02786058 - Band_Aid     33.81%\n",
            " n03970156 - plunger      9.51%\n",
            " n03250847 - drumstick    7.97%\n",
            "\n",
            "Image #138\n",
            " n04591157 - Windsor_tie  62.01%\n",
            " n04357314 - sunscreen    8.04%\n",
            " n03970156 - plunger      2.77%\n",
            "\n",
            "Image #139\n",
            " n04591157 - Windsor_tie  14.89%\n",
            " n04357314 - sunscreen    10.94%\n",
            " n03250847 - drumstick    8.39%\n",
            "\n",
            "Image #140\n",
            " n02786058 - Band_Aid     11.07%\n",
            " n04372370 - switch       9.80%\n",
            " n03970156 - plunger      6.83%\n",
            "\n",
            "Image #141\n",
            " n04357314 - sunscreen    15.41%\n",
            " n02786058 - Band_Aid     6.10%\n",
            " n03250847 - drumstick    5.23%\n",
            "\n",
            "Image #142\n",
            " n04254120 - soap_dispenser 8.83%\n",
            " n03476991 - hair_spray   8.72%\n",
            " n03314780 - face_powder  6.10%\n",
            "\n",
            "Image #143\n",
            " n04591157 - Windsor_tie  27.35%\n",
            " n02786058 - Band_Aid     14.61%\n",
            " n03970156 - plunger      6.54%\n",
            "\n",
            "Image #144\n",
            " n03970156 - plunger      22.75%\n",
            " n02786058 - Band_Aid     13.02%\n",
            " n04270147 - spatula      12.97%\n",
            "\n",
            "Image #145\n",
            " n03970156 - plunger      13.17%\n",
            " n04254120 - soap_dispenser 9.96%\n",
            " n03676483 - lipstick     5.84%\n",
            "\n",
            "Image #146\n",
            " n04372370 - switch       14.38%\n",
            " n03970156 - plunger      11.92%\n",
            " n02786058 - Band_Aid     6.97%\n",
            "\n",
            "Image #147\n",
            " n04328186 - stopwatch    12.00%\n",
            " n02951585 - can_opener   10.09%\n",
            " n03476991 - hair_spray   8.29%\n",
            "\n",
            "Image #148\n",
            " n02786058 - Band_Aid     15.24%\n",
            " n04357314 - sunscreen    8.86%\n",
            " n04254120 - soap_dispenser 7.00%\n",
            "\n",
            "Image #149\n",
            " n04591157 - Windsor_tie  12.77%\n",
            " n03485407 - hand-held_computer 11.75%\n",
            " n02786058 - Band_Aid     11.42%\n",
            "\n",
            "Image #150\n",
            " n02786058 - Band_Aid     31.91%\n",
            " n04357314 - sunscreen    11.23%\n",
            " n03970156 - plunger      4.03%\n",
            "\n",
            "Image #151\n",
            " n04372370 - switch       49.31%\n",
            " n04254120 - soap_dispenser 4.42%\n",
            " n02786058 - Band_Aid     4.16%\n",
            "\n",
            "Image #152\n",
            " n02786058 - Band_Aid     28.77%\n",
            " n04591157 - Windsor_tie  8.47%\n",
            " n04357314 - sunscreen    7.44%\n",
            "\n",
            "Image #153\n",
            " n03970156 - plunger      25.99%\n",
            " n02786058 - Band_Aid     10.71%\n",
            " n04270147 - spatula      6.94%\n",
            "\n",
            "Image #154\n",
            " n02786058 - Band_Aid     20.07%\n",
            " n04372370 - switch       16.46%\n",
            " n03485407 - hand-held_computer 7.84%\n",
            "\n",
            "Image #155\n",
            " n02786058 - Band_Aid     43.38%\n",
            " n04357314 - sunscreen    21.56%\n",
            " n04591157 - Windsor_tie  7.18%\n",
            "\n",
            "Image #156\n",
            " n04254120 - soap_dispenser 9.97%\n",
            " n02951585 - can_opener   9.97%\n",
            " n03314780 - face_powder  5.60%\n",
            "\n",
            "Image #157\n",
            " n04357314 - sunscreen    14.41%\n",
            " n02786058 - Band_Aid     13.60%\n",
            " n04270147 - spatula      11.36%\n",
            "\n",
            "Image #158\n",
            " n04372370 - switch       43.98%\n",
            " n04591157 - Windsor_tie  7.11%\n",
            " n02951585 - can_opener   6.97%\n",
            "\n",
            "Image #159\n",
            " n02786058 - Band_Aid     27.01%\n",
            " n04357314 - sunscreen    15.77%\n",
            " n03250847 - drumstick    7.19%\n",
            "\n",
            "Image #160\n",
            " n04372370 - switch       21.27%\n",
            " n04591157 - Windsor_tie  19.51%\n",
            " n04254120 - soap_dispenser 12.17%\n",
            "\n",
            "Image #161\n",
            " n04372370 - switch       29.36%\n",
            " n02786058 - Band_Aid     12.07%\n",
            " n04591157 - Windsor_tie  10.37%\n",
            "\n",
            "Image #162\n",
            " n04254120 - soap_dispenser 20.31%\n",
            " n02786058 - Band_Aid     10.19%\n",
            " n04357314 - sunscreen    10.00%\n",
            "\n",
            "Image #163\n",
            " n04591157 - Windsor_tie  32.31%\n",
            " n04357314 - sunscreen    7.38%\n",
            " n04254120 - soap_dispenser 5.80%\n",
            "\n",
            "Image #164\n",
            " n04372370 - switch       38.27%\n",
            " n02951585 - can_opener   18.61%\n",
            " n04254120 - soap_dispenser 13.05%\n",
            "\n",
            "Image #165\n",
            " n02786058 - Band_Aid     15.34%\n",
            " n04357314 - sunscreen    14.76%\n",
            " n04591157 - Windsor_tie  12.37%\n",
            "\n",
            "Image #166\n",
            " n02786058 - Band_Aid     41.71%\n",
            " n04357314 - sunscreen    7.45%\n",
            " n04591157 - Windsor_tie  5.99%\n",
            "\n",
            "Image #167\n",
            " n04357314 - sunscreen    22.94%\n",
            " n02786058 - Band_Aid     9.54%\n",
            " n04591157 - Windsor_tie  5.79%\n",
            "\n",
            "Image #168\n",
            " n04591157 - Windsor_tie  18.54%\n",
            " n02786058 - Band_Aid     16.32%\n",
            " n04357314 - sunscreen    9.72%\n",
            "\n",
            "Image #169\n",
            " n02786058 - Band_Aid     21.46%\n",
            " n04591157 - Windsor_tie  16.12%\n",
            " n04357314 - sunscreen    10.95%\n",
            "\n",
            "Image #170\n",
            " n04591157 - Windsor_tie  47.50%\n",
            " n04270147 - spatula      4.47%\n",
            " n03970156 - plunger      4.18%\n",
            "\n",
            "Image #171\n",
            " n04357314 - sunscreen    15.64%\n",
            " n02786058 - Band_Aid     14.41%\n",
            " n03250847 - drumstick    10.63%\n",
            "\n",
            "Image #172\n",
            " n02786058 - Band_Aid     12.88%\n",
            " n04591157 - Windsor_tie  12.41%\n",
            " n03970156 - plunger      7.68%\n",
            "\n",
            "Image #173\n",
            " n04372370 - switch       35.70%\n",
            " n04591157 - Windsor_tie  13.48%\n",
            " n03485407 - hand-held_computer 9.87%\n",
            "\n",
            "Image #174\n",
            " n03970156 - plunger      68.36%\n",
            " n03250847 - drumstick    5.15%\n",
            " n04270147 - spatula      4.06%\n",
            "\n",
            "Image #175\n",
            " n04357314 - sunscreen    15.82%\n",
            " n02786058 - Band_Aid     9.04%\n",
            " n04540053 - volleyball   3.20%\n",
            "\n",
            "Image #176\n",
            " n04357314 - sunscreen    14.67%\n",
            " n04591157 - Windsor_tie  8.11%\n",
            " n02786058 - Band_Aid     6.80%\n",
            "\n",
            "Image #177\n",
            " n04372370 - switch       32.41%\n",
            " n04591157 - Windsor_tie  10.76%\n",
            " n04254120 - soap_dispenser 10.67%\n",
            "\n",
            "Image #178\n",
            " n02786058 - Band_Aid     33.31%\n",
            " n04357314 - sunscreen    10.71%\n",
            " n04591157 - Windsor_tie  8.22%\n",
            "\n",
            "Image #179\n",
            " n04357314 - sunscreen    17.77%\n",
            " n02786058 - Band_Aid     17.69%\n",
            " n04591157 - Windsor_tie  4.40%\n",
            "\n",
            "Image #180\n",
            " n03970156 - plunger      22.96%\n",
            " n02786058 - Band_Aid     7.89%\n",
            " n04591157 - Windsor_tie  7.30%\n",
            "\n",
            "Image #181\n",
            " n03250847 - drumstick    35.37%\n",
            " n02951585 - can_opener   5.28%\n",
            " n03970156 - plunger      4.09%\n",
            "\n",
            "Image #182\n",
            " n03970156 - plunger      28.79%\n",
            " n03250847 - drumstick    11.64%\n",
            " n04270147 - spatula      8.91%\n",
            "\n",
            "Image #183\n",
            " n03250847 - drumstick    32.74%\n",
            " n02786058 - Band_Aid     13.37%\n",
            " n04357314 - sunscreen    5.63%\n",
            "\n",
            "Image #184\n",
            " n03476991 - hair_spray   15.52%\n",
            " n02951585 - can_opener   8.20%\n",
            " n04372370 - switch       5.56%\n",
            "\n",
            "Image #185\n",
            " n02951585 - can_opener   15.52%\n",
            " n04372370 - switch       15.38%\n",
            " n03970156 - plunger      14.20%\n",
            "\n",
            "Image #186\n",
            " n02786058 - Band_Aid     14.60%\n",
            " n04372370 - switch       9.32%\n",
            " n04591157 - Windsor_tie  7.56%\n",
            "\n",
            "Image #187\n",
            " n03970156 - plunger      19.70%\n",
            " n03250847 - drumstick    11.87%\n",
            " n04270147 - spatula      9.71%\n",
            "\n",
            "Image #188\n",
            " n04357314 - sunscreen    35.31%\n",
            " n02786058 - Band_Aid     21.07%\n",
            " n03250847 - drumstick    7.74%\n",
            "\n",
            "Image #189\n",
            " n04357314 - sunscreen    9.97%\n",
            " n02786058 - Band_Aid     6.12%\n",
            " n04591157 - Windsor_tie  5.18%\n",
            "\n",
            "Image #190\n",
            " n04357314 - sunscreen    31.98%\n",
            " n04591157 - Windsor_tie  15.09%\n",
            " n02786058 - Band_Aid     12.96%\n",
            "\n",
            "Image #191\n",
            " n02786058 - Band_Aid     28.56%\n",
            " n04357314 - sunscreen    20.32%\n",
            " n03314780 - face_powder  3.70%\n",
            "\n",
            "Image #192\n",
            " n02786058 - Band_Aid     20.06%\n",
            " n03250847 - drumstick    19.55%\n",
            " n04591157 - Windsor_tie  16.02%\n",
            "\n",
            "Image #193\n",
            " n03970156 - plunger      19.94%\n",
            " n02786058 - Band_Aid     15.10%\n",
            " n04591157 - Windsor_tie  12.28%\n",
            "\n",
            "Image #194\n",
            " n04372370 - switch       39.10%\n",
            " n04254120 - soap_dispenser 6.20%\n",
            " n03825788 - nipple       4.39%\n",
            "\n",
            "Image #195\n",
            " n03250847 - drumstick    22.73%\n",
            " n04591157 - Windsor_tie  16.67%\n",
            " n03970156 - plunger      11.96%\n",
            "\n",
            "Image #196\n",
            " n04357314 - sunscreen    26.08%\n",
            " n04591157 - Windsor_tie  13.83%\n",
            " n03250847 - drumstick    9.34%\n",
            "\n",
            "Image #197\n",
            " n04591157 - Windsor_tie  53.17%\n",
            " n04372370 - switch       12.40%\n",
            " n02786058 - Band_Aid     5.52%\n",
            "\n",
            "Image #198\n",
            " n02786058 - Band_Aid     32.82%\n",
            " n04591157 - Windsor_tie  8.54%\n",
            " n04357314 - sunscreen    4.77%\n",
            "\n",
            "Image #199\n",
            " n02786058 - Band_Aid     23.85%\n",
            " n04357314 - sunscreen    17.92%\n",
            " n04591157 - Windsor_tie  16.48%\n",
            "\n",
            "Image #200\n",
            " n03970156 - plunger      20.79%\n",
            " n04591157 - Windsor_tie  11.32%\n",
            " n02786058 - Band_Aid     9.94%\n",
            "\n",
            "Image #201\n",
            " n02786058 - Band_Aid     27.45%\n",
            " n04357314 - sunscreen    8.57%\n",
            " n04591157 - Windsor_tie  3.67%\n",
            "\n",
            "Image #202\n",
            " n04591157 - Windsor_tie  24.40%\n",
            " n03970156 - plunger      10.02%\n",
            " n03250847 - drumstick    6.44%\n",
            "\n",
            "Image #203\n",
            " n04372370 - switch       32.43%\n",
            " n02951585 - can_opener   8.63%\n",
            " n04254120 - soap_dispenser 8.36%\n",
            "\n",
            "Image #204\n",
            " n03250847 - drumstick    12.25%\n",
            " n04591157 - Windsor_tie  11.41%\n",
            " n04357314 - sunscreen    11.29%\n",
            "\n",
            "Image #205\n",
            " n04372370 - switch       24.29%\n",
            " n02951585 - can_opener   19.18%\n",
            " n04254120 - soap_dispenser 7.62%\n",
            "\n",
            "Image #206\n",
            " n04591157 - Windsor_tie  22.02%\n",
            " n04357314 - sunscreen    12.24%\n",
            " n02786058 - Band_Aid     4.76%\n",
            "\n",
            "Image #207\n",
            " n02786058 - Band_Aid     29.90%\n",
            " n04357314 - sunscreen    7.45%\n",
            " n04591157 - Windsor_tie  4.70%\n",
            "\n",
            "Image #208\n",
            " n04591157 - Windsor_tie  32.96%\n",
            " n04357314 - sunscreen    16.84%\n",
            " n02786058 - Band_Aid     4.48%\n",
            "\n",
            "Image #209\n",
            " n04357314 - sunscreen    12.21%\n",
            " n04591157 - Windsor_tie  10.00%\n",
            " n02786058 - Band_Aid     9.82%\n",
            "\n",
            "Image #210\n",
            " n04591157 - Windsor_tie  10.21%\n",
            " n02786058 - Band_Aid     7.94%\n",
            " n03970156 - plunger      6.90%\n",
            "\n",
            "Image #211\n",
            " n04591157 - Windsor_tie  18.67%\n",
            " n04357314 - sunscreen    17.87%\n",
            " n02786058 - Band_Aid     12.15%\n",
            "\n",
            "Image #212\n",
            " n02786058 - Band_Aid     19.68%\n",
            " n03250847 - drumstick    14.25%\n",
            " n04357314 - sunscreen    13.25%\n",
            "\n",
            "Image #213\n",
            " n03970156 - plunger      40.36%\n",
            " n04597913 - wooden_spoon 7.19%\n",
            " n03250847 - drumstick    7.11%\n",
            "\n",
            "Image #214\n",
            " n04591157 - Windsor_tie  20.26%\n",
            " n02786058 - Band_Aid     12.58%\n",
            " n04357314 - sunscreen    8.81%\n",
            "\n",
            "Image #215\n",
            " n02786058 - Band_Aid     29.49%\n",
            " n04591157 - Windsor_tie  12.91%\n",
            " n04357314 - sunscreen    9.90%\n",
            "\n",
            "Image #216\n",
            " n04591157 - Windsor_tie  19.07%\n",
            " n02786058 - Band_Aid     14.15%\n",
            " n04357314 - sunscreen    8.58%\n",
            "\n",
            "Image #217\n",
            " n02786058 - Band_Aid     18.51%\n",
            " n04591157 - Windsor_tie  17.65%\n",
            " n04357314 - sunscreen    8.73%\n",
            "\n",
            "Image #218\n",
            " n02786058 - Band_Aid     37.26%\n",
            " n04357314 - sunscreen    26.07%\n",
            " n03314780 - face_powder  2.53%\n",
            "\n",
            "Image #219\n",
            " n04357314 - sunscreen    22.05%\n",
            " n02786058 - Band_Aid     14.31%\n",
            " n04591157 - Windsor_tie  6.51%\n",
            "\n",
            "Image #220\n",
            " n02786058 - Band_Aid     12.12%\n",
            " n04591157 - Windsor_tie  9.78%\n",
            " n04254120 - soap_dispenser 7.92%\n",
            "\n",
            "Image #221\n",
            " n02786058 - Band_Aid     52.50%\n",
            " n04357314 - sunscreen    17.98%\n",
            " n03314780 - face_powder  4.20%\n",
            "\n",
            "Image #222\n",
            " n04270147 - spatula      14.97%\n",
            " n02951585 - can_opener   10.07%\n",
            " n02786058 - Band_Aid     9.05%\n",
            "\n",
            "Image #223\n",
            " n04591157 - Windsor_tie  7.15%\n",
            " n04270147 - spatula      6.66%\n",
            " n04357314 - sunscreen    6.19%\n",
            "\n",
            "Image #224\n",
            " n04357314 - sunscreen    13.86%\n",
            " n04591157 - Windsor_tie  10.80%\n",
            " n02786058 - Band_Aid     6.55%\n",
            "\n",
            "Image #225\n",
            " n04591157 - Windsor_tie  15.86%\n",
            " n04357314 - sunscreen    12.22%\n",
            " n02786058 - Band_Aid     6.30%\n",
            "\n",
            "Image #226\n",
            " n02786058 - Band_Aid     25.97%\n",
            " n03970156 - plunger      8.12%\n",
            " n04591157 - Windsor_tie  7.09%\n",
            "\n",
            "Image #227\n",
            " n04591157 - Windsor_tie  18.15%\n",
            " n03970156 - plunger      11.05%\n",
            " n02786058 - Band_Aid     4.36%\n",
            "\n",
            "Image #228\n",
            " n03485407 - hand-held_computer 17.90%\n",
            " n02786058 - Band_Aid     16.40%\n",
            " n04591157 - Windsor_tie  9.81%\n",
            "\n",
            "Image #229\n",
            " n03970156 - plunger      25.81%\n",
            " n03250847 - drumstick    25.64%\n",
            " n04270147 - spatula      10.35%\n",
            "\n",
            "Image #230\n",
            " n04372370 - switch       25.80%\n",
            " n04254120 - soap_dispenser 20.75%\n",
            " n02786058 - Band_Aid     7.34%\n",
            "\n",
            "Image #231\n",
            " n04372370 - switch       29.88%\n",
            " n03825788 - nipple       19.91%\n",
            " n03970156 - plunger      6.38%\n",
            "\n",
            "Image #232\n",
            " n04591157 - Windsor_tie  18.58%\n",
            " n02786058 - Band_Aid     10.90%\n",
            " n03970156 - plunger      7.82%\n",
            "\n",
            "Image #233\n",
            " n03250847 - drumstick    27.83%\n",
            " n03970156 - plunger      15.71%\n",
            " n04357314 - sunscreen    10.70%\n",
            "\n",
            "Image #234\n",
            " n02786058 - Band_Aid     17.12%\n",
            " n04591157 - Windsor_tie  14.41%\n",
            " n04357314 - sunscreen    10.49%\n",
            "\n",
            "Image #235\n",
            " n03970156 - plunger      20.35%\n",
            " n04591157 - Windsor_tie  15.08%\n",
            " n03250847 - drumstick    9.20%\n",
            "\n",
            "Image #236\n",
            " n02786058 - Band_Aid     25.33%\n",
            " n04591157 - Windsor_tie  15.04%\n",
            " n04357314 - sunscreen    13.99%\n",
            "\n",
            "Image #237\n",
            " n03250847 - drumstick    42.63%\n",
            " n03970156 - plunger      9.43%\n",
            " n02951585 - can_opener   7.22%\n",
            "\n",
            "Image #238\n",
            " n02786058 - Band_Aid     50.69%\n",
            " n03250847 - drumstick    8.69%\n",
            " n03658185 - letter_opener 4.75%\n",
            "\n",
            "Image #239\n",
            " n04357314 - sunscreen    23.34%\n",
            " n02786058 - Band_Aid     14.91%\n",
            " n02951585 - can_opener   9.75%\n",
            "\n",
            "Image #240\n",
            " n03970156 - plunger      52.75%\n",
            " n03250847 - drumstick    16.05%\n",
            " n02786058 - Band_Aid     6.78%\n",
            "\n",
            "Image #241\n",
            " n04372370 - switch       40.01%\n",
            " n02951585 - can_opener   11.79%\n",
            " n04591157 - Windsor_tie  9.68%\n",
            "\n",
            "Image #242\n",
            " n02786058 - Band_Aid     35.44%\n",
            " n04357314 - sunscreen    14.32%\n",
            " n04591157 - Windsor_tie  13.04%\n",
            "\n",
            "Image #243\n",
            " n02786058 - Band_Aid     11.47%\n",
            " n04357314 - sunscreen    11.16%\n",
            " n04372370 - switch       9.74%\n",
            "\n",
            "Image #244\n",
            " n04372370 - switch       30.13%\n",
            " n02951585 - can_opener   11.76%\n",
            " n04254120 - soap_dispenser 10.79%\n",
            "\n",
            "Image #245\n",
            " n03970156 - plunger      20.96%\n",
            " n04591157 - Windsor_tie  18.42%\n",
            " n02786058 - Band_Aid     17.24%\n",
            "\n",
            "Image #246\n",
            " n04357314 - sunscreen    6.44%\n",
            " n04254120 - soap_dispenser 5.28%\n",
            " n04372370 - switch       5.03%\n",
            "\n",
            "Image #247\n",
            " n03250847 - drumstick    43.26%\n",
            " n03970156 - plunger      27.16%\n",
            " n02786058 - Band_Aid     6.68%\n",
            "\n",
            "Image #248\n",
            " n04357314 - sunscreen    28.53%\n",
            " n02786058 - Band_Aid     8.77%\n",
            " n04540053 - volleyball   8.30%\n",
            "\n",
            "Image #249\n",
            " n04591157 - Windsor_tie  25.96%\n",
            " n02786058 - Band_Aid     25.11%\n",
            " n04357314 - sunscreen    7.82%\n",
            "\n",
            "Image #250\n",
            " n02786058 - Band_Aid     35.40%\n",
            " n03970156 - plunger      12.57%\n",
            " n04372370 - switch       3.77%\n",
            "\n",
            "Image #251\n",
            " n04591157 - Windsor_tie  8.77%\n",
            " n02786058 - Band_Aid     8.13%\n",
            " n04270147 - spatula      7.96%\n",
            "\n",
            "Image #252\n",
            " n02786058 - Band_Aid     14.28%\n",
            " n04591157 - Windsor_tie  13.06%\n",
            " n04357314 - sunscreen    10.49%\n",
            "\n",
            "Image #253\n",
            " n02786058 - Band_Aid     12.10%\n",
            " n04372370 - switch       10.67%\n",
            " n04357314 - sunscreen    8.82%\n",
            "\n",
            "Image #254\n",
            " n03676483 - lipstick     12.89%\n",
            " n04254120 - soap_dispenser 11.24%\n",
            " n03825788 - nipple       7.20%\n",
            "\n",
            "Image #255\n",
            " n02786058 - Band_Aid     14.18%\n",
            " n04357314 - sunscreen    13.85%\n",
            " n04591157 - Windsor_tie  11.12%\n",
            "\n",
            "Image #256\n",
            " n04254120 - soap_dispenser 22.58%\n",
            " n04591157 - Windsor_tie  17.08%\n",
            " n02786058 - Band_Aid     15.30%\n",
            "\n",
            "Image #257\n",
            " n02786058 - Band_Aid     56.07%\n",
            " n04357314 - sunscreen    6.95%\n",
            " n03250847 - drumstick    3.01%\n",
            "\n",
            "Image #258\n",
            " n03970156 - plunger      22.42%\n",
            " n04357314 - sunscreen    7.87%\n",
            " n04270147 - spatula      7.01%\n",
            "\n",
            "Image #259\n",
            " n04591157 - Windsor_tie  21.51%\n",
            " n02786058 - Band_Aid     13.06%\n",
            " n04357314 - sunscreen    7.62%\n",
            "\n",
            "Image #260\n",
            " n04357314 - sunscreen    19.30%\n",
            " n04591157 - Windsor_tie  7.66%\n",
            " n02808440 - bathtub      7.16%\n",
            "\n",
            "Image #261\n",
            " n03970156 - plunger      30.98%\n",
            " n04270147 - spatula      12.38%\n",
            " n04591157 - Windsor_tie  10.20%\n",
            "\n",
            "Image #262\n",
            " n04357314 - sunscreen    34.58%\n",
            " n02786058 - Band_Aid     10.76%\n",
            " n04591157 - Windsor_tie  8.09%\n",
            "\n",
            "Image #263\n",
            " n02786058 - Band_Aid     36.18%\n",
            " n03970156 - plunger      12.46%\n",
            " n04591157 - Windsor_tie  11.42%\n",
            "\n",
            "Image #264\n",
            " n02786058 - Band_Aid     7.90%\n",
            " n04372370 - switch       7.57%\n",
            " n03970156 - plunger      6.67%\n",
            "\n",
            "Image #265\n",
            " n04591157 - Windsor_tie  17.81%\n",
            " n03970156 - plunger      11.26%\n",
            " n04254120 - soap_dispenser 8.81%\n",
            "\n",
            "Image #266\n",
            " n04372370 - switch       52.37%\n",
            " n02786058 - Band_Aid     12.70%\n",
            " n03485407 - hand-held_computer 8.53%\n",
            "\n",
            "Image #267\n",
            " n04372370 - switch       21.88%\n",
            " n03476991 - hair_spray   13.61%\n",
            " n03825788 - nipple       7.04%\n",
            "\n",
            "Image #268\n",
            " n03970156 - plunger      22.87%\n",
            " n04591157 - Windsor_tie  17.48%\n",
            " n02786058 - Band_Aid     9.81%\n",
            "\n",
            "Image #269\n",
            " n04591157 - Windsor_tie  34.73%\n",
            " n02786058 - Band_Aid     14.44%\n",
            " n04372370 - switch       11.01%\n",
            "\n",
            "Image #270\n",
            " n02786058 - Band_Aid     32.63%\n",
            " n04357314 - sunscreen    15.63%\n",
            " n03970156 - plunger      7.48%\n",
            "\n",
            "Image #271\n",
            " n04591157 - Windsor_tie  27.66%\n",
            " n04372370 - switch       24.93%\n",
            " n02786058 - Band_Aid     8.02%\n",
            "\n",
            "Image #272\n",
            " n04372370 - switch       32.42%\n",
            " n04254120 - soap_dispenser 13.26%\n",
            " n03825788 - nipple       5.81%\n",
            "\n",
            "Image #273\n",
            " n04591157 - Windsor_tie  11.68%\n",
            " n03970156 - plunger      11.42%\n",
            " n04270147 - spatula      10.72%\n",
            "\n",
            "Image #274\n",
            " n04357314 - sunscreen    31.98%\n",
            " n04591157 - Windsor_tie  15.09%\n",
            " n02786058 - Band_Aid     12.96%\n",
            "\n",
            "Image #275\n",
            " n04357314 - sunscreen    18.00%\n",
            " n04591157 - Windsor_tie  16.91%\n",
            " n04209239 - shower_curtain 5.21%\n",
            "\n",
            "Image #276\n",
            " n02786058 - Band_Aid     78.23%\n",
            " n04357314 - sunscreen    5.68%\n",
            " n04591157 - Windsor_tie  1.64%\n",
            "\n",
            "Image #277\n",
            " n04270147 - spatula      12.17%\n",
            " n02786058 - Band_Aid     10.14%\n",
            " n03970156 - plunger      9.76%\n",
            "\n",
            "Image #278\n",
            " n04591157 - Windsor_tie  29.42%\n",
            " n02786058 - Band_Aid     15.21%\n",
            " n03250847 - drumstick    11.60%\n",
            "\n",
            "Image #279\n",
            " n02786058 - Band_Aid     23.82%\n",
            " n04591157 - Windsor_tie  16.46%\n",
            " n04357314 - sunscreen    7.64%\n",
            "\n",
            "Image #280\n",
            " n03250847 - drumstick    32.25%\n",
            " n02786058 - Band_Aid     14.15%\n",
            " n04357314 - sunscreen    5.64%\n",
            "\n",
            "Image #281\n",
            " n03970156 - plunger      34.00%\n",
            " n03250847 - drumstick    32.99%\n",
            " n02786058 - Band_Aid     10.39%\n",
            "\n",
            "Image #282\n",
            " n04372370 - switch       30.06%\n",
            " n04591157 - Windsor_tie  23.07%\n",
            " n02786058 - Band_Aid     6.20%\n",
            "\n",
            "Image #283\n",
            " n03970156 - plunger      21.06%\n",
            " n04372370 - switch       17.48%\n",
            " n03876231 - paintbrush   4.91%\n",
            "\n",
            "Image #284\n",
            " n03970156 - plunger      7.60%\n",
            " n04591157 - Windsor_tie  6.62%\n",
            " n04357314 - sunscreen    4.89%\n",
            "\n",
            "Image #285\n",
            " n04357314 - sunscreen    16.45%\n",
            " n02786058 - Band_Aid     11.59%\n",
            " n04591157 - Windsor_tie  7.40%\n",
            "\n",
            "Image #286\n",
            " n03840681 - ocarina      17.41%\n",
            " n04591157 - Windsor_tie  10.31%\n",
            " n04579432 - whistle      6.91%\n",
            "\n",
            "Image #287\n",
            " n04591157 - Windsor_tie  53.17%\n",
            " n04372370 - switch       12.40%\n",
            " n02786058 - Band_Aid     5.52%\n",
            "\n",
            "Image #288\n",
            " n03970156 - plunger      38.31%\n",
            " n03250847 - drumstick    7.26%\n",
            " n02786058 - Band_Aid     6.03%\n",
            "\n",
            "Image #289\n",
            " n02786058 - Band_Aid     22.14%\n",
            " n03250847 - drumstick    16.58%\n",
            " n04357314 - sunscreen    5.88%\n",
            "\n",
            "Image #290\n",
            " n02786058 - Band_Aid     21.19%\n",
            " n04357314 - sunscreen    8.92%\n",
            " n03250847 - drumstick    8.42%\n",
            "\n",
            "Image #291\n",
            " n04372370 - switch       11.96%\n",
            " n04254120 - soap_dispenser 8.96%\n",
            " n02951585 - can_opener   8.71%\n",
            "\n",
            "Image #292\n",
            " n02786058 - Band_Aid     26.86%\n",
            " n04372370 - switch       13.11%\n",
            " n04591157 - Windsor_tie  8.10%\n",
            "\n",
            "Image #293\n",
            " n02786058 - Band_Aid     32.95%\n",
            " n03250847 - drumstick    10.87%\n",
            " n04591157 - Windsor_tie  9.79%\n",
            "\n",
            "Image #294\n",
            " n04357314 - sunscreen    20.87%\n",
            " n02786058 - Band_Aid     10.74%\n",
            " n04540053 - volleyball   5.01%\n",
            "\n",
            "Image #295\n",
            " n02786058 - Band_Aid     38.80%\n",
            " n04357314 - sunscreen    13.48%\n",
            " n03314780 - face_powder  4.95%\n",
            "\n",
            "Image #296\n",
            " n04372370 - switch       30.64%\n",
            " n04254120 - soap_dispenser 6.73%\n",
            " n04591157 - Windsor_tie  6.00%\n",
            "\n",
            "Image #297\n",
            " n03970156 - plunger      25.96%\n",
            " n04591157 - Windsor_tie  25.12%\n",
            " n02786058 - Band_Aid     8.21%\n",
            "\n",
            "Image #298\n",
            " n04591157 - Windsor_tie  26.96%\n",
            " n03250847 - drumstick    17.54%\n",
            " n03970156 - plunger      8.31%\n",
            "\n",
            "Image #299\n",
            " n04591157 - Windsor_tie  51.10%\n",
            " n04357314 - sunscreen    9.31%\n",
            " n04270147 - spatula      7.73%\n",
            "\n",
            "Image #300\n",
            " n04372370 - switch       16.93%\n",
            " n04493381 - tub          7.28%\n",
            " n02808440 - bathtub      6.11%\n",
            "\n",
            "Image #301\n",
            " n04372370 - switch       37.32%\n",
            " n04591157 - Windsor_tie  15.78%\n",
            " n02786058 - Band_Aid     14.44%\n",
            "\n",
            "Image #302\n",
            " n04372370 - switch       25.63%\n",
            " n02951585 - can_opener   17.67%\n",
            " n04254120 - soap_dispenser 7.11%\n",
            "\n",
            "Image #303\n",
            " n04591157 - Windsor_tie  14.17%\n",
            " n02786058 - Band_Aid     13.03%\n",
            " n02951585 - can_opener   7.31%\n",
            "\n",
            "Image #304\n",
            " n02786058 - Band_Aid     18.41%\n",
            " n04357314 - sunscreen    16.16%\n",
            " n03970156 - plunger      10.74%\n",
            "\n",
            "Image #305\n",
            " n02786058 - Band_Aid     17.67%\n",
            " n04357314 - sunscreen    15.85%\n",
            " n04591157 - Windsor_tie  6.90%\n",
            "\n",
            "Image #306\n",
            " n04591157 - Windsor_tie  26.96%\n",
            " n04357314 - sunscreen    11.50%\n",
            " n04270147 - spatula      5.49%\n",
            "\n",
            "Image #307\n",
            " n04591157 - Windsor_tie  36.26%\n",
            " n02786058 - Band_Aid     8.97%\n",
            " n03970156 - plunger      8.39%\n",
            "\n",
            "Image #308\n",
            " n02951585 - can_opener   18.01%\n",
            " n04372370 - switch       16.61%\n",
            " n04357314 - sunscreen    8.63%\n",
            "\n",
            "Image #309\n",
            " n04357314 - sunscreen    26.41%\n",
            " n02786058 - Band_Aid     14.90%\n",
            " n03314780 - face_powder  3.54%\n",
            "\n",
            "Image #310\n",
            " n04372370 - switch       11.38%\n",
            " n03793489 - mouse        6.49%\n",
            " n02808440 - bathtub      6.19%\n",
            "\n",
            "Image #311\n",
            " n02786058 - Band_Aid     16.88%\n",
            " n04591157 - Windsor_tie  11.18%\n",
            " n04357314 - sunscreen    10.80%\n",
            "\n",
            "Image #312\n",
            " n03970156 - plunger      35.55%\n",
            " n03250847 - drumstick    23.92%\n",
            " n02786058 - Band_Aid     3.77%\n",
            "\n",
            "Image #313\n",
            " n04357314 - sunscreen    23.14%\n",
            " n04591157 - Windsor_tie  9.21%\n",
            " n02786058 - Band_Aid     6.31%\n",
            "\n",
            "Image #314\n",
            " n02786058 - Band_Aid     54.89%\n",
            " n03970156 - plunger      10.99%\n",
            " n04591157 - Windsor_tie  4.44%\n",
            "\n",
            "Image #315\n",
            " n04372370 - switch       52.37%\n",
            " n02786058 - Band_Aid     12.70%\n",
            " n03485407 - hand-held_computer 8.53%\n",
            "\n",
            "Image #316\n",
            " n02786058 - Band_Aid     44.81%\n",
            " n04357314 - sunscreen    7.72%\n",
            " n03314780 - face_powder  3.93%\n",
            "\n",
            "Image #317\n",
            " n04591157 - Windsor_tie  35.66%\n",
            " n02786058 - Band_Aid     13.22%\n",
            " n03250847 - drumstick    8.40%\n",
            "\n",
            "Image #318\n",
            " n04372370 - switch       44.96%\n",
            " n02951585 - can_opener   9.79%\n",
            " n04254120 - soap_dispenser 9.68%\n",
            "\n",
            "Image #319\n",
            " n03970156 - plunger      36.11%\n",
            " n03786901 - mortar       7.72%\n",
            " n04591157 - Windsor_tie  6.92%\n",
            "\n",
            "Image #320\n",
            " n04591157 - Windsor_tie  37.26%\n",
            " n02786058 - Band_Aid     14.44%\n",
            " n04357314 - sunscreen    11.73%\n",
            "\n",
            "Image #321\n",
            " n04357314 - sunscreen    19.02%\n",
            " n03970156 - plunger      16.00%\n",
            " n02786058 - Band_Aid     11.87%\n",
            "\n",
            "Image #322\n",
            " n04357314 - sunscreen    23.33%\n",
            " n03970156 - plunger      10.76%\n",
            " n03250847 - drumstick    10.55%\n",
            "\n",
            "Image #323\n",
            " n03970156 - plunger      40.21%\n",
            " n03250847 - drumstick    11.02%\n",
            " n04270147 - spatula      6.85%\n",
            "\n",
            "Image #324\n",
            " n02786058 - Band_Aid     21.31%\n",
            " n04591157 - Windsor_tie  8.96%\n",
            " n03970156 - plunger      8.04%\n",
            "\n",
            "Image #325\n",
            " n04357314 - sunscreen    17.09%\n",
            " n02786058 - Band_Aid     9.18%\n",
            " n03690938 - lotion       7.42%\n",
            "\n",
            "Image #326\n",
            " n02786058 - Band_Aid     16.12%\n",
            " n03250847 - drumstick    10.23%\n",
            " n04357314 - sunscreen    6.98%\n",
            "\n",
            "Image #327\n",
            " n02786058 - Band_Aid     20.52%\n",
            " n03250847 - drumstick    20.42%\n",
            " n04357314 - sunscreen    8.25%\n",
            "\n",
            "Image #328\n",
            " n04372370 - switch       44.78%\n",
            " n02786058 - Band_Aid     11.27%\n",
            " n04254120 - soap_dispenser 4.47%\n",
            "\n",
            "Image #329\n",
            " n02786058 - Band_Aid     50.66%\n",
            " n02951585 - can_opener   7.96%\n",
            " n04254120 - soap_dispenser 4.00%\n",
            "\n",
            "Image #330\n",
            " n02786058 - Band_Aid     22.37%\n",
            " n03970156 - plunger      21.01%\n",
            " n03250847 - drumstick    10.96%\n",
            "\n",
            "Image #331\n",
            " n03476991 - hair_spray   15.29%\n",
            " n04372370 - switch       8.73%\n",
            " n04254120 - soap_dispenser 6.84%\n",
            "\n",
            "Image #332\n",
            " n04372370 - switch       19.94%\n",
            " n02786058 - Band_Aid     11.05%\n",
            " n02951585 - can_opener   10.56%\n",
            "\n",
            "Image #333\n",
            " n03970156 - plunger      19.25%\n",
            " n02786058 - Band_Aid     7.37%\n",
            " n02951585 - can_opener   4.47%\n",
            "\n",
            "Image #334\n",
            " n02786058 - Band_Aid     14.94%\n",
            " n04357314 - sunscreen    8.66%\n",
            " n04591157 - Windsor_tie  8.02%\n",
            "\n",
            "Image #335\n",
            " n02786058 - Band_Aid     19.11%\n",
            " n03970156 - plunger      16.50%\n",
            " n03250847 - drumstick    5.15%\n",
            "\n",
            "Image #336\n",
            " n02786058 - Band_Aid     51.43%\n",
            " n04357314 - sunscreen    10.61%\n",
            " n03314780 - face_powder  2.12%\n",
            "\n",
            "Image #337\n",
            " n03250847 - drumstick    17.48%\n",
            " n03970156 - plunger      12.95%\n",
            " n02786058 - Band_Aid     10.35%\n",
            "\n",
            "Image #338\n",
            " n03970156 - plunger      19.19%\n",
            " n04591157 - Windsor_tie  10.89%\n",
            " n02786058 - Band_Aid     10.37%\n",
            "\n",
            "Image #339\n",
            " n02786058 - Band_Aid     58.04%\n",
            " n04357314 - sunscreen    4.74%\n",
            " n04591157 - Windsor_tie  4.13%\n",
            "\n",
            "Image #340\n",
            " n04591157 - Windsor_tie  15.36%\n",
            " n02786058 - Band_Aid     15.17%\n",
            " n04357314 - sunscreen    14.03%\n",
            "\n",
            "Image #341\n",
            " n03250847 - drumstick    18.02%\n",
            " n02786058 - Band_Aid     8.57%\n",
            " n03970156 - plunger      6.59%\n",
            "\n",
            "Image #342\n",
            " n03250847 - drumstick    7.78%\n",
            " n04357314 - sunscreen    6.23%\n",
            " n02786058 - Band_Aid     5.38%\n",
            "\n",
            "Image #343\n",
            " n04357314 - sunscreen    16.98%\n",
            " n04591157 - Windsor_tie  16.31%\n",
            " n02786058 - Band_Aid     8.79%\n",
            "\n",
            "Image #344\n",
            " n02786058 - Band_Aid     16.36%\n",
            " n04357314 - sunscreen    10.21%\n",
            " n04591157 - Windsor_tie  8.84%\n",
            "\n",
            "Image #345\n",
            " n03250847 - drumstick    40.93%\n",
            " n03970156 - plunger      15.46%\n",
            " n04591157 - Windsor_tie  7.80%\n",
            "\n",
            "Image #346\n",
            " n03476991 - hair_spray   14.30%\n",
            " n04591157 - Windsor_tie  12.08%\n",
            " n04254120 - soap_dispenser 8.57%\n",
            "\n",
            "Image #347\n",
            " n03250847 - drumstick    13.24%\n",
            " n04591157 - Windsor_tie  10.80%\n",
            " n02951585 - can_opener   9.93%\n",
            "\n",
            "Image #348\n",
            " n03970156 - plunger      18.47%\n",
            " n03250847 - drumstick    16.88%\n",
            " n04591157 - Windsor_tie  14.88%\n",
            "\n",
            "Image #349\n",
            " n04357314 - sunscreen    12.69%\n",
            " n02786058 - Band_Aid     10.67%\n",
            " n04254120 - soap_dispenser 8.08%\n",
            "\n",
            "Image #350\n",
            " n04357314 - sunscreen    32.53%\n",
            " n02786058 - Band_Aid     13.61%\n",
            " n04591157 - Windsor_tie  8.70%\n",
            "\n",
            "Image #351\n",
            " n04372370 - switch       29.36%\n",
            " n02786058 - Band_Aid     12.07%\n",
            " n04591157 - Windsor_tie  10.37%\n",
            "\n",
            "Image #352\n",
            " n04591157 - Windsor_tie  20.47%\n",
            " n04357314 - sunscreen    13.28%\n",
            " n02786058 - Band_Aid     13.04%\n",
            "\n",
            "Image #353\n",
            " n03970156 - plunger      24.93%\n",
            " n03250847 - drumstick    16.38%\n",
            " n04591157 - Windsor_tie  11.48%\n",
            "\n",
            "Image #354\n",
            " n03970156 - plunger      21.23%\n",
            " n04591157 - Windsor_tie  10.02%\n",
            " n04372370 - switch       8.68%\n",
            "\n",
            "Image #355\n",
            " n03250847 - drumstick    14.87%\n",
            " n03970156 - plunger      13.78%\n",
            " n02786058 - Band_Aid     11.88%\n",
            "\n",
            "Image #356\n",
            " n03970156 - plunger      47.76%\n",
            " n03250847 - drumstick    12.09%\n",
            " n04270147 - spatula      7.72%\n",
            "\n",
            "Image #357\n",
            " n04372370 - switch       31.33%\n",
            " n04254120 - soap_dispenser 17.25%\n",
            " n02951585 - can_opener   15.71%\n",
            "\n",
            "Image #358\n",
            " n02786058 - Band_Aid     19.24%\n",
            " n04591157 - Windsor_tie  17.63%\n",
            " n04357314 - sunscreen    6.67%\n",
            "\n",
            "Image #359\n",
            " n02786058 - Band_Aid     20.73%\n",
            " n04357314 - sunscreen    12.23%\n",
            " n03970156 - plunger      6.27%\n",
            "\n",
            "Image #360\n",
            " n04357314 - sunscreen    46.36%\n",
            " n02786058 - Band_Aid     19.56%\n",
            " n04591157 - Windsor_tie  2.38%\n",
            "\n",
            "Image #361\n",
            " n04372370 - switch       24.71%\n",
            " n02786058 - Band_Aid     23.08%\n",
            " n04357314 - sunscreen    15.55%\n",
            "\n",
            "Image #362\n",
            " n02786058 - Band_Aid     64.19%\n",
            " n04591157 - Windsor_tie  4.09%\n",
            " n04357314 - sunscreen    2.85%\n",
            "\n",
            "Image #363\n",
            " n04372370 - switch       37.69%\n",
            " n04591157 - Windsor_tie  12.86%\n",
            " n02786058 - Band_Aid     9.98%\n",
            "\n",
            "Image #364\n",
            " n04591157 - Windsor_tie  32.42%\n",
            " n04357314 - sunscreen    19.04%\n",
            " n02786058 - Band_Aid     5.24%\n",
            "\n",
            "Image #365\n",
            " n03970156 - plunger      21.06%\n",
            " n04372370 - switch       17.48%\n",
            " n03876231 - paintbrush   4.91%\n",
            "\n",
            "Image #366\n",
            " n02786058 - Band_Aid     24.43%\n",
            " n04357314 - sunscreen    19.71%\n",
            " n03314780 - face_powder  6.98%\n",
            "\n",
            "Image #367\n",
            " n04591157 - Windsor_tie  40.66%\n",
            " n02786058 - Band_Aid     11.21%\n",
            " n03970156 - plunger      4.29%\n",
            "\n",
            "Image #368\n",
            " n04357314 - sunscreen    40.48%\n",
            " n02786058 - Band_Aid     10.20%\n",
            " n03314780 - face_powder  9.74%\n",
            "\n",
            "Image #369\n",
            " n04357314 - sunscreen    14.88%\n",
            " n02786058 - Band_Aid     8.92%\n",
            " n04591157 - Windsor_tie  8.76%\n",
            "\n",
            "Image #370\n",
            " n02786058 - Band_Aid     26.15%\n",
            " n04591157 - Windsor_tie  15.88%\n",
            " n04372370 - switch       7.12%\n",
            "\n",
            "Image #371\n",
            " n03250847 - drumstick    25.03%\n",
            " n02786058 - Band_Aid     16.95%\n",
            " n03970156 - plunger      11.98%\n",
            "\n",
            "Image #372\n",
            " n02786058 - Band_Aid     32.39%\n",
            " n04357314 - sunscreen    11.34%\n",
            " n03314780 - face_powder  6.42%\n",
            "\n",
            "Image #373\n",
            " n04357314 - sunscreen    15.10%\n",
            " n02786058 - Band_Aid     10.97%\n",
            " n04591157 - Windsor_tie  9.81%\n",
            "\n",
            "Image #374\n",
            " n02786058 - Band_Aid     67.88%\n",
            " n04591157 - Windsor_tie  9.59%\n",
            " n03250847 - drumstick    3.92%\n",
            "\n",
            "Image #375\n",
            " n04591157 - Windsor_tie  23.59%\n",
            " n03970156 - plunger      11.05%\n",
            " n04357314 - sunscreen    5.31%\n",
            "\n",
            "Image #376\n",
            " n03970156 - plunger      61.54%\n",
            " n03250847 - drumstick    11.91%\n",
            " n02786058 - Band_Aid     5.74%\n",
            "\n",
            "Image #377\n",
            " n04357314 - sunscreen    14.50%\n",
            " n02786058 - Band_Aid     8.14%\n",
            " n04591157 - Windsor_tie  5.58%\n",
            "\n",
            "Image #378\n",
            " n02786058 - Band_Aid     31.49%\n",
            " n04357314 - sunscreen    17.46%\n",
            " n03314780 - face_powder  4.73%\n",
            "\n",
            "Image #379\n",
            " n03250847 - drumstick    35.99%\n",
            " n02786058 - Band_Aid     23.34%\n",
            " n04591157 - Windsor_tie  7.49%\n",
            "\n",
            "Image #380\n",
            " n04254120 - soap_dispenser 31.55%\n",
            " n04372370 - switch       19.17%\n",
            " n04591157 - Windsor_tie  6.60%\n",
            "\n",
            "Image #381\n",
            " n02786058 - Band_Aid     38.41%\n",
            " n04591157 - Windsor_tie  4.96%\n",
            " n04357314 - sunscreen    4.84%\n",
            "\n",
            "Image #382\n",
            " n04357314 - sunscreen    15.29%\n",
            " n04270147 - spatula      10.07%\n",
            " n02786058 - Band_Aid     9.69%\n",
            "\n",
            "Image #383\n",
            " n02786058 - Band_Aid     17.84%\n",
            " n04372370 - switch       7.77%\n",
            " n03970156 - plunger      6.12%\n",
            "\n",
            "Image #384\n",
            " n02786058 - Band_Aid     22.46%\n",
            " n04254120 - soap_dispenser 6.31%\n",
            " n04591157 - Windsor_tie  5.91%\n",
            "\n",
            "Image #385\n",
            " n04591157 - Windsor_tie  29.53%\n",
            " n02786058 - Band_Aid     14.89%\n",
            " n04254120 - soap_dispenser 11.36%\n",
            "\n",
            "Image #386\n",
            " n04591157 - Windsor_tie  21.98%\n",
            " n03250847 - drumstick    9.15%\n",
            " n02786058 - Band_Aid     7.05%\n",
            "\n",
            "Image #387\n",
            " n03250847 - drumstick    19.53%\n",
            " n02786058 - Band_Aid     9.11%\n",
            " n03876231 - paintbrush   5.85%\n",
            "\n",
            "Image #388\n",
            " n03250847 - drumstick    26.41%\n",
            " n02786058 - Band_Aid     17.93%\n",
            " n03970156 - plunger      16.54%\n",
            "\n",
            "Image #389\n",
            " n03970156 - plunger      29.28%\n",
            " n03250847 - drumstick    13.27%\n",
            " n02786058 - Band_Aid     8.91%\n",
            "\n",
            "Image #390\n",
            " n02786058 - Band_Aid     15.20%\n",
            " n04591157 - Windsor_tie  10.10%\n",
            " n04254120 - soap_dispenser 10.10%\n",
            "\n",
            "Image #391\n",
            " n04591157 - Windsor_tie  49.61%\n",
            " n03250847 - drumstick    6.56%\n",
            " n02786058 - Band_Aid     4.90%\n",
            "\n",
            "Image #392\n",
            " n04591157 - Windsor_tie  64.72%\n",
            " n04372370 - switch       6.35%\n",
            " n03970156 - plunger      3.98%\n",
            "\n",
            "Image #393\n",
            " n02786058 - Band_Aid     23.67%\n",
            " n04357314 - sunscreen    9.37%\n",
            " n04591157 - Windsor_tie  5.85%\n",
            "\n",
            "Image #394\n",
            " n03970156 - plunger      42.35%\n",
            " n04591157 - Windsor_tie  9.31%\n",
            " n03250847 - drumstick    8.53%\n",
            "\n",
            "Image #395\n",
            " n03970156 - plunger      44.71%\n",
            " n03250847 - drumstick    16.35%\n",
            " n02786058 - Band_Aid     5.44%\n",
            "\n",
            "Image #396\n",
            " n04372370 - switch       28.31%\n",
            " n04357314 - sunscreen    11.89%\n",
            " n02951585 - can_opener   9.97%\n",
            "\n",
            "Image #397\n",
            " n04372370 - switch       32.48%\n",
            " n04591157 - Windsor_tie  11.92%\n",
            " n02786058 - Band_Aid     8.10%\n",
            "\n",
            "Image #398\n",
            " n03970156 - plunger      25.33%\n",
            " n02786058 - Band_Aid     11.18%\n",
            " n03250847 - drumstick    7.78%\n",
            "\n",
            "Image #399\n",
            " n04357314 - sunscreen    11.57%\n",
            " n02786058 - Band_Aid     9.17%\n",
            " n04591157 - Windsor_tie  9.09%\n",
            "\n",
            "Image #400\n",
            " n04591157 - Windsor_tie  37.24%\n",
            " n03970156 - plunger      8.90%\n",
            " n02951585 - can_opener   7.77%\n",
            "\n",
            "Image #401\n",
            " n04372370 - switch       19.89%\n",
            " n03314780 - face_powder  10.25%\n",
            " n04254120 - soap_dispenser 5.59%\n",
            "\n",
            "Image #402\n",
            " n04591157 - Windsor_tie  34.79%\n",
            " n02951585 - can_opener   16.36%\n",
            " n03485407 - hand-held_computer 7.38%\n",
            "\n",
            "Image #403\n",
            " n04591157 - Windsor_tie  29.57%\n",
            " n02786058 - Band_Aid     25.54%\n",
            " n03250847 - drumstick    6.76%\n",
            "\n",
            "Image #404\n",
            " n03207941 - dishwasher   65.99%\n",
            " n04554684 - washer       19.52%\n",
            " n04070727 - refrigerator 4.36%\n",
            "\n",
            "Image #405\n",
            " n03250847 - drumstick    23.64%\n",
            " n04591157 - Windsor_tie  19.33%\n",
            " n02786058 - Band_Aid     9.00%\n",
            "\n",
            "Image #406\n",
            " n04591157 - Windsor_tie  64.43%\n",
            " n02786058 - Band_Aid     8.37%\n",
            " n03970156 - plunger      6.65%\n",
            "\n",
            "Image #407\n",
            " n03825788 - nipple       8.53%\n",
            " n04254120 - soap_dispenser 8.49%\n",
            " n03970156 - plunger      7.45%\n",
            "\n",
            "Image #408\n",
            " n04591157 - Windsor_tie  18.12%\n",
            " n04357314 - sunscreen    14.23%\n",
            " n02786058 - Band_Aid     13.50%\n",
            "\n",
            "Image #409\n",
            " n04591157 - Windsor_tie  20.61%\n",
            " n04023962 - punching_bag 14.57%\n",
            " n02786058 - Band_Aid     4.51%\n",
            "\n",
            "Image #410\n",
            " n04357314 - sunscreen    12.24%\n",
            " n02786058 - Band_Aid     11.29%\n",
            " n04254120 - soap_dispenser 6.53%\n",
            "\n",
            "Image #411\n",
            " n04357314 - sunscreen    20.73%\n",
            " n02786058 - Band_Aid     11.32%\n",
            " n03314780 - face_powder  7.78%\n",
            "\n",
            "Image #412\n",
            " n02786058 - Band_Aid     17.44%\n",
            " n04372370 - switch       12.17%\n",
            " n04591157 - Windsor_tie  8.50%\n",
            "\n",
            "Image #413\n",
            " n03970156 - plunger      15.56%\n",
            " n04591157 - Windsor_tie  10.79%\n",
            " n02786058 - Band_Aid     7.01%\n",
            "\n",
            "Image #414\n",
            " n02786058 - Band_Aid     14.00%\n",
            " n03250847 - drumstick    12.70%\n",
            " n03970156 - plunger      8.61%\n",
            "\n",
            "Image #415\n",
            " n02786058 - Band_Aid     19.47%\n",
            " n03970156 - plunger      14.79%\n",
            " n03250847 - drumstick    11.36%\n",
            "\n",
            "Image #416\n",
            " n02786058 - Band_Aid     16.28%\n",
            " n03250847 - drumstick    16.12%\n",
            " n04357314 - sunscreen    13.77%\n",
            "\n",
            "Image #417\n",
            " n03250847 - drumstick    23.65%\n",
            " n03970156 - plunger      8.74%\n",
            " n04357314 - sunscreen    6.73%\n",
            "\n",
            "Image #418\n",
            " n03970156 - plunger      31.89%\n",
            " n03250847 - drumstick    17.57%\n",
            " n04591157 - Windsor_tie  9.72%\n",
            "\n",
            "Image #419\n",
            " n04591157 - Windsor_tie  13.71%\n",
            " n02786058 - Band_Aid     10.95%\n",
            " n04357314 - sunscreen    10.33%\n",
            "\n",
            "Image #420\n",
            " n04357314 - sunscreen    7.41%\n",
            " n02786058 - Band_Aid     7.20%\n",
            " n03970156 - plunger      7.13%\n",
            "\n",
            "Image #421\n",
            " n03250847 - drumstick    18.61%\n",
            " n02786058 - Band_Aid     14.71%\n",
            " n03485407 - hand-held_computer 7.38%\n",
            "\n",
            "Image #422\n",
            " n04357314 - sunscreen    12.52%\n",
            " n02786058 - Band_Aid     10.97%\n",
            " n04591157 - Windsor_tie  8.16%\n",
            "\n",
            "Image #423\n",
            " n02786058 - Band_Aid     27.54%\n",
            " n04357314 - sunscreen    10.34%\n",
            " n03250847 - drumstick    7.79%\n",
            "\n",
            "Image #424\n",
            " n04357314 - sunscreen    31.68%\n",
            " n02786058 - Band_Aid     11.65%\n",
            " n04591157 - Windsor_tie  7.35%\n",
            "\n",
            "Image #425\n",
            " n04357314 - sunscreen    18.20%\n",
            " n02786058 - Band_Aid     12.06%\n",
            " n04591157 - Windsor_tie  10.04%\n",
            "\n",
            "Image #426\n",
            " n04591157 - Windsor_tie  27.34%\n",
            " n02786058 - Band_Aid     22.65%\n",
            " n02951585 - can_opener   8.79%\n",
            "\n",
            "Image #427\n",
            " n04357314 - sunscreen    24.61%\n",
            " n04591157 - Windsor_tie  14.07%\n",
            " n02786058 - Band_Aid     8.45%\n",
            "\n",
            "Image #428\n",
            " n02786058 - Band_Aid     12.57%\n",
            " n04591157 - Windsor_tie  11.00%\n",
            " n04372370 - switch       8.25%\n",
            "\n",
            "Image #429\n",
            " n02951585 - can_opener   16.53%\n",
            " n04372370 - switch       8.18%\n",
            " n04591157 - Windsor_tie  7.02%\n",
            "\n",
            "Image #430\n",
            " n04591157 - Windsor_tie  53.55%\n",
            " n03970156 - plunger      6.85%\n",
            " n03720891 - maraca       6.30%\n",
            "\n",
            "Image #431\n",
            " n02786058 - Band_Aid     17.42%\n",
            " n04591157 - Windsor_tie  7.48%\n",
            " n03250847 - drumstick    4.54%\n",
            "\n",
            "Image #432\n",
            " n02786058 - Band_Aid     18.92%\n",
            " n04591157 - Windsor_tie  7.80%\n",
            " n03970156 - plunger      6.63%\n",
            "\n",
            "Image #433\n",
            " n02951585 - can_opener   15.57%\n",
            " n04372370 - switch       9.76%\n",
            " n04357314 - sunscreen    7.82%\n",
            "\n",
            "Image #434\n",
            " n04591157 - Windsor_tie  11.97%\n",
            " n02786058 - Band_Aid     9.75%\n",
            " n03970156 - plunger      6.91%\n",
            "\n",
            "Image #435\n",
            " n04254120 - soap_dispenser 26.34%\n",
            " n03476991 - hair_spray   11.19%\n",
            " n04372370 - switch       10.73%\n",
            "\n",
            "Image #436\n",
            " n02786058 - Band_Aid     31.68%\n",
            " n04357314 - sunscreen    10.55%\n",
            " n04591157 - Windsor_tie  5.35%\n",
            "\n",
            "Image #437\n",
            " n04591157 - Windsor_tie  33.30%\n",
            " n02786058 - Band_Aid     13.21%\n",
            " n04270147 - spatula      9.07%\n",
            "\n",
            "Image #438\n",
            " n04372370 - switch       41.69%\n",
            " n02786058 - Band_Aid     7.87%\n",
            " n03476991 - hair_spray   6.06%\n",
            "\n",
            "Image #439\n",
            " n04591157 - Windsor_tie  27.06%\n",
            " n02786058 - Band_Aid     11.71%\n",
            " n04357314 - sunscreen    9.20%\n",
            "\n",
            "Image #440\n",
            " n04591157 - Windsor_tie  29.51%\n",
            " n03970156 - plunger      13.71%\n",
            " n02786058 - Band_Aid     10.54%\n",
            "\n",
            "Image #441\n",
            " n04591157 - Windsor_tie  29.32%\n",
            " n02786058 - Band_Aid     14.71%\n",
            " n04357314 - sunscreen    7.86%\n",
            "\n",
            "Image #442\n",
            " n04591157 - Windsor_tie  16.48%\n",
            " n03250847 - drumstick    15.83%\n",
            " n04372370 - switch       8.35%\n",
            "\n",
            "Image #443\n",
            " n03250847 - drumstick    43.26%\n",
            " n03970156 - plunger      27.16%\n",
            " n02786058 - Band_Aid     6.68%\n",
            "\n",
            "Image #444\n",
            " n02786058 - Band_Aid     24.56%\n",
            " n03250847 - drumstick    13.10%\n",
            " n04372370 - switch       9.25%\n",
            "\n",
            "Image #445\n",
            " n02786058 - Band_Aid     74.92%\n",
            " n03250847 - drumstick    3.62%\n",
            " n04357314 - sunscreen    2.60%\n",
            "\n",
            "Image #446\n",
            " n04591157 - Windsor_tie  32.91%\n",
            " n04357314 - sunscreen    17.67%\n",
            " n02786058 - Band_Aid     5.03%\n",
            "\n",
            "Image #447\n",
            " n02786058 - Band_Aid     38.47%\n",
            " n04357314 - sunscreen    8.15%\n",
            " n03970156 - plunger      8.01%\n",
            "\n",
            "Image #448\n",
            " n02786058 - Band_Aid     17.82%\n",
            " n04372370 - switch       14.24%\n",
            " n04591157 - Windsor_tie  12.74%\n",
            "\n",
            "Image #449\n",
            " n02786058 - Band_Aid     19.80%\n",
            " n04357314 - sunscreen    9.32%\n",
            " n04591157 - Windsor_tie  8.61%\n",
            "\n",
            "Image #450\n",
            " n03970156 - plunger      40.64%\n",
            " n04270147 - spatula      5.97%\n",
            " n03498962 - hatchet      5.67%\n",
            "\n",
            "Image #451\n",
            " n02786058 - Band_Aid     76.57%\n",
            " n04591157 - Windsor_tie  14.51%\n",
            " n04372370 - switch       1.61%\n",
            "\n",
            "Image #452\n",
            " n04591157 - Windsor_tie  10.69%\n",
            " n04357314 - sunscreen    9.26%\n",
            " n02786058 - Band_Aid     7.98%\n",
            "\n",
            "Image #453\n",
            " n03250847 - drumstick    23.40%\n",
            " n03970156 - plunger      11.18%\n",
            " n02786058 - Band_Aid     8.17%\n",
            "\n",
            "Image #454\n",
            " n04270147 - spatula      31.55%\n",
            " n03970156 - plunger      10.67%\n",
            " n04591157 - Windsor_tie  8.20%\n",
            "\n",
            "Image #455\n",
            " n03970156 - plunger      12.61%\n",
            " n04270147 - spatula      8.05%\n",
            " n04357314 - sunscreen    6.24%\n",
            "\n",
            "Image #456\n",
            " n04357314 - sunscreen    12.04%\n",
            " n04270147 - spatula      9.68%\n",
            " n04591157 - Windsor_tie  8.97%\n",
            "\n",
            "Image #457\n",
            " n02786058 - Band_Aid     17.82%\n",
            " n04357314 - sunscreen    14.77%\n",
            " n03250847 - drumstick    8.73%\n",
            "\n",
            "Image #458\n",
            " n04357314 - sunscreen    42.16%\n",
            " n04591157 - Windsor_tie  9.38%\n",
            " n03250847 - drumstick    5.11%\n",
            "\n",
            "Image #459\n",
            " n04591157 - Windsor_tie  34.70%\n",
            " n02786058 - Band_Aid     7.36%\n",
            " n04357314 - sunscreen    6.82%\n",
            "\n",
            "Image #460\n",
            " n02951585 - can_opener   28.52%\n",
            " n04372370 - switch       11.42%\n",
            " n04254120 - soap_dispenser 9.22%\n",
            "\n",
            "Image #461\n",
            " n04357314 - sunscreen    16.45%\n",
            " n02786058 - Band_Aid     15.45%\n",
            " n04540053 - volleyball   7.79%\n",
            "\n",
            "Image #462\n",
            " n04591157 - Windsor_tie  30.06%\n",
            " n02786058 - Band_Aid     13.62%\n",
            " n03250847 - drumstick    12.18%\n",
            "\n",
            "Image #463\n",
            " n02786058 - Band_Aid     58.60%\n",
            " n04591157 - Windsor_tie  14.70%\n",
            " n04357314 - sunscreen    11.85%\n",
            "\n",
            "Image #464\n",
            " n04591157 - Windsor_tie  33.20%\n",
            " n03970156 - plunger      11.79%\n",
            " n02786058 - Band_Aid     6.55%\n",
            "\n",
            "Image #465\n",
            " n04357314 - sunscreen    9.50%\n",
            " n02786058 - Band_Aid     8.60%\n",
            " n04591157 - Windsor_tie  4.71%\n",
            "\n",
            "Image #466\n",
            " n04372370 - switch       32.42%\n",
            " n04254120 - soap_dispenser 13.26%\n",
            " n03825788 - nipple       5.81%\n",
            "\n",
            "Image #467\n",
            " n02786058 - Band_Aid     19.24%\n",
            " n04357314 - sunscreen    12.26%\n",
            " n04591157 - Windsor_tie  8.20%\n",
            "\n",
            "Image #468\n",
            " n04591157 - Windsor_tie  22.18%\n",
            " n03970156 - plunger      14.95%\n",
            " n04270147 - spatula      9.38%\n",
            "\n",
            "Image #469\n",
            " n03970156 - plunger      23.45%\n",
            " n03250847 - drumstick    10.11%\n",
            " n04270147 - spatula      8.14%\n",
            "\n",
            "Image #470\n",
            " n02786058 - Band_Aid     22.48%\n",
            " n04357314 - sunscreen    14.67%\n",
            " n03314780 - face_powder  4.22%\n",
            "\n",
            "Image #471\n",
            " n04372370 - switch       22.92%\n",
            " n04254120 - soap_dispenser 8.81%\n",
            " n04209239 - shower_curtain 5.66%\n",
            "\n",
            "Image #472\n",
            " n03250847 - drumstick    17.56%\n",
            " n04357314 - sunscreen    13.79%\n",
            " n02786058 - Band_Aid     7.41%\n",
            "\n",
            "Image #473\n",
            " n04591157 - Windsor_tie  36.13%\n",
            " n04357314 - sunscreen    6.95%\n",
            " n02808440 - bathtub      6.08%\n",
            "\n",
            "Image #474\n",
            " n04372370 - switch       33.57%\n",
            " n02951585 - can_opener   11.47%\n",
            " n04254120 - soap_dispenser 6.43%\n",
            "\n",
            "Image #475\n",
            " n04254120 - soap_dispenser 33.28%\n",
            " n02951585 - can_opener   8.38%\n",
            " n04372370 - switch       6.19%\n",
            "\n",
            "Image #476\n",
            " n03250847 - drumstick    26.99%\n",
            " n04591157 - Windsor_tie  10.54%\n",
            " n02786058 - Band_Aid     10.27%\n",
            "\n",
            "Image #477\n",
            " n04357314 - sunscreen    16.42%\n",
            " n02786058 - Band_Aid     15.87%\n",
            " n03970156 - plunger      12.75%\n",
            "\n",
            "Image #478\n",
            " n04591157 - Windsor_tie  64.72%\n",
            " n04372370 - switch       6.35%\n",
            " n03970156 - plunger      3.98%\n",
            "\n",
            "Image #479\n",
            " n03970156 - plunger      9.55%\n",
            " n04270147 - spatula      8.73%\n",
            " n03250847 - drumstick    8.51%\n",
            "\n",
            "Image #480\n",
            " n03476991 - hair_spray   10.85%\n",
            " n04372370 - switch       9.15%\n",
            " n04254120 - soap_dispenser 5.94%\n",
            "\n",
            "Image #481\n",
            " n02786058 - Band_Aid     15.47%\n",
            " n04591157 - Windsor_tie  6.73%\n",
            " n03250847 - drumstick    6.36%\n",
            "\n",
            "Image #482\n",
            " n02786058 - Band_Aid     18.46%\n",
            " n04357314 - sunscreen    17.99%\n",
            " n04591157 - Windsor_tie  5.50%\n",
            "\n",
            "Image #483\n",
            " n04357314 - sunscreen    10.67%\n",
            " n04591157 - Windsor_tie  9.45%\n",
            " n03876231 - paintbrush   6.68%\n",
            "\n",
            "Image #484\n",
            " n04357314 - sunscreen    37.44%\n",
            " n02786058 - Band_Aid     23.61%\n",
            " n03314780 - face_powder  3.38%\n",
            "\n",
            "Image #485\n",
            " n03970156 - plunger      24.55%\n",
            " n04270147 - spatula      12.86%\n",
            " n03250847 - drumstick    12.35%\n",
            "\n",
            "Image #486\n",
            " n03250847 - drumstick    18.68%\n",
            " n03970156 - plunger      13.64%\n",
            " n02786058 - Band_Aid     8.28%\n",
            "\n",
            "Image #487\n",
            " n04357314 - sunscreen    30.81%\n",
            " n02786058 - Band_Aid     22.35%\n",
            " n03314780 - face_powder  5.12%\n",
            "\n",
            "Image #488\n",
            " n04357314 - sunscreen    23.77%\n",
            " n04254120 - soap_dispenser 8.99%\n",
            " n02786058 - Band_Aid     6.95%\n",
            "\n",
            "Image #489\n",
            " n03250847 - drumstick    26.97%\n",
            " n03970156 - plunger      8.97%\n",
            " n02786058 - Band_Aid     6.26%\n",
            "\n",
            "Image #490\n",
            " n02786058 - Band_Aid     13.16%\n",
            " n04540053 - volleyball   12.56%\n",
            " n04357314 - sunscreen    12.38%\n",
            "\n",
            "Image #491\n",
            " n04357314 - sunscreen    16.43%\n",
            " n04591157 - Windsor_tie  14.84%\n",
            " n02786058 - Band_Aid     10.41%\n",
            "\n",
            "Image #492\n",
            " n03970156 - plunger      44.70%\n",
            " n03250847 - drumstick    12.02%\n",
            " n04270147 - spatula      5.13%\n",
            "\n",
            "Image #493\n",
            " n02786058 - Band_Aid     29.84%\n",
            " n04357314 - sunscreen    16.06%\n",
            " n03314780 - face_powder  3.98%\n",
            "\n",
            "Image #494\n",
            " n04591157 - Windsor_tie  37.02%\n",
            " n04372370 - switch       9.06%\n",
            " n02786058 - Band_Aid     7.47%\n",
            "\n",
            "Image #495\n",
            " n04357314 - sunscreen    15.47%\n",
            " n02786058 - Band_Aid     14.54%\n",
            " n04591157 - Windsor_tie  12.63%\n",
            "\n",
            "Image #496\n",
            " n02786058 - Band_Aid     23.03%\n",
            " n04357314 - sunscreen    18.77%\n",
            " n04591157 - Windsor_tie  6.51%\n",
            "\n",
            "Image #497\n",
            " n04372370 - switch       18.91%\n",
            " n02786058 - Band_Aid     8.71%\n",
            " n03825788 - nipple       8.16%\n",
            "\n",
            "Image #498\n",
            " n02786058 - Band_Aid     28.85%\n",
            " n04357314 - sunscreen    17.97%\n",
            " n04591157 - Windsor_tie  8.47%\n",
            "\n",
            "Image #499\n",
            " n04270147 - spatula      21.75%\n",
            " n03970156 - plunger      11.80%\n",
            " n04591157 - Windsor_tie  7.07%\n",
            "\n",
            "Image #500\n",
            " n02786058 - Band_Aid     15.07%\n",
            " n04357314 - sunscreen    12.03%\n",
            " n03250847 - drumstick    7.11%\n",
            "\n",
            "Image #501\n",
            " n02786058 - Band_Aid     25.13%\n",
            " n04357314 - sunscreen    18.43%\n",
            " n04591157 - Windsor_tie  15.28%\n",
            "\n",
            "Image #502\n",
            " n03970156 - plunger      28.10%\n",
            " n03498962 - hatchet      14.81%\n",
            " n03041632 - cleaver      9.79%\n",
            "\n",
            "Image #503\n",
            " n04357314 - sunscreen    26.40%\n",
            " n02786058 - Band_Aid     17.63%\n",
            " n03970156 - plunger      6.56%\n",
            "\n",
            "Image #504\n",
            " n04591157 - Windsor_tie  30.02%\n",
            " n04357314 - sunscreen    16.27%\n",
            " n02786058 - Band_Aid     7.69%\n",
            "\n",
            "Image #505\n",
            " n02786058 - Band_Aid     36.17%\n",
            " n04357314 - sunscreen    17.89%\n",
            " n02667093 - abaya        4.66%\n",
            "\n",
            "Image #506\n",
            " n04591157 - Windsor_tie  23.75%\n",
            " n02786058 - Band_Aid     23.37%\n",
            " n03250847 - drumstick    10.11%\n",
            "\n",
            "Image #507\n",
            " n04591157 - Windsor_tie  25.77%\n",
            " n04372370 - switch       18.01%\n",
            " n02786058 - Band_Aid     8.79%\n",
            "\n",
            "Image #508\n",
            " n02786058 - Band_Aid     36.29%\n",
            " n04357314 - sunscreen    20.76%\n",
            " n03314780 - face_powder  3.83%\n",
            "\n",
            "Image #509\n",
            " n03970156 - plunger      53.21%\n",
            " n03250847 - drumstick    14.29%\n",
            " n02786058 - Band_Aid     4.68%\n",
            "\n",
            "Image #510\n",
            " n02786058 - Band_Aid     28.08%\n",
            " n04357314 - sunscreen    22.55%\n",
            " n04254120 - soap_dispenser 3.73%\n",
            "\n",
            "Image #511\n",
            " n02786058 - Band_Aid     15.69%\n",
            " n04357314 - sunscreen    8.41%\n",
            " n03250847 - drumstick    5.53%\n",
            "\n",
            "Image #512\n",
            " n04357314 - sunscreen    12.37%\n",
            " n02786058 - Band_Aid     10.26%\n",
            " n04591157 - Windsor_tie  6.53%\n",
            "\n",
            "Image #513\n",
            " n04372370 - switch       39.60%\n",
            " n04254120 - soap_dispenser 8.72%\n",
            " n04591157 - Windsor_tie  7.59%\n",
            "\n",
            "Image #514\n",
            " n02786058 - Band_Aid     23.26%\n",
            " n03970156 - plunger      20.49%\n",
            " n04357314 - sunscreen    6.62%\n",
            "\n",
            "Image #515\n",
            " n03970156 - plunger      31.38%\n",
            " n03250847 - drumstick    19.20%\n",
            " n04270147 - spatula      15.05%\n",
            "\n",
            "Image #516\n",
            " n04591157 - Windsor_tie  36.72%\n",
            " n04270147 - spatula      23.27%\n",
            " n03970156 - plunger      3.15%\n",
            "\n",
            "Image #517\n",
            " n04591157 - Windsor_tie  13.44%\n",
            " n02786058 - Band_Aid     11.36%\n",
            " n02951585 - can_opener   7.31%\n",
            "\n",
            "Image #518\n",
            " n04357314 - sunscreen    13.24%\n",
            " n02786058 - Band_Aid     11.18%\n",
            " n04591157 - Windsor_tie  10.56%\n",
            "\n",
            "Image #519\n",
            " n02786058 - Band_Aid     19.59%\n",
            " n04591157 - Windsor_tie  8.36%\n",
            " n04357314 - sunscreen    7.02%\n",
            "\n",
            "Image #520\n",
            " n03970156 - plunger      51.34%\n",
            " n03250847 - drumstick    8.07%\n",
            " n04270147 - spatula      6.66%\n",
            "\n",
            "Image #521\n",
            " n04591157 - Windsor_tie  21.54%\n",
            " n04357314 - sunscreen    12.49%\n",
            " n03476991 - hair_spray   3.65%\n",
            "\n",
            "Image #522\n",
            " n04372370 - switch       20.22%\n",
            " n02786058 - Band_Aid     8.42%\n",
            " n03825788 - nipple       6.69%\n",
            "\n",
            "Image #523\n",
            " n04357314 - sunscreen    18.02%\n",
            " n02786058 - Band_Aid     17.65%\n",
            " n04540053 - volleyball   6.74%\n",
            "\n",
            "Image #524\n",
            " n02786058 - Band_Aid     43.72%\n",
            " n04372370 - switch       13.21%\n",
            " n04357314 - sunscreen    7.68%\n",
            "\n",
            "Image #525\n",
            " n04357314 - sunscreen    23.26%\n",
            " n02786058 - Band_Aid     16.61%\n",
            " n03314780 - face_powder  3.36%\n",
            "\n",
            "Image #526\n",
            " n02786058 - Band_Aid     50.47%\n",
            " n04357314 - sunscreen    10.25%\n",
            " n03970156 - plunger      3.16%\n",
            "\n",
            "Image #527\n",
            " n04357314 - sunscreen    21.60%\n",
            " n04591157 - Windsor_tie  13.56%\n",
            " n03314780 - face_powder  5.88%\n",
            "\n",
            "Image #528\n",
            " n04591157 - Windsor_tie  23.30%\n",
            " n04357314 - sunscreen    14.96%\n",
            " n02951585 - can_opener   7.73%\n",
            "\n",
            "Image #529\n",
            " n03250847 - drumstick    20.60%\n",
            " n04591157 - Windsor_tie  9.30%\n",
            " n04357314 - sunscreen    6.46%\n",
            "\n",
            "Image #530\n",
            " n04254120 - soap_dispenser 31.72%\n",
            " n02786058 - Band_Aid     11.87%\n",
            " n04357314 - sunscreen    10.22%\n",
            "\n",
            "Image #531\n",
            " n02786058 - Band_Aid     15.06%\n",
            " n04372370 - switch       13.10%\n",
            " n02951585 - can_opener   10.12%\n",
            "\n",
            "Image #532\n",
            " n03250847 - drumstick    25.71%\n",
            " n03970156 - plunger      11.41%\n",
            " n04591157 - Windsor_tie  5.17%\n",
            "\n",
            "Image #533\n",
            " n03970156 - plunger      27.99%\n",
            " n02786058 - Band_Aid     22.68%\n",
            " n04591157 - Windsor_tie  9.60%\n",
            "\n",
            "Image #534\n",
            " n04357314 - sunscreen    9.73%\n",
            " n04540053 - volleyball   7.34%\n",
            " n01675722 - banded_gecko 6.94%\n",
            "\n",
            "Image #535\n",
            " n03476991 - hair_spray   10.27%\n",
            " n04254120 - soap_dispenser 10.15%\n",
            " n04372370 - switch       8.39%\n",
            "\n",
            "Image #536\n",
            " n04270147 - spatula      11.09%\n",
            " n04591157 - Windsor_tie  10.56%\n",
            " n03250847 - drumstick    7.30%\n",
            "\n",
            "Image #537\n",
            " n03250847 - drumstick    44.93%\n",
            " n02786058 - Band_Aid     11.43%\n",
            " n04591157 - Windsor_tie  9.72%\n",
            "\n",
            "Image #538\n",
            " n02786058 - Band_Aid     54.99%\n",
            " n04591157 - Windsor_tie  21.90%\n",
            " n04357314 - sunscreen    5.11%\n",
            "\n",
            "Image #539\n",
            " n04328186 - stopwatch    25.43%\n",
            " n02786058 - Band_Aid     9.88%\n",
            " n03929660 - pick         8.80%\n",
            "\n",
            "Image #540\n",
            " n02786058 - Band_Aid     21.60%\n",
            " n03250847 - drumstick    11.93%\n",
            " n04357314 - sunscreen    8.02%\n",
            "\n",
            "Image #541\n",
            " n04591157 - Windsor_tie  13.12%\n",
            " n02786058 - Band_Aid     11.56%\n",
            " n04357314 - sunscreen    7.17%\n",
            "\n",
            "Image #542\n",
            " n02786058 - Band_Aid     36.33%\n",
            " n04357314 - sunscreen    7.77%\n",
            " n04254120 - soap_dispenser 6.80%\n",
            "\n",
            "Image #543\n",
            " n04357314 - sunscreen    24.43%\n",
            " n04591157 - Windsor_tie  19.46%\n",
            " n02786058 - Band_Aid     6.52%\n",
            "\n",
            "Image #544\n",
            " n04357314 - sunscreen    31.92%\n",
            " n02786058 - Band_Aid     18.50%\n",
            " n04591157 - Windsor_tie  7.37%\n",
            "\n",
            "Image #545\n",
            " n02786058 - Band_Aid     19.17%\n",
            " n04591157 - Windsor_tie  11.56%\n",
            " n03250847 - drumstick    8.65%\n",
            "\n",
            "Image #546\n",
            " n03970156 - plunger      29.02%\n",
            " n03250847 - drumstick    14.71%\n",
            " n02786058 - Band_Aid     6.87%\n",
            "\n",
            "Image #547\n",
            " n02786058 - Band_Aid     13.60%\n",
            " n04591157 - Windsor_tie  13.21%\n",
            " n04357314 - sunscreen    10.61%\n",
            "\n",
            "Image #548\n",
            " n02951585 - can_opener   18.17%\n",
            " n04372370 - switch       7.56%\n",
            " n03476991 - hair_spray   6.36%\n",
            "\n",
            "Image #549\n",
            " n02951585 - can_opener   17.07%\n",
            " n04372370 - switch       16.72%\n",
            " n04254120 - soap_dispenser 14.69%\n",
            "\n",
            "Image #550\n",
            " n04372370 - switch       20.22%\n",
            " n02786058 - Band_Aid     8.42%\n",
            " n03825788 - nipple       6.69%\n",
            "\n",
            "Image #551\n",
            " n04357314 - sunscreen    19.86%\n",
            " n04254120 - soap_dispenser 12.01%\n",
            " n04591157 - Windsor_tie  10.41%\n",
            "\n",
            "Image #552\n",
            " n04372370 - switch       19.63%\n",
            " n03970156 - plunger      10.91%\n",
            " n02951585 - can_opener   8.65%\n",
            "\n",
            "Image #553\n",
            " n04357314 - sunscreen    26.06%\n",
            " n02786058 - Band_Aid     13.37%\n",
            " n02808440 - bathtub      8.20%\n",
            "\n",
            "Image #554\n",
            " n02786058 - Band_Aid     18.45%\n",
            " n04591157 - Windsor_tie  15.75%\n",
            " n03970156 - plunger      15.73%\n",
            "\n",
            "Image #555\n",
            " n02786058 - Band_Aid     20.58%\n",
            " n03250847 - drumstick    12.94%\n",
            " n03970156 - plunger      10.57%\n",
            "\n",
            "Image #556\n",
            " n02786058 - Band_Aid     16.97%\n",
            " n04357314 - sunscreen    8.84%\n",
            " n03970156 - plunger      7.00%\n",
            "\n",
            "Image #557\n",
            " n03970156 - plunger      27.48%\n",
            " n04270147 - spatula      9.59%\n",
            " n02786058 - Band_Aid     6.78%\n",
            "\n",
            "Image #558\n",
            " n02786058 - Band_Aid     15.69%\n",
            " n04591157 - Windsor_tie  14.43%\n",
            " n03970156 - plunger      9.27%\n",
            "\n",
            "Image #559\n",
            " n03970156 - plunger      41.06%\n",
            " n03250847 - drumstick    11.69%\n",
            " n02786058 - Band_Aid     7.23%\n",
            "\n",
            "Image #560\n",
            " n04357314 - sunscreen    23.90%\n",
            " n02786058 - Band_Aid     13.86%\n",
            " n03314780 - face_powder  6.58%\n",
            "\n",
            "Image #561\n",
            " n02786058 - Band_Aid     27.95%\n",
            " n04591157 - Windsor_tie  9.48%\n",
            " n04357314 - sunscreen    7.11%\n",
            "\n",
            "Image #562\n",
            " n02786058 - Band_Aid     41.73%\n",
            " n04357314 - sunscreen    11.21%\n",
            " n04591157 - Windsor_tie  8.98%\n",
            "\n",
            "Image #563\n",
            " n04357314 - sunscreen    17.56%\n",
            " n04591157 - Windsor_tie  8.48%\n",
            " n04493381 - tub          4.06%\n",
            "\n",
            "Image #564\n",
            " n02786058 - Band_Aid     91.36%\n",
            " n04357314 - sunscreen    3.25%\n",
            " n03314780 - face_powder  0.42%\n",
            "\n",
            "Image #565\n",
            " n04372370 - switch       39.10%\n",
            " n04254120 - soap_dispenser 6.20%\n",
            " n03825788 - nipple       4.39%\n",
            "\n",
            "Image #566\n",
            " n04357314 - sunscreen    31.39%\n",
            " n02786058 - Band_Aid     19.84%\n",
            " n03314780 - face_powder  4.13%\n",
            "\n",
            "Image #567\n",
            " n03825788 - nipple       13.24%\n",
            " n03476991 - hair_spray   13.04%\n",
            " n03690938 - lotion       8.74%\n",
            "\n",
            "Image #568\n",
            " n04591157 - Windsor_tie  27.85%\n",
            " n04357314 - sunscreen    7.83%\n",
            " n03970156 - plunger      5.59%\n",
            "\n",
            "Image #569\n",
            " n04254120 - soap_dispenser 25.59%\n",
            " n02786058 - Band_Aid     15.83%\n",
            " n04357314 - sunscreen    10.34%\n",
            "\n",
            "Image #570\n",
            " n04357314 - sunscreen    17.04%\n",
            " n02786058 - Band_Aid     10.47%\n",
            " n04591157 - Windsor_tie  5.46%\n",
            "\n",
            "Image #571\n",
            " n03970156 - plunger      18.37%\n",
            " n04591157 - Windsor_tie  12.06%\n",
            " n04270147 - spatula      9.03%\n",
            "\n",
            "Image #572\n",
            " n02786058 - Band_Aid     57.49%\n",
            " n03250847 - drumstick    5.03%\n",
            " n02951585 - can_opener   4.39%\n",
            "\n",
            "Image #573\n",
            " n02786058 - Band_Aid     22.65%\n",
            " n03970156 - plunger      9.95%\n",
            " n04597913 - wooden_spoon 6.08%\n",
            "\n",
            "Image #574\n",
            " n04357314 - sunscreen    18.32%\n",
            " n02786058 - Band_Aid     16.90%\n",
            " n04591157 - Windsor_tie  11.35%\n",
            "\n",
            "Image #575\n",
            " n02786058 - Band_Aid     9.78%\n",
            " n04357314 - sunscreen    4.64%\n",
            " n03314780 - face_powder  4.50%\n",
            "\n",
            "Image #576\n",
            " n02786058 - Band_Aid     27.70%\n",
            " n04357314 - sunscreen    15.99%\n",
            " n03314780 - face_powder  7.01%\n",
            "\n",
            "Image #577\n",
            " n03250847 - drumstick    30.58%\n",
            " n02786058 - Band_Aid     17.99%\n",
            " n04357314 - sunscreen    6.34%\n",
            "\n",
            "Image #578\n",
            " n03970156 - plunger      21.07%\n",
            " n04591157 - Windsor_tie  13.53%\n",
            " n03250847 - drumstick    8.80%\n",
            "\n",
            "Image #579\n",
            " n02786058 - Band_Aid     28.91%\n",
            " n04591157 - Windsor_tie  13.36%\n",
            " n04357314 - sunscreen    6.22%\n",
            "\n",
            "Image #580\n",
            " n02786058 - Band_Aid     18.34%\n",
            " n03970156 - plunger      14.66%\n",
            " n04591157 - Windsor_tie  12.79%\n",
            "\n",
            "Image #581\n",
            " n02786058 - Band_Aid     32.46%\n",
            " n03250847 - drumstick    16.28%\n",
            " n03720891 - maraca       7.12%\n",
            "\n",
            "Image #582\n",
            " n02786058 - Band_Aid     22.00%\n",
            " n04591157 - Windsor_tie  9.14%\n",
            " n04357314 - sunscreen    7.81%\n",
            "\n",
            "Image #583\n",
            " n03476991 - hair_spray   14.98%\n",
            " n02951585 - can_opener   10.94%\n",
            " n04328186 - stopwatch    7.22%\n",
            "\n",
            "Image #584\n",
            " n04357314 - sunscreen    23.12%\n",
            " n02786058 - Band_Aid     18.18%\n",
            " n04591157 - Windsor_tie  3.47%\n",
            "\n",
            "Image #585\n",
            " n04591157 - Windsor_tie  65.05%\n",
            " n04357314 - sunscreen    4.98%\n",
            " n04254120 - soap_dispenser 1.24%\n",
            "\n",
            "Image #586\n",
            " n02786058 - Band_Aid     33.04%\n",
            " n04357314 - sunscreen    7.59%\n",
            " n03250847 - drumstick    5.96%\n",
            "\n",
            "Image #587\n",
            " n04357314 - sunscreen    23.61%\n",
            " n02786058 - Band_Aid     19.69%\n",
            " n03314780 - face_powder  4.19%\n",
            "\n",
            "Image #588\n",
            " n04591157 - Windsor_tie  26.50%\n",
            " n04357314 - sunscreen    15.05%\n",
            " n02786058 - Band_Aid     11.43%\n",
            "\n",
            "Image #589\n",
            " n03970156 - plunger      23.04%\n",
            " n04357314 - sunscreen    8.09%\n",
            " n02786058 - Band_Aid     5.13%\n",
            "\n",
            "Image #590\n",
            " n03250847 - drumstick    26.64%\n",
            " n03970156 - plunger      14.11%\n",
            " n04357314 - sunscreen    3.61%\n",
            "\n",
            "Image #591\n",
            " n04357314 - sunscreen    18.59%\n",
            " n02786058 - Band_Aid     8.83%\n",
            " n04254120 - soap_dispenser 4.16%\n",
            "\n",
            "Image #592\n",
            " n02951585 - can_opener   19.68%\n",
            " n04591157 - Windsor_tie  15.99%\n",
            " n04372370 - switch       12.11%\n",
            "\n",
            "Image #593\n",
            " n03970156 - plunger      13.23%\n",
            " n04357314 - sunscreen    10.24%\n",
            " n04270147 - spatula      9.76%\n",
            "\n",
            "Image #594\n",
            " n04372370 - switch       16.66%\n",
            " n04254120 - soap_dispenser 9.17%\n",
            " n03903868 - pedestal     6.92%\n",
            "\n",
            "Image #595\n",
            " n04357314 - sunscreen    23.78%\n",
            " n02786058 - Band_Aid     17.51%\n",
            " n04125021 - safe         4.67%\n",
            "\n",
            "Image #596\n",
            " n02786058 - Band_Aid     23.89%\n",
            " n04357314 - sunscreen    9.43%\n",
            " n04591157 - Windsor_tie  6.86%\n",
            "\n",
            "Image #597\n",
            " n04372370 - switch       24.10%\n",
            " n03970156 - plunger      6.34%\n",
            " n03825788 - nipple       5.61%\n",
            "\n",
            "Image #598\n",
            " n02786058 - Band_Aid     52.85%\n",
            " n04591157 - Windsor_tie  11.93%\n",
            " n04357314 - sunscreen    4.49%\n",
            "\n",
            "Image #599\n",
            " n04591157 - Windsor_tie  17.18%\n",
            " n03903868 - pedestal     7.13%\n",
            " n02786058 - Band_Aid     5.21%\n",
            "\n",
            "Image #600\n",
            " n04591157 - Windsor_tie  15.86%\n",
            " n04254120 - soap_dispenser 12.36%\n",
            " n02951585 - can_opener   8.24%\n",
            "\n",
            "Image #601\n",
            " n02786058 - Band_Aid     19.46%\n",
            " n03250847 - drumstick    9.92%\n",
            " n03970156 - plunger      7.35%\n",
            "\n",
            "Image #602\n",
            " n04372370 - switch       25.80%\n",
            " n04254120 - soap_dispenser 20.75%\n",
            " n02786058 - Band_Aid     7.34%\n",
            "\n",
            "Image #603\n",
            " n02786058 - Band_Aid     28.71%\n",
            " n04591157 - Windsor_tie  9.26%\n",
            " n04254120 - soap_dispenser 5.96%\n",
            "\n",
            "Image #604\n",
            " n04372370 - switch       37.32%\n",
            " n04591157 - Windsor_tie  15.78%\n",
            " n02786058 - Band_Aid     14.44%\n",
            "\n",
            "Image #605\n",
            " n03970156 - plunger      39.46%\n",
            " n03250847 - drumstick    10.68%\n",
            " n02786058 - Band_Aid     7.86%\n",
            "\n",
            "Image #606\n",
            " n04591157 - Windsor_tie  34.90%\n",
            " n02786058 - Band_Aid     9.71%\n",
            " n04357314 - sunscreen    8.99%\n",
            "\n",
            "Image #607\n",
            " n03970156 - plunger      26.14%\n",
            " n03250847 - drumstick    15.78%\n",
            " n02786058 - Band_Aid     7.08%\n",
            "\n",
            "Image #608\n",
            " n03970156 - plunger      21.64%\n",
            " n03250847 - drumstick    13.97%\n",
            " n04591157 - Windsor_tie  8.65%\n",
            "\n",
            "Image #609\n",
            " n03250847 - drumstick    22.61%\n",
            " n02786058 - Band_Aid     9.37%\n",
            " n03970156 - plunger      8.32%\n",
            "\n",
            "Image #610\n",
            " n03970156 - plunger      21.92%\n",
            " n04270147 - spatula      10.00%\n",
            " n03498962 - hatchet      9.04%\n",
            "\n",
            "Image #611\n",
            " n03970156 - plunger      13.05%\n",
            " n04270147 - spatula      10.39%\n",
            " n04597913 - wooden_spoon 6.84%\n",
            "\n",
            "Image #612\n",
            " n02786058 - Band_Aid     21.72%\n",
            " n04357314 - sunscreen    9.46%\n",
            " n04591157 - Windsor_tie  7.77%\n",
            "\n",
            "Image #613\n",
            " n04591157 - Windsor_tie  11.81%\n",
            " n03250847 - drumstick    10.86%\n",
            " n03970156 - plunger      10.13%\n",
            "\n",
            "Image #614\n",
            " n04591157 - Windsor_tie  13.58%\n",
            " n04357314 - sunscreen    7.75%\n",
            " n04270147 - spatula      5.13%\n",
            "\n",
            "Image #615\n",
            " n04357314 - sunscreen    13.99%\n",
            " n04540053 - volleyball   9.14%\n",
            " n02786058 - Band_Aid     4.49%\n",
            "\n",
            "Image #616\n",
            " n02786058 - Band_Aid     42.65%\n",
            " n04357314 - sunscreen    20.46%\n",
            " n04591157 - Windsor_tie  9.87%\n",
            "\n",
            "Image #617\n",
            " n04357314 - sunscreen    33.70%\n",
            " n02786058 - Band_Aid     30.13%\n",
            " n03314780 - face_powder  9.36%\n",
            "\n",
            "Image #618\n",
            " n03970156 - plunger      44.00%\n",
            " n03250847 - drumstick    28.52%\n",
            " n02786058 - Band_Aid     4.39%\n",
            "\n",
            "Image #619\n",
            " n03250847 - drumstick    42.25%\n",
            " n03970156 - plunger      8.91%\n",
            " n02786058 - Band_Aid     5.93%\n",
            "\n",
            "Image #620\n",
            " n04591157 - Windsor_tie  11.11%\n",
            " n03942813 - ping-pong_ball 10.84%\n",
            " n02786058 - Band_Aid     10.69%\n",
            "\n",
            "Image #621\n",
            " n02786058 - Band_Aid     30.88%\n",
            " n04357314 - sunscreen    10.36%\n",
            " n04591157 - Windsor_tie  7.50%\n",
            "\n",
            "Image #622\n",
            " n02786058 - Band_Aid     27.84%\n",
            " n03250847 - drumstick    17.42%\n",
            " n04254120 - soap_dispenser 6.31%\n",
            "\n",
            "Image #623\n",
            " n04372370 - switch       57.61%\n",
            " n02951585 - can_opener   5.22%\n",
            " n03476991 - hair_spray   5.01%\n",
            "\n",
            "Image #624\n",
            " n03250847 - drumstick    21.17%\n",
            " n03970156 - plunger      18.85%\n",
            " n02786058 - Band_Aid     11.34%\n",
            "\n",
            "Image #625\n",
            " n04591157 - Windsor_tie  37.02%\n",
            " n04372370 - switch       9.06%\n",
            " n02786058 - Band_Aid     7.47%\n",
            "\n",
            "Image #626\n",
            " n03250847 - drumstick    42.37%\n",
            " n03970156 - plunger      14.30%\n",
            " n02786058 - Band_Aid     12.01%\n",
            "\n",
            "Image #627\n",
            " n02786058 - Band_Aid     26.75%\n",
            " n04357314 - sunscreen    9.69%\n",
            " n03970156 - plunger      6.17%\n",
            "\n",
            "Image #628\n",
            " n04591157 - Windsor_tie  39.74%\n",
            " n04357314 - sunscreen    13.84%\n",
            " n02786058 - Band_Aid     5.84%\n",
            "\n",
            "Image #629\n",
            " n04591157 - Windsor_tie  44.59%\n",
            " n04357314 - sunscreen    15.57%\n",
            " n02786058 - Band_Aid     2.22%\n",
            "\n",
            "Image #630\n",
            " n04372370 - switch       16.33%\n",
            " n04591157 - Windsor_tie  9.91%\n",
            " n03314780 - face_powder  9.17%\n",
            "\n",
            "Image #631\n",
            " n03250847 - drumstick    10.41%\n",
            " n04591157 - Windsor_tie  7.32%\n",
            " n02951585 - can_opener   5.98%\n",
            "\n",
            "Image #632\n",
            " n03970156 - plunger      41.28%\n",
            " n03250847 - drumstick    19.43%\n",
            " n04270147 - spatula      5.73%\n",
            "\n",
            "Image #633\n",
            " n04591157 - Windsor_tie  28.26%\n",
            " n04357314 - sunscreen    8.16%\n",
            " n02786058 - Band_Aid     5.67%\n",
            "\n",
            "Image #634\n",
            " n04591157 - Windsor_tie  41.14%\n",
            " n02786058 - Band_Aid     14.28%\n",
            " n03970156 - plunger      9.76%\n",
            "\n",
            "Image #635\n",
            " n04357314 - sunscreen    9.92%\n",
            " n04591157 - Windsor_tie  9.30%\n",
            " n02786058 - Band_Aid     8.39%\n",
            "\n",
            "Image #636\n",
            " n02786058 - Band_Aid     16.07%\n",
            " n03250847 - drumstick    12.53%\n",
            " n04357314 - sunscreen    6.24%\n",
            "\n",
            "Image #637\n",
            " n04372370 - switch       44.96%\n",
            " n02951585 - can_opener   9.79%\n",
            " n04254120 - soap_dispenser 9.68%\n",
            "\n",
            "Image #638\n",
            " n03970156 - plunger      16.61%\n",
            " n02786058 - Band_Aid     10.65%\n",
            " n04372370 - switch       9.38%\n",
            "\n",
            "Image #639\n",
            " n02786058 - Band_Aid     27.86%\n",
            " n03250847 - drumstick    9.77%\n",
            " n04357314 - sunscreen    8.90%\n",
            "\n",
            "Image #640\n",
            " n02786058 - Band_Aid     22.38%\n",
            " n04357314 - sunscreen    9.12%\n",
            " n04591157 - Windsor_tie  8.48%\n",
            "\n",
            "Image #641\n",
            " n03929660 - pick         10.85%\n",
            " n04591157 - Windsor_tie  6.74%\n",
            " n04328186 - stopwatch    6.60%\n",
            "\n",
            "Image #642\n",
            " n04372370 - switch       17.24%\n",
            " n03476991 - hair_spray   13.17%\n",
            " n02951585 - can_opener   6.55%\n",
            "\n",
            "Image #643\n",
            " n04357314 - sunscreen    16.69%\n",
            " n02786058 - Band_Aid     14.56%\n",
            " n04591157 - Windsor_tie  5.76%\n",
            "\n",
            "Image #644\n",
            " n02786058 - Band_Aid     54.35%\n",
            " n04591157 - Windsor_tie  18.20%\n",
            " n04357314 - sunscreen    4.86%\n",
            "\n",
            "Image #645\n",
            " n02786058 - Band_Aid     10.21%\n",
            " n04357314 - sunscreen    10.16%\n",
            " n04591157 - Windsor_tie  10.08%\n",
            "\n",
            "Image #646\n",
            " n04591157 - Windsor_tie  68.50%\n",
            " n03250847 - drumstick    4.88%\n",
            " n02786058 - Band_Aid     3.99%\n",
            "\n",
            "Image #647\n",
            " n04357314 - sunscreen    11.00%\n",
            " n04591157 - Windsor_tie  8.75%\n",
            " n04270147 - spatula      8.56%\n",
            "\n",
            "Image #648\n",
            " n03250847 - drumstick    25.76%\n",
            " n03970156 - plunger      10.61%\n",
            " n02786058 - Band_Aid     8.45%\n",
            "\n",
            "Image #649\n",
            " n03970156 - plunger      17.17%\n",
            " n03250847 - drumstick    15.94%\n",
            " n02786058 - Band_Aid     9.64%\n",
            "\n",
            "Image #650\n",
            " n04270147 - spatula      19.88%\n",
            " n03970156 - plunger      17.39%\n",
            " n03250847 - drumstick    9.66%\n",
            "\n",
            "Image #651\n",
            " n04591157 - Windsor_tie  15.40%\n",
            " n03970156 - plunger      13.21%\n",
            " n02951585 - can_opener   7.09%\n",
            "\n",
            "Image #652\n",
            " n02786058 - Band_Aid     38.67%\n",
            " n04357314 - sunscreen    10.02%\n",
            " n04591157 - Windsor_tie  8.27%\n",
            "\n",
            "Image #653\n",
            " n02786058 - Band_Aid     32.57%\n",
            " n04357314 - sunscreen    16.96%\n",
            " n04591157 - Windsor_tie  3.85%\n",
            "\n",
            "Image #654\n",
            " n02786058 - Band_Aid     15.25%\n",
            " n04357314 - sunscreen    10.52%\n",
            " n03250847 - drumstick    8.76%\n",
            "\n",
            "Image #655\n",
            " n02786058 - Band_Aid     31.38%\n",
            " n04357314 - sunscreen    31.26%\n",
            " n03314780 - face_powder  6.32%\n",
            "\n",
            "Image #656\n",
            " n02786058 - Band_Aid     15.92%\n",
            " n03250847 - drumstick    8.14%\n",
            " n04357314 - sunscreen    6.56%\n",
            "\n",
            "Image #657\n",
            " n02951585 - can_opener   21.09%\n",
            " n04372370 - switch       17.81%\n",
            " n04328186 - stopwatch    8.22%\n",
            "\n",
            "Image #658\n",
            " n03250847 - drumstick    18.49%\n",
            " n03970156 - plunger      18.45%\n",
            " n02786058 - Band_Aid     8.29%\n",
            "\n",
            "Image #659\n",
            " n03250847 - drumstick    28.61%\n",
            " n04591157 - Windsor_tie  12.83%\n",
            " n03970156 - plunger      11.25%\n",
            "\n",
            "Image #660\n",
            " n02786058 - Band_Aid     17.78%\n",
            " n04254120 - soap_dispenser 8.91%\n",
            " n04372370 - switch       6.73%\n",
            "\n",
            "Image #661\n",
            " n04372370 - switch       57.70%\n",
            " n04591157 - Windsor_tie  7.71%\n",
            " n04254120 - soap_dispenser 5.49%\n",
            "\n",
            "Image #662\n",
            " n04357314 - sunscreen    21.45%\n",
            " n02786058 - Band_Aid     21.42%\n",
            " n04591157 - Windsor_tie  15.87%\n",
            "\n",
            "Image #663\n",
            " n04254120 - soap_dispenser 15.35%\n",
            " n03476991 - hair_spray   7.38%\n",
            " n04372370 - switch       6.44%\n",
            "\n",
            "Image #664\n",
            " n02786058 - Band_Aid     30.03%\n",
            " n04357314 - sunscreen    25.03%\n",
            " n04254120 - soap_dispenser 5.80%\n",
            "\n",
            "Image #665\n",
            " n02786058 - Band_Aid     24.67%\n",
            " n03970156 - plunger      18.64%\n",
            " n04591157 - Windsor_tie  5.27%\n",
            "\n",
            "Image #666\n",
            " n03970156 - plunger      46.98%\n",
            " n03250847 - drumstick    25.05%\n",
            " n02786058 - Band_Aid     6.34%\n",
            "\n",
            "Image #667\n",
            " n03250847 - drumstick    28.55%\n",
            " n03970156 - plunger      7.83%\n",
            " n02786058 - Band_Aid     6.25%\n",
            "\n",
            "Image #668\n",
            " n02786058 - Band_Aid     18.22%\n",
            " n04591157 - Windsor_tie  14.04%\n",
            " n04357314 - sunscreen    13.52%\n",
            "\n",
            "Image #669\n",
            " n04357314 - sunscreen    21.85%\n",
            " n02786058 - Band_Aid     15.84%\n",
            " n04591157 - Windsor_tie  4.45%\n",
            "\n",
            "Image #670\n",
            " n02786058 - Band_Aid     14.18%\n",
            " n03250847 - drumstick    7.67%\n",
            " n04357314 - sunscreen    6.96%\n",
            "\n",
            "Image #671\n",
            " n04125021 - safe         18.23%\n",
            " n04372370 - switch       16.93%\n",
            " n04357314 - sunscreen    13.68%\n",
            "\n",
            "Image #672\n",
            " n03250847 - drumstick    15.21%\n",
            " n04357314 - sunscreen    13.90%\n",
            " n02786058 - Band_Aid     12.59%\n",
            "\n",
            "Image #673\n",
            " n04357314 - sunscreen    7.54%\n",
            " n02786058 - Band_Aid     6.46%\n",
            " n03970156 - plunger      4.09%\n",
            "\n",
            "Image #674\n",
            " n02786058 - Band_Aid     22.00%\n",
            " n04357314 - sunscreen    14.53%\n",
            " n04591157 - Windsor_tie  4.91%\n",
            "\n",
            "Image #675\n",
            " n02786058 - Band_Aid     14.68%\n",
            " n04357314 - sunscreen    13.00%\n",
            " n04591157 - Windsor_tie  12.33%\n",
            "\n",
            "Image #676\n",
            " n02786058 - Band_Aid     14.36%\n",
            " n04357314 - sunscreen    13.10%\n",
            " n04591157 - Windsor_tie  10.99%\n",
            "\n",
            "Image #677\n",
            " n02786058 - Band_Aid     20.09%\n",
            " n03188531 - diaper       5.57%\n",
            " n04357314 - sunscreen    4.78%\n",
            "\n",
            "Image #678\n",
            " n04372370 - switch       56.32%\n",
            " n04254120 - soap_dispenser 15.91%\n",
            " n02951585 - can_opener   6.15%\n",
            "\n",
            "Image #679\n",
            " n04357314 - sunscreen    25.19%\n",
            " n02786058 - Band_Aid     14.52%\n",
            " n04591157 - Windsor_tie  11.47%\n",
            "\n",
            "Image #680\n",
            " n04372370 - switch       83.43%\n",
            " n04254120 - soap_dispenser 2.53%\n",
            " n04591157 - Windsor_tie  2.46%\n",
            "\n",
            "Image #681\n",
            " n04591157 - Windsor_tie  23.16%\n",
            " n04357314 - sunscreen    9.00%\n",
            " n03970156 - plunger      5.02%\n",
            "\n",
            "Image #682\n",
            " n04357314 - sunscreen    18.19%\n",
            " n02786058 - Band_Aid     13.62%\n",
            " n04591157 - Windsor_tie  7.83%\n",
            "\n",
            "Image #683\n",
            " n04591157 - Windsor_tie  37.14%\n",
            " n02786058 - Band_Aid     29.88%\n",
            " n04357314 - sunscreen    2.32%\n",
            "\n",
            "Image #684\n",
            " n04591157 - Windsor_tie  23.17%\n",
            " n03876231 - paintbrush   11.77%\n",
            " n02786058 - Band_Aid     6.32%\n",
            "\n",
            "Image #685\n",
            " n04357314 - sunscreen    7.12%\n",
            " n04372370 - switch       6.89%\n",
            " n04254120 - soap_dispenser 6.87%\n",
            "\n",
            "Image #686\n",
            " n04591157 - Windsor_tie  21.14%\n",
            " n03250847 - drumstick    11.03%\n",
            " n02786058 - Band_Aid     8.91%\n",
            "\n",
            "Image #687\n",
            " n03250847 - drumstick    40.88%\n",
            " n02951585 - can_opener   5.77%\n",
            " n02786058 - Band_Aid     5.62%\n",
            "\n",
            "Image #688\n",
            " n03250847 - drumstick    18.85%\n",
            " n02786058 - Band_Aid     15.26%\n",
            " n03970156 - plunger      7.44%\n",
            "\n",
            "Image #689\n",
            " n04357314 - sunscreen    18.88%\n",
            " n04591157 - Windsor_tie  8.19%\n",
            " n02786058 - Band_Aid     7.45%\n",
            "\n",
            "Image #690\n",
            " n04591157 - Windsor_tie  15.93%\n",
            " n02786058 - Band_Aid     9.03%\n",
            " n04254120 - soap_dispenser 6.79%\n",
            "\n",
            "Image #691\n",
            " n04357314 - sunscreen    30.59%\n",
            " n02786058 - Band_Aid     9.68%\n",
            " n04591157 - Windsor_tie  4.96%\n",
            "\n",
            "Image #692\n",
            " n03250847 - drumstick    48.31%\n",
            " n02786058 - Band_Aid     12.74%\n",
            " n03658185 - letter_opener 3.94%\n",
            "\n",
            "Image #693\n",
            " n04357314 - sunscreen    13.97%\n",
            " n02786058 - Band_Aid     11.15%\n",
            " n04254120 - soap_dispenser 6.37%\n",
            "\n",
            "Image #694\n",
            " n03250847 - drumstick    72.99%\n",
            " n03970156 - plunger      4.08%\n",
            " n03249569 - drum         3.73%\n",
            "\n",
            "Image #695\n",
            " n04357314 - sunscreen    13.78%\n",
            " n04591157 - Windsor_tie  10.33%\n",
            " n02786058 - Band_Aid     6.99%\n",
            "\n",
            "Image #696\n",
            " n03970156 - plunger      20.99%\n",
            " n04270147 - spatula      12.35%\n",
            " n03250847 - drumstick    9.17%\n",
            "\n",
            "Image #697\n",
            " n02786058 - Band_Aid     16.20%\n",
            " n04357314 - sunscreen    13.91%\n",
            " n04591157 - Windsor_tie  11.29%\n",
            "\n",
            "Image #698\n",
            " n04357314 - sunscreen    20.65%\n",
            " n02786058 - Band_Aid     13.33%\n",
            " n03970156 - plunger      12.68%\n",
            "\n",
            "Image #699\n",
            " n02786058 - Band_Aid     12.57%\n",
            " n04591157 - Windsor_tie  7.51%\n",
            " n04254120 - soap_dispenser 5.08%\n",
            "\n",
            "Image #700\n",
            " n04591157 - Windsor_tie  15.80%\n",
            " n04254120 - soap_dispenser 10.32%\n",
            " n03250847 - drumstick    10.07%\n",
            "\n",
            "Image #701\n",
            " n04372370 - switch       18.64%\n",
            " n04254120 - soap_dispenser 12.91%\n",
            " n02951585 - can_opener   11.48%\n",
            "\n",
            "Image #702\n",
            " n04357314 - sunscreen    12.70%\n",
            " n02786058 - Band_Aid     9.30%\n",
            " n03970156 - plunger      8.55%\n",
            "\n",
            "Image #703\n",
            " n04357314 - sunscreen    45.28%\n",
            " n02786058 - Band_Aid     13.60%\n",
            " n04591157 - Windsor_tie  5.73%\n",
            "\n",
            "Image #704\n",
            " n04591157 - Windsor_tie  12.40%\n",
            " n04357314 - sunscreen    12.13%\n",
            " n02786058 - Band_Aid     6.96%\n",
            "\n",
            "Image #705\n",
            " n02786058 - Band_Aid     18.75%\n",
            " n04591157 - Windsor_tie  14.53%\n",
            " n03720891 - maraca       6.27%\n",
            "\n",
            "Image #706\n",
            " n03970156 - plunger      23.81%\n",
            " n03250847 - drumstick    18.32%\n",
            " n04591157 - Windsor_tie  14.17%\n",
            "\n",
            "Image #707\n",
            " n04591157 - Windsor_tie  23.26%\n",
            " n04357314 - sunscreen    11.47%\n",
            " n03250847 - drumstick    8.11%\n",
            "\n",
            "Image #708\n",
            " n02786058 - Band_Aid     13.62%\n",
            " n03970156 - plunger      9.16%\n",
            " n04591157 - Windsor_tie  5.94%\n",
            "\n",
            "Image #709\n",
            " n04372370 - switch       11.96%\n",
            " n04254120 - soap_dispenser 8.96%\n",
            " n02951585 - can_opener   8.71%\n",
            "\n",
            "Image #710\n",
            " n04372370 - switch       16.33%\n",
            " n04591157 - Windsor_tie  9.91%\n",
            " n03314780 - face_powder  9.17%\n",
            "\n",
            "Image #711\n",
            " n03250847 - drumstick    37.72%\n",
            " n03970156 - plunger      13.32%\n",
            " n04270147 - spatula      8.55%\n",
            "\n",
            "Image #712\n",
            " n04357314 - sunscreen    19.64%\n",
            " n02786058 - Band_Aid     17.06%\n",
            " n04254120 - soap_dispenser 3.76%\n",
            "\n",
            "Image #713\n",
            " n04591157 - Windsor_tie  43.50%\n",
            " n04357314 - sunscreen    8.64%\n",
            " n02786058 - Band_Aid     6.39%\n",
            "\n",
            "Image #714\n",
            " n02786058 - Band_Aid     27.48%\n",
            " n04357314 - sunscreen    7.33%\n",
            " n04591157 - Windsor_tie  7.01%\n",
            "\n",
            "Image #715\n",
            " n02786058 - Band_Aid     19.37%\n",
            " n03970156 - plunger      12.38%\n",
            " n03250847 - drumstick    8.68%\n",
            "\n",
            "Image #716\n",
            " n03970156 - plunger      31.50%\n",
            " n03250847 - drumstick    27.93%\n",
            " n03720891 - maraca       11.53%\n",
            "\n",
            "Image #717\n",
            " n02786058 - Band_Aid     34.28%\n",
            " n04357314 - sunscreen    13.70%\n",
            " n03188531 - diaper       2.35%\n",
            "\n",
            "Image #718\n",
            " n04591157 - Windsor_tie  39.97%\n",
            " n04357314 - sunscreen    9.04%\n",
            " n02786058 - Band_Aid     5.70%\n",
            "\n",
            "Image #719\n",
            " n03250847 - drumstick    24.34%\n",
            " n03970156 - plunger      17.09%\n",
            " n04591157 - Windsor_tie  10.16%\n",
            "\n",
            "Image #720\n",
            " n04372370 - switch       27.81%\n",
            " n03970156 - plunger      8.99%\n",
            " n03476991 - hair_spray   5.36%\n",
            "\n",
            "Image #721\n",
            " n04372370 - switch       24.91%\n",
            " n04591157 - Windsor_tie  10.97%\n",
            " n02786058 - Band_Aid     9.44%\n",
            "\n",
            "Image #722\n",
            " n03970156 - plunger      12.95%\n",
            " n02786058 - Band_Aid     9.12%\n",
            " n04357314 - sunscreen    8.29%\n",
            "\n",
            "Image #723\n",
            " n03250847 - drumstick    10.77%\n",
            " n02786058 - Band_Aid     10.68%\n",
            " n04270147 - spatula      9.84%\n",
            "\n",
            "Image #724\n",
            " n04357314 - sunscreen    21.24%\n",
            " n04254120 - soap_dispenser 13.58%\n",
            " n04372370 - switch       6.27%\n",
            "\n",
            "Image #725\n",
            " n02951585 - can_opener   15.05%\n",
            " n02786058 - Band_Aid     7.96%\n",
            " n04591157 - Windsor_tie  6.68%\n",
            "\n",
            "Image #726\n",
            " n03970156 - plunger      19.46%\n",
            " n02786058 - Band_Aid     12.59%\n",
            " n04591157 - Windsor_tie  7.53%\n",
            "\n",
            "Image #727\n",
            " n02786058 - Band_Aid     20.17%\n",
            " n04591157 - Windsor_tie  15.98%\n",
            " n04357314 - sunscreen    6.42%\n",
            "\n",
            "Image #728\n",
            " n04372370 - switch       26.17%\n",
            " n02786058 - Band_Aid     13.39%\n",
            " n02951585 - can_opener   8.37%\n",
            "\n",
            "Image #729\n",
            " n04357314 - sunscreen    27.24%\n",
            " n02786058 - Band_Aid     22.09%\n",
            " n04591157 - Windsor_tie  13.65%\n",
            "\n",
            "Image #730\n",
            " n03250847 - drumstick    16.33%\n",
            " n04357314 - sunscreen    14.61%\n",
            " n02786058 - Band_Aid     12.81%\n",
            "\n",
            "Image #731\n",
            " n02786058 - Band_Aid     16.38%\n",
            " n03250847 - drumstick    14.85%\n",
            " n02951585 - can_opener   6.08%\n",
            "\n",
            "Image #732\n",
            " n04357314 - sunscreen    21.79%\n",
            " n02786058 - Band_Aid     8.76%\n",
            " n04371430 - swimming_trunks 4.16%\n",
            "\n",
            "Image #733\n",
            " n02786058 - Band_Aid     10.76%\n",
            " n04372370 - switch       9.74%\n",
            " n02951585 - can_opener   5.96%\n",
            "\n",
            "Image #734\n",
            " n04254120 - soap_dispenser 16.44%\n",
            " n02786058 - Band_Aid     16.32%\n",
            " n04372370 - switch       14.19%\n",
            "\n",
            "Image #735\n",
            " n03970156 - plunger      10.41%\n",
            " n04270147 - spatula      10.14%\n",
            " n04591157 - Windsor_tie  8.83%\n",
            "\n",
            "Image #736\n",
            " n02786058 - Band_Aid     30.27%\n",
            " n03970156 - plunger      11.43%\n",
            " n03250847 - drumstick    7.78%\n",
            "\n",
            "Image #737\n",
            " n03970156 - plunger      14.30%\n",
            " n04591157 - Windsor_tie  10.24%\n",
            " n03250847 - drumstick    9.84%\n",
            "\n",
            "Image #738\n",
            " n02786058 - Band_Aid     36.84%\n",
            " n03250847 - drumstick    17.99%\n",
            " n04591157 - Windsor_tie  13.03%\n",
            "\n",
            "Image #739\n",
            " n04591157 - Windsor_tie  45.81%\n",
            " n04372370 - switch       8.94%\n",
            " n02951585 - can_opener   7.66%\n",
            "\n",
            "Image #740\n",
            " n04591157 - Windsor_tie  37.40%\n",
            " n03970156 - plunger      14.59%\n",
            " n02951585 - can_opener   6.61%\n",
            "\n",
            "Image #741\n",
            " n04591157 - Windsor_tie  37.73%\n",
            " n02786058 - Band_Aid     13.05%\n",
            " n04357314 - sunscreen    10.35%\n",
            "\n",
            "Image #742\n",
            " n04357314 - sunscreen    29.11%\n",
            " n02786058 - Band_Aid     21.55%\n",
            " n04591157 - Windsor_tie  4.89%\n",
            "\n",
            "Image #743\n",
            " n04591157 - Windsor_tie  18.95%\n",
            " n03250847 - drumstick    13.43%\n",
            " n04357314 - sunscreen    6.38%\n",
            "\n",
            "Image #744\n",
            " n04591157 - Windsor_tie  26.01%\n",
            " n04254120 - soap_dispenser 13.10%\n",
            " n02786058 - Band_Aid     6.00%\n",
            "\n",
            "Image #745\n",
            " n02951585 - can_opener   15.52%\n",
            " n04372370 - switch       15.38%\n",
            " n03970156 - plunger      14.20%\n",
            "\n",
            "Image #746\n",
            " n04357314 - sunscreen    32.65%\n",
            " n02786058 - Band_Aid     14.79%\n",
            " n03250847 - drumstick    9.70%\n",
            "\n",
            "Image #747\n",
            " n04357314 - sunscreen    20.16%\n",
            " n02786058 - Band_Aid     8.26%\n",
            " n04209239 - shower_curtain 6.68%\n",
            "\n",
            "Image #748\n",
            " n03970156 - plunger      17.94%\n",
            " n03250847 - drumstick    14.47%\n",
            " n02951585 - can_opener   11.51%\n",
            "\n",
            "Image #749\n",
            " n03970156 - plunger      44.33%\n",
            " n04591157 - Windsor_tie  13.61%\n",
            " n03250847 - drumstick    3.81%\n",
            "\n",
            "Image #750\n",
            " n04357314 - sunscreen    18.35%\n",
            " n02786058 - Band_Aid     12.87%\n",
            " n03250847 - drumstick    10.64%\n",
            "\n",
            "Image #751\n",
            " n03970156 - plunger      31.70%\n",
            " n02786058 - Band_Aid     9.03%\n",
            " n04357314 - sunscreen    6.03%\n",
            "\n",
            "Image #752\n",
            " n03970156 - plunger      14.41%\n",
            " n04591157 - Windsor_tie  11.04%\n",
            " n04372370 - switch       7.02%\n",
            "\n",
            "Image #753\n",
            " n02786058 - Band_Aid     30.07%\n",
            " n03970156 - plunger      23.09%\n",
            " n03250847 - drumstick    5.52%\n",
            "\n",
            "Image #754\n",
            " n04591157 - Windsor_tie  50.86%\n",
            " n02786058 - Band_Aid     4.09%\n",
            " n02951585 - can_opener   3.80%\n",
            "\n",
            "Image #755\n",
            " n02786058 - Band_Aid     27.60%\n",
            " n04591157 - Windsor_tie  15.45%\n",
            " n03970156 - plunger      8.14%\n",
            "\n",
            "Image #756\n",
            " n03970156 - plunger      19.37%\n",
            " n03250847 - drumstick    17.47%\n",
            " n02786058 - Band_Aid     15.34%\n",
            "\n",
            "Image #757\n",
            " n03250847 - drumstick    20.56%\n",
            " n03970156 - plunger      19.19%\n",
            " n02786058 - Band_Aid     7.51%\n",
            "\n",
            "Image #758\n",
            " n04591157 - Windsor_tie  50.59%\n",
            " n03970156 - plunger      8.34%\n",
            " n02951585 - can_opener   6.72%\n",
            "\n",
            "Image #759\n",
            " n04372370 - switch       19.89%\n",
            " n03314780 - face_powder  10.25%\n",
            " n04254120 - soap_dispenser 5.59%\n",
            "\n",
            "Image #760\n",
            " n04591157 - Windsor_tie  23.05%\n",
            " n03970156 - plunger      12.62%\n",
            " n04270147 - spatula      8.96%\n",
            "\n",
            "Image #761\n",
            " n02786058 - Band_Aid     17.09%\n",
            " n03970156 - plunger      11.87%\n",
            " n04591157 - Windsor_tie  11.09%\n",
            "\n",
            "Image #762\n",
            " n04357314 - sunscreen    21.03%\n",
            " n02786058 - Band_Aid     15.43%\n",
            " n03314780 - face_powder  5.91%\n",
            "\n",
            "Image #763\n",
            " n02786058 - Band_Aid     29.95%\n",
            " n03250847 - drumstick    11.63%\n",
            " n04357314 - sunscreen    3.55%\n",
            "\n",
            "Image #764\n",
            " n04591157 - Windsor_tie  16.78%\n",
            " n03250847 - drumstick    14.36%\n",
            " n02786058 - Band_Aid     14.22%\n",
            "\n",
            "Image #765\n",
            " n04372370 - switch       35.70%\n",
            " n04591157 - Windsor_tie  13.48%\n",
            " n03485407 - hand-held_computer 9.87%\n",
            "\n",
            "Image #766\n",
            " n04357314 - sunscreen    15.48%\n",
            " n04591157 - Windsor_tie  14.55%\n",
            " n03250847 - drumstick    10.85%\n",
            "\n",
            "Image #767\n",
            " n02786058 - Band_Aid     19.13%\n",
            " n03970156 - plunger      11.42%\n",
            " n04357314 - sunscreen    8.98%\n",
            "\n",
            "Image #768\n",
            " n04591157 - Windsor_tie  18.32%\n",
            " n02786058 - Band_Aid     13.04%\n",
            " n04357314 - sunscreen    11.66%\n",
            "\n",
            "Image #769\n",
            " n04591157 - Windsor_tie  31.09%\n",
            " n04254120 - soap_dispenser 18.46%\n",
            " n04357314 - sunscreen    5.22%\n",
            "\n",
            "Image #770\n",
            " n03970156 - plunger      37.11%\n",
            " n02786058 - Band_Aid     20.18%\n",
            " n03498962 - hatchet      5.35%\n",
            "\n",
            "Image #771\n",
            " n02786058 - Band_Aid     32.38%\n",
            " n04357314 - sunscreen    15.97%\n",
            " n03314780 - face_powder  8.44%\n",
            "\n",
            "Image #772\n",
            " n03250847 - drumstick    21.65%\n",
            " n02786058 - Band_Aid     12.30%\n",
            " n04357314 - sunscreen    8.33%\n",
            "\n",
            "Image #773\n",
            " n02786058 - Band_Aid     18.19%\n",
            " n04357314 - sunscreen    17.18%\n",
            " n04591157 - Windsor_tie  8.19%\n",
            "\n",
            "Image #774\n",
            " n04357314 - sunscreen    21.12%\n",
            " n02786058 - Band_Aid     14.38%\n",
            " n03314780 - face_powder  6.96%\n",
            "\n",
            "Image #775\n",
            " n02786058 - Band_Aid     39.92%\n",
            " n03970156 - plunger      12.23%\n",
            " n03250847 - drumstick    6.51%\n",
            "\n",
            "Image #776\n",
            " n04591157 - Windsor_tie  40.76%\n",
            " n04270147 - spatula      20.75%\n",
            " n02951585 - can_opener   3.41%\n",
            "\n",
            "Image #777\n",
            " n03250847 - drumstick    25.77%\n",
            " n03970156 - plunger      18.32%\n",
            " n02786058 - Band_Aid     5.47%\n",
            "\n",
            "Image #778\n",
            " n03250847 - drumstick    21.06%\n",
            " n04591157 - Windsor_tie  11.71%\n",
            " n02786058 - Band_Aid     11.12%\n",
            "\n",
            "Image #779\n",
            " n04372370 - switch       43.26%\n",
            " n04591157 - Windsor_tie  12.59%\n",
            " n02786058 - Band_Aid     7.15%\n",
            "\n",
            "Image #780\n",
            " n03250847 - drumstick    30.14%\n",
            " n02786058 - Band_Aid     10.42%\n",
            " n03970156 - plunger      6.34%\n",
            "\n",
            "Image #781\n",
            " n02786058 - Band_Aid     25.52%\n",
            " n04357314 - sunscreen    15.07%\n",
            " n04591157 - Windsor_tie  5.46%\n",
            "\n",
            "Image #782\n",
            " n02786058 - Band_Aid     42.07%\n",
            " n04357314 - sunscreen    18.90%\n",
            " n04591157 - Windsor_tie  8.03%\n",
            "\n",
            "Image #783\n",
            " n04254120 - soap_dispenser 20.97%\n",
            " n03942813 - ping-pong_ball 13.75%\n",
            " n04591157 - Windsor_tie  11.64%\n",
            "\n",
            "Image #784\n",
            " n04357314 - sunscreen    27.05%\n",
            " n02786058 - Band_Aid     23.05%\n",
            " n04591157 - Windsor_tie  8.13%\n",
            "\n",
            "Image #785\n",
            " n03970156 - plunger      44.94%\n",
            " n02786058 - Band_Aid     7.94%\n",
            " n03250847 - drumstick    4.59%\n",
            "\n",
            "Image #786\n",
            " n04591157 - Windsor_tie  30.10%\n",
            " n03970156 - plunger      11.54%\n",
            " n02786058 - Band_Aid     6.16%\n",
            "\n",
            "Image #787\n",
            " n02786058 - Band_Aid     15.33%\n",
            " n04357314 - sunscreen    14.53%\n",
            " n04254120 - soap_dispenser 9.96%\n",
            "\n",
            "Image #788\n",
            " n02951585 - can_opener   23.64%\n",
            " n04372370 - switch       14.80%\n",
            " n04328186 - stopwatch    7.29%\n",
            "\n",
            "Image #789\n",
            " n02786058 - Band_Aid     24.94%\n",
            " n04357314 - sunscreen    14.29%\n",
            " n04591157 - Windsor_tie  6.03%\n",
            "\n",
            "Image #790\n",
            " n04372370 - switch       11.24%\n",
            " n04254120 - soap_dispenser 9.05%\n",
            " n03825788 - nipple       6.92%\n",
            "\n",
            "Image #791\n",
            " n04372370 - switch       25.63%\n",
            " n02951585 - can_opener   17.67%\n",
            " n04254120 - soap_dispenser 7.11%\n",
            "\n",
            "Image #792\n",
            " n03970156 - plunger      33.44%\n",
            " n02786058 - Band_Aid     12.72%\n",
            " n03250847 - drumstick    7.38%\n",
            "\n",
            "Image #793\n",
            " n02786058 - Band_Aid     23.77%\n",
            " n04357314 - sunscreen    22.08%\n",
            " n04591157 - Windsor_tie  21.40%\n",
            "\n",
            "Image #794\n",
            " n03250847 - drumstick    22.38%\n",
            " n04591157 - Windsor_tie  21.83%\n",
            " n03970156 - plunger      10.73%\n",
            "\n",
            "Image #795\n",
            " n02786058 - Band_Aid     42.63%\n",
            " n04591157 - Windsor_tie  17.05%\n",
            " n03250847 - drumstick    7.69%\n",
            "\n",
            "Image #796\n",
            " n04357314 - sunscreen    23.66%\n",
            " n04540053 - volleyball   20.20%\n",
            " n02786058 - Band_Aid     4.91%\n",
            "\n",
            "Image #797\n",
            " n04254120 - soap_dispenser 8.93%\n",
            " n03825788 - nipple       4.52%\n",
            " n02786058 - Band_Aid     4.09%\n",
            "\n",
            "Image #798\n",
            " n04372370 - switch       44.19%\n",
            " n02951585 - can_opener   10.73%\n",
            " n04254120 - soap_dispenser 8.75%\n",
            "\n",
            "Image #799\n",
            " n04591157 - Windsor_tie  13.65%\n",
            " n04357314 - sunscreen    6.62%\n",
            " n04254120 - soap_dispenser 6.37%\n",
            "\n",
            "Image #800\n",
            " n04372370 - switch       33.36%\n",
            " n04591157 - Windsor_tie  12.07%\n",
            " n02786058 - Band_Aid     8.88%\n",
            "\n",
            "Image #801\n",
            " n04270147 - spatula      18.25%\n",
            " n03720891 - maraca       14.92%\n",
            " n03970156 - plunger      6.84%\n",
            "\n",
            "Image #802\n",
            " n02786058 - Band_Aid     26.90%\n",
            " n03250847 - drumstick    12.08%\n",
            " n04357314 - sunscreen    7.77%\n",
            "\n",
            "Image #803\n",
            " n02786058 - Band_Aid     21.50%\n",
            " n04357314 - sunscreen    8.08%\n",
            " n04591157 - Windsor_tie  6.18%\n",
            "\n",
            "Image #804\n",
            " n04591157 - Windsor_tie  24.10%\n",
            " n04357314 - sunscreen    9.92%\n",
            " n02786058 - Band_Aid     4.84%\n",
            "\n",
            "Image #805\n",
            " n04591157 - Windsor_tie  28.91%\n",
            " n02786058 - Band_Aid     14.29%\n",
            " n04357314 - sunscreen    8.75%\n",
            "\n",
            "Image #806\n",
            " n04591157 - Windsor_tie  25.40%\n",
            " n02786058 - Band_Aid     17.19%\n",
            " n04357314 - sunscreen    13.47%\n",
            "\n",
            "Image #807\n",
            " n02786058 - Band_Aid     20.56%\n",
            " n04357314 - sunscreen    9.33%\n",
            " n04591157 - Windsor_tie  4.00%\n",
            "\n",
            "Image #808\n",
            " n03970156 - plunger      26.34%\n",
            " n04591157 - Windsor_tie  10.61%\n",
            " n03250847 - drumstick    4.85%\n",
            "\n",
            "Image #809\n",
            " n04597913 - wooden_spoon 20.51%\n",
            " n03970156 - plunger      14.94%\n",
            " n04270147 - spatula      9.87%\n",
            "\n",
            "Image #810\n",
            " n04357314 - sunscreen    29.68%\n",
            " n02786058 - Band_Aid     12.96%\n",
            " n04270147 - spatula      7.34%\n",
            "\n",
            "Image #811\n",
            " n04591157 - Windsor_tie  23.74%\n",
            " n04254120 - soap_dispenser 13.28%\n",
            " n04372370 - switch       5.11%\n",
            "\n",
            "Image #812\n",
            " n04357314 - sunscreen    20.95%\n",
            " n02786058 - Band_Aid     6.76%\n",
            " n04554684 - washer       5.39%\n",
            "\n",
            "Image #813\n",
            " n03970156 - plunger      29.83%\n",
            " n04270147 - spatula      17.41%\n",
            " n04591157 - Windsor_tie  6.57%\n",
            "\n",
            "Image #814\n",
            " n03970156 - plunger      13.21%\n",
            " n03250847 - drumstick    13.19%\n",
            " n04591157 - Windsor_tie  9.90%\n",
            "\n",
            "Image #815\n",
            " n04357314 - sunscreen    27.56%\n",
            " n02786058 - Band_Aid     18.93%\n",
            " n03314780 - face_powder  3.73%\n",
            "\n",
            "Image #816\n",
            " n02786058 - Band_Aid     13.53%\n",
            " n04591157 - Windsor_tie  11.51%\n",
            " n03903868 - pedestal     5.21%\n",
            "\n",
            "Image #817\n",
            " n02786058 - Band_Aid     25.65%\n",
            " n04591157 - Windsor_tie  12.42%\n",
            " n04357314 - sunscreen    9.49%\n",
            "\n",
            "Image #818\n",
            " n04591157 - Windsor_tie  25.19%\n",
            " n04357314 - sunscreen    16.68%\n",
            " n02786058 - Band_Aid     16.68%\n",
            "\n",
            "Image #819\n",
            " n02786058 - Band_Aid     17.80%\n",
            " n04591157 - Windsor_tie  13.54%\n",
            " n04357314 - sunscreen    7.67%\n",
            "\n",
            "Image #820\n",
            " n02786058 - Band_Aid     21.01%\n",
            " n04357314 - sunscreen    17.12%\n",
            " n04591157 - Windsor_tie  12.15%\n",
            "\n",
            "Image #821\n",
            " n04372370 - switch       6.96%\n",
            " n03041632 - cleaver      5.91%\n",
            " n02786058 - Band_Aid     5.48%\n",
            "\n",
            "Image #822\n",
            " n04357314 - sunscreen    14.38%\n",
            " n04591157 - Windsor_tie  14.22%\n",
            " n02786058 - Band_Aid     11.89%\n",
            "\n",
            "Image #823\n",
            " n03250847 - drumstick    20.84%\n",
            " n02786058 - Band_Aid     16.16%\n",
            " n03970156 - plunger      5.83%\n",
            "\n",
            "Image #824\n",
            " n03250847 - drumstick    25.40%\n",
            " n03970156 - plunger      14.98%\n",
            " n02786058 - Band_Aid     6.69%\n",
            "\n",
            "Image #825\n",
            " n03825788 - nipple       12.22%\n",
            " n02786058 - Band_Aid     10.43%\n",
            " n04254120 - soap_dispenser 8.80%\n",
            "\n",
            "Image #826\n",
            " n03250847 - drumstick    20.32%\n",
            " n04591157 - Windsor_tie  18.06%\n",
            " n02786058 - Band_Aid     8.33%\n",
            "\n",
            "Image #827\n",
            " n02786058 - Band_Aid     51.24%\n",
            " n04254120 - soap_dispenser 8.61%\n",
            " n04372370 - switch       3.71%\n",
            "\n",
            "Image #828\n",
            " n04591157 - Windsor_tie  33.53%\n",
            " n04357314 - sunscreen    16.21%\n",
            " n02786058 - Band_Aid     4.88%\n",
            "\n",
            "Image #829\n",
            " n04591157 - Windsor_tie  20.41%\n",
            " n02786058 - Band_Aid     13.50%\n",
            " n04270147 - spatula      11.75%\n",
            "\n",
            "Image #830\n",
            " n04591157 - Windsor_tie  38.35%\n",
            " n02786058 - Band_Aid     8.69%\n",
            " n03970156 - plunger      4.86%\n",
            "\n",
            "Image #831\n",
            " n04591157 - Windsor_tie  43.34%\n",
            " n02786058 - Band_Aid     22.60%\n",
            " n04357314 - sunscreen    7.85%\n",
            "\n",
            "Image #832\n",
            " n04357314 - sunscreen    16.87%\n",
            " n02786058 - Band_Aid     14.85%\n",
            " n04254120 - soap_dispenser 6.20%\n",
            "\n",
            "Image #833\n",
            " n02786058 - Band_Aid     22.63%\n",
            " n03970156 - plunger      16.68%\n",
            " n04591157 - Windsor_tie  11.90%\n",
            "\n",
            "Image #834\n",
            " n04591157 - Windsor_tie  23.82%\n",
            " n03250847 - drumstick    11.02%\n",
            " n02786058 - Band_Aid     9.07%\n",
            "\n",
            "Image #835\n",
            " n02786058 - Band_Aid     16.01%\n",
            " n04357314 - sunscreen    7.32%\n",
            " n04372370 - switch       6.50%\n",
            "\n",
            "Image #836\n",
            " n03250847 - drumstick    71.28%\n",
            " n03970156 - plunger      5.58%\n",
            " n03481172 - hammer       1.92%\n",
            "\n",
            "Image #837\n",
            " n03250847 - drumstick    57.68%\n",
            " n02951585 - can_opener   4.17%\n",
            " n03249569 - drum         3.34%\n",
            "\n",
            "Image #838\n",
            " n04591157 - Windsor_tie  11.11%\n",
            " n03942813 - ping-pong_ball 10.84%\n",
            " n02786058 - Band_Aid     10.69%\n",
            "\n",
            "Image #839\n",
            " n03970156 - plunger      37.41%\n",
            " n03250847 - drumstick    13.41%\n",
            " n04591157 - Windsor_tie  6.56%\n",
            "\n",
            "Image #840\n",
            " n04591157 - Windsor_tie  34.73%\n",
            " n02786058 - Band_Aid     14.44%\n",
            " n04372370 - switch       11.01%\n",
            "\n",
            "Image #841\n",
            " n04591157 - Windsor_tie  25.63%\n",
            " n02786058 - Band_Aid     8.32%\n",
            " n04357314 - sunscreen    6.69%\n",
            "\n",
            "Image #842\n",
            " n04357314 - sunscreen    25.02%\n",
            " n04591157 - Windsor_tie  11.18%\n",
            " n02786058 - Band_Aid     8.45%\n",
            "\n",
            "Image #843\n",
            " n02786058 - Band_Aid     47.73%\n",
            " n04357314 - sunscreen    7.13%\n",
            " n03970156 - plunger      4.58%\n",
            "\n",
            "Image #844\n",
            " n04357314 - sunscreen    34.83%\n",
            " n02786058 - Band_Aid     15.02%\n",
            " n04540053 - volleyball   2.58%\n",
            "\n",
            "Image #845\n",
            " n04254120 - soap_dispenser 19.62%\n",
            " n04591157 - Windsor_tie  17.51%\n",
            " n04372370 - switch       10.58%\n",
            "\n",
            "Image #846\n",
            " n04372370 - switch       68.06%\n",
            " n02786058 - Band_Aid     8.12%\n",
            " n04591157 - Windsor_tie  5.54%\n",
            "\n",
            "Image #847\n",
            " n02786058 - Band_Aid     32.16%\n",
            " n04372370 - switch       27.24%\n",
            " n04254120 - soap_dispenser 9.25%\n",
            "\n",
            "Image #848\n",
            " n03970156 - plunger      49.75%\n",
            " n03250847 - drumstick    15.06%\n",
            " n04591157 - Windsor_tie  6.95%\n",
            "\n",
            "Image #849\n",
            " n03250847 - drumstick    16.72%\n",
            " n03970156 - plunger      10.81%\n",
            " n04591157 - Windsor_tie  5.17%\n",
            "\n",
            "Image #850\n",
            " n04372370 - switch       32.48%\n",
            " n04591157 - Windsor_tie  11.92%\n",
            " n02786058 - Band_Aid     8.10%\n",
            "\n",
            "Image #851\n",
            " n02786058 - Band_Aid     18.91%\n",
            " n04357314 - sunscreen    18.86%\n",
            " n03250847 - drumstick    5.11%\n",
            "\n",
            "Image #852\n",
            " n02786058 - Band_Aid     28.82%\n",
            " n04357314 - sunscreen    17.68%\n",
            " n04591157 - Windsor_tie  13.69%\n",
            "\n",
            "Image #853\n",
            " n02786058 - Band_Aid     27.94%\n",
            " n04591157 - Windsor_tie  9.85%\n",
            " n04357314 - sunscreen    6.15%\n",
            "\n",
            "Image #854\n",
            " n04357314 - sunscreen    24.69%\n",
            " n02786058 - Band_Aid     12.63%\n",
            " n03314780 - face_powder  3.20%\n",
            "\n",
            "Image #855\n",
            " n04591157 - Windsor_tie  31.47%\n",
            " n03250847 - drumstick    18.89%\n",
            " n03970156 - plunger      16.05%\n",
            "\n",
            "Image #856\n",
            " n02786058 - Band_Aid     45.91%\n",
            " n04357314 - sunscreen    10.18%\n",
            " n03314780 - face_powder  3.07%\n",
            "\n",
            "Image #857\n",
            " n03970156 - plunger      35.13%\n",
            " n03250847 - drumstick    34.47%\n",
            " n02786058 - Band_Aid     8.48%\n",
            "\n",
            "Image #858\n",
            " n04357314 - sunscreen    36.53%\n",
            " n04591157 - Windsor_tie  11.48%\n",
            " n02786058 - Band_Aid     8.70%\n",
            "\n",
            "Image #859\n",
            " n04254120 - soap_dispenser 9.49%\n",
            " n04372370 - switch       7.95%\n",
            " n03825788 - nipple       5.75%\n",
            "\n",
            "Image #860\n",
            " n02786058 - Band_Aid     37.39%\n",
            " n04591157 - Windsor_tie  9.47%\n",
            " n03188531 - diaper       5.65%\n",
            "\n",
            "Image #861\n",
            " n04372370 - switch       31.73%\n",
            " n04591157 - Windsor_tie  26.07%\n",
            " n04254120 - soap_dispenser 10.29%\n",
            "\n",
            "Image #862\n",
            " n02786058 - Band_Aid     17.75%\n",
            " n04357314 - sunscreen    14.88%\n",
            " n04372370 - switch       11.81%\n",
            "\n",
            "Image #863\n",
            " n04591157 - Windsor_tie  14.97%\n",
            " n03970156 - plunger      10.30%\n",
            " n02786058 - Band_Aid     9.49%\n",
            "\n",
            "Image #864\n",
            " n03476991 - hair_spray   10.27%\n",
            " n03970156 - plunger      7.71%\n",
            " n04372370 - switch       5.86%\n",
            "\n",
            "Image #865\n",
            " n02786058 - Band_Aid     19.62%\n",
            " n04357314 - sunscreen    15.57%\n",
            " n04591157 - Windsor_tie  14.08%\n",
            "\n",
            "Image #866\n",
            " n04591157 - Windsor_tie  18.26%\n",
            " n04357314 - sunscreen    17.99%\n",
            " n02786058 - Band_Aid     7.44%\n",
            "\n",
            "Image #867\n",
            " n04372370 - switch       19.57%\n",
            " n02951585 - can_opener   11.99%\n",
            " n04591157 - Windsor_tie  6.28%\n",
            "\n",
            "Image #868\n",
            " n04372370 - switch       32.41%\n",
            " n04591157 - Windsor_tie  10.76%\n",
            " n04254120 - soap_dispenser 10.67%\n",
            "\n",
            "Image #869\n",
            " n04357314 - sunscreen    16.75%\n",
            " n04591157 - Windsor_tie  16.65%\n",
            " n02786058 - Band_Aid     11.83%\n",
            "\n",
            "Image #870\n",
            " n04591157 - Windsor_tie  32.75%\n",
            " n02786058 - Band_Aid     5.34%\n",
            " n04357314 - sunscreen    5.30%\n",
            "\n",
            "Image #871\n",
            " n03250847 - drumstick    26.68%\n",
            " n02786058 - Band_Aid     16.16%\n",
            " n03970156 - plunger      10.87%\n",
            "\n",
            "Image #872\n",
            " n03970156 - plunger      61.95%\n",
            " n03250847 - drumstick    13.48%\n",
            " n04270147 - spatula      3.42%\n",
            "\n",
            "Image #873\n",
            " n04357314 - sunscreen    32.95%\n",
            " n04591157 - Windsor_tie  11.13%\n",
            " n02786058 - Band_Aid     8.18%\n",
            "\n",
            "Image #874\n",
            " n04270147 - spatula      12.37%\n",
            " n04591157 - Windsor_tie  11.03%\n",
            " n04357314 - sunscreen    6.08%\n",
            "\n",
            "Image #875\n",
            " n04372370 - switch       9.79%\n",
            " n04254120 - soap_dispenser 9.25%\n",
            " n04591157 - Windsor_tie  5.75%\n",
            "\n",
            "Image #876\n",
            " n03970156 - plunger      42.26%\n",
            " n03250847 - drumstick    36.30%\n",
            " n04597913 - wooden_spoon 2.40%\n",
            "\n",
            "Image #877\n",
            " n03838899 - oboe         12.44%\n",
            " n02786058 - Band_Aid     12.41%\n",
            " n03250847 - drumstick    10.22%\n",
            "\n",
            "Image #878\n",
            " n03970156 - plunger      19.93%\n",
            " n04591157 - Windsor_tie  12.78%\n",
            " n03720891 - maraca       9.53%\n",
            "\n",
            "Image #879\n",
            " n04372370 - switch       42.80%\n",
            " n04254120 - soap_dispenser 12.14%\n",
            " n02786058 - Band_Aid     4.77%\n",
            "\n",
            "Image #880\n",
            " n03250847 - drumstick    27.71%\n",
            " n03970156 - plunger      17.25%\n",
            " n04591157 - Windsor_tie  12.99%\n",
            "\n",
            "Image #881\n",
            " n04254120 - soap_dispenser 19.74%\n",
            " n04372370 - switch       11.60%\n",
            " n02951585 - can_opener   7.49%\n",
            "\n",
            "Image #882\n",
            " n04254120 - soap_dispenser 15.77%\n",
            " n02951585 - can_opener   6.02%\n",
            " n03825788 - nipple       5.71%\n",
            "\n",
            "Image #883\n",
            " n04270147 - spatula      27.02%\n",
            " n04591157 - Windsor_tie  18.96%\n",
            " n03970156 - plunger      9.32%\n",
            "\n",
            "Image #884\n",
            " n02786058 - Band_Aid     16.21%\n",
            " n04357314 - sunscreen    12.54%\n",
            " n03970156 - plunger      12.27%\n",
            "\n",
            "Image #885\n",
            " n04591157 - Windsor_tie  30.78%\n",
            " n04357314 - sunscreen    9.75%\n",
            " n02786058 - Band_Aid     8.31%\n",
            "\n",
            "Image #886\n",
            " n04357314 - sunscreen    14.59%\n",
            " n04270147 - spatula      8.37%\n",
            " n04591157 - Windsor_tie  6.36%\n",
            "\n",
            "Image #887\n",
            " n04254120 - soap_dispenser 13.28%\n",
            " n04372370 - switch       9.34%\n",
            " n02786058 - Band_Aid     6.14%\n",
            "\n",
            "Image #888\n",
            " n04357314 - sunscreen    32.33%\n",
            " n04591157 - Windsor_tie  11.69%\n",
            " n04540053 - volleyball   4.70%\n",
            "\n",
            "Image #889\n",
            " n03970156 - plunger      50.26%\n",
            " n03250847 - drumstick    31.09%\n",
            " n02786058 - Band_Aid     3.01%\n",
            "\n",
            "Image #890\n",
            " n04357314 - sunscreen    6.35%\n",
            " n03970156 - plunger      4.57%\n",
            " n04270147 - spatula      4.39%\n",
            "\n",
            "Image #891\n",
            " n04357314 - sunscreen    16.20%\n",
            " n04591157 - Windsor_tie  13.66%\n",
            " n02786058 - Band_Aid     12.44%\n",
            "\n",
            "Image #892\n",
            " n04357314 - sunscreen    18.26%\n",
            " n02786058 - Band_Aid     11.60%\n",
            " n03314780 - face_powder  3.76%\n",
            "\n",
            "Image #893\n",
            " n04357314 - sunscreen    29.17%\n",
            " n02786058 - Band_Aid     16.54%\n",
            " n03314780 - face_powder  3.72%\n",
            "\n",
            "Image #894\n",
            " n03970156 - plunger      16.98%\n",
            " n04372370 - switch       11.93%\n",
            " n02786058 - Band_Aid     10.64%\n",
            "\n",
            "Image #895\n",
            " n04357314 - sunscreen    36.66%\n",
            " n02786058 - Band_Aid     24.30%\n",
            " n04371430 - swimming_trunks 3.36%\n",
            "\n",
            "Image #896\n",
            " n04591157 - Windsor_tie  35.93%\n",
            " n02951585 - can_opener   16.00%\n",
            " n02786058 - Band_Aid     8.28%\n",
            "\n",
            "Image #897\n",
            " n03970156 - plunger      25.61%\n",
            " n04591157 - Windsor_tie  11.27%\n",
            " n04270147 - spatula      6.01%\n",
            "\n",
            "Image #898\n",
            " n03250847 - drumstick    30.17%\n",
            " n03970156 - plunger      17.43%\n",
            " n02786058 - Band_Aid     12.25%\n",
            "\n",
            "Image #899\n",
            " n02786058 - Band_Aid     25.84%\n",
            " n04357314 - sunscreen    16.81%\n",
            " n04591157 - Windsor_tie  12.69%\n",
            "\n",
            "Image #900\n",
            " n03970156 - plunger      15.58%\n",
            " n04591157 - Windsor_tie  13.37%\n",
            " n02786058 - Band_Aid     9.78%\n",
            "\n",
            "Image #901\n",
            " n04372370 - switch       16.80%\n",
            " n03970156 - plunger      15.93%\n",
            " n02951585 - can_opener   8.37%\n",
            "\n",
            "Image #902\n",
            " n02786058 - Band_Aid     23.59%\n",
            " n04357314 - sunscreen    10.53%\n",
            " n04591157 - Windsor_tie  6.52%\n",
            "\n",
            "Image #903\n",
            " n02786058 - Band_Aid     30.48%\n",
            " n04357314 - sunscreen    27.74%\n",
            " n03314780 - face_powder  5.27%\n",
            "\n",
            "Image #904\n",
            " n03250847 - drumstick    47.13%\n",
            " n03970156 - plunger      19.78%\n",
            " n02786058 - Band_Aid     3.86%\n",
            "\n",
            "Image #905\n",
            " n02786058 - Band_Aid     21.23%\n",
            " n04357314 - sunscreen    15.64%\n",
            " n02667093 - abaya        7.46%\n",
            "\n",
            "Image #906\n",
            " n04357314 - sunscreen    23.16%\n",
            " n02786058 - Band_Aid     8.85%\n",
            " n04591157 - Windsor_tie  5.92%\n",
            "\n",
            "Image #907\n",
            " n04357314 - sunscreen    20.00%\n",
            " n02786058 - Band_Aid     11.69%\n",
            " n04591157 - Windsor_tie  9.16%\n",
            "\n",
            "Image #908\n",
            " n02786058 - Band_Aid     10.95%\n",
            " n03970156 - plunger      6.90%\n",
            " n04591157 - Windsor_tie  4.45%\n",
            "\n",
            "Image #909\n",
            " n04254120 - soap_dispenser 8.75%\n",
            " n02786058 - Band_Aid     8.28%\n",
            " n04372370 - switch       6.82%\n",
            "\n",
            "Image #910\n",
            " n02786058 - Band_Aid     55.44%\n",
            " n04372370 - switch       7.10%\n",
            " n03314780 - face_powder  6.59%\n",
            "\n",
            "Image #911\n",
            " n03970156 - plunger      22.07%\n",
            " n02786058 - Band_Aid     9.28%\n",
            " n04372370 - switch       7.38%\n",
            "\n",
            "Image #912\n",
            " n04357314 - sunscreen    22.11%\n",
            " n03250847 - drumstick    9.77%\n",
            " n04591157 - Windsor_tie  8.09%\n",
            "\n",
            "Image #913\n",
            " n03250847 - drumstick    21.97%\n",
            " n03970156 - plunger      11.71%\n",
            " n02786058 - Band_Aid     9.52%\n",
            "\n",
            "Image #914\n",
            " n04372370 - switch       13.77%\n",
            " n02786058 - Band_Aid     11.52%\n",
            " n04254120 - soap_dispenser 4.98%\n",
            "\n",
            "Image #915\n",
            " n04372370 - switch       33.36%\n",
            " n04591157 - Windsor_tie  12.07%\n",
            " n02786058 - Band_Aid     8.88%\n",
            "\n",
            "Image #916\n",
            " n04357314 - sunscreen    38.58%\n",
            " n04540053 - volleyball   7.48%\n",
            " n03690938 - lotion       6.92%\n",
            "\n",
            "Image #917\n",
            " n03250847 - drumstick    20.75%\n",
            " n03970156 - plunger      18.16%\n",
            " n02786058 - Band_Aid     5.12%\n",
            "\n",
            "Image #918\n",
            " n04372370 - switch       19.94%\n",
            " n02786058 - Band_Aid     11.05%\n",
            " n02951585 - can_opener   10.56%\n",
            "\n",
            "Image #919\n",
            " n02786058 - Band_Aid     17.08%\n",
            " n03970156 - plunger      13.00%\n",
            " n03250847 - drumstick    10.67%\n",
            "\n",
            "Image #920\n",
            " n03970156 - plunger      48.13%\n",
            " n03250847 - drumstick    6.93%\n",
            " n02786058 - Band_Aid     4.71%\n",
            "\n",
            "Image #921\n",
            " n02786058 - Band_Aid     20.07%\n",
            " n04372370 - switch       16.46%\n",
            " n03485407 - hand-held_computer 7.84%\n",
            "\n",
            "Image #922\n",
            " n04209239 - shower_curtain 7.79%\n",
            " n04254120 - soap_dispenser 6.01%\n",
            " n04493381 - tub          4.47%\n",
            "\n",
            "Image #923\n",
            " n03970156 - plunger      13.61%\n",
            " n03250847 - drumstick    9.86%\n",
            " n04591157 - Windsor_tie  8.98%\n",
            "\n",
            "Image #924\n",
            " n04357314 - sunscreen    19.54%\n",
            " n02786058 - Band_Aid     12.10%\n",
            " n04591157 - Windsor_tie  5.68%\n",
            "\n",
            "Image #925\n",
            " n02786058 - Band_Aid     25.63%\n",
            " n04591157 - Windsor_tie  14.25%\n",
            " n03250847 - drumstick    5.83%\n",
            "\n",
            "Image #926\n",
            " n02786058 - Band_Aid     9.54%\n",
            " n04591157 - Windsor_tie  6.48%\n",
            " n03970156 - plunger      6.48%\n",
            "\n",
            "Image #927\n",
            " n03250847 - drumstick    25.33%\n",
            " n03970156 - plunger      20.83%\n",
            " n04591157 - Windsor_tie  7.50%\n",
            "\n",
            "Image #928\n",
            " n02786058 - Band_Aid     17.44%\n",
            " n04372370 - switch       12.17%\n",
            " n04591157 - Windsor_tie  8.50%\n",
            "\n",
            "Image #929\n",
            " n04357314 - sunscreen    27.63%\n",
            " n02786058 - Band_Aid     10.46%\n",
            " n04254120 - soap_dispenser 5.18%\n",
            "\n",
            "Image #930\n",
            " n04357314 - sunscreen    27.37%\n",
            " n02786058 - Band_Aid     16.55%\n",
            " n04540053 - volleyball   8.08%\n",
            "\n",
            "Image #931\n",
            " n02786058 - Band_Aid     17.07%\n",
            " n03250847 - drumstick    16.29%\n",
            " n03970156 - plunger      7.90%\n",
            "\n",
            "Image #932\n",
            " n02786058 - Band_Aid     15.05%\n",
            " n04357314 - sunscreen    14.63%\n",
            " n04591157 - Windsor_tie  5.05%\n",
            "\n",
            "Image #933\n",
            " n04372370 - switch       28.95%\n",
            " n04254120 - soap_dispenser 14.96%\n",
            " n02951585 - can_opener   7.09%\n",
            "\n",
            "Image #934\n",
            " n04591157 - Windsor_tie  19.31%\n",
            " n03250847 - drumstick    19.20%\n",
            " n03970156 - plunger      12.08%\n",
            "\n",
            "Image #935\n",
            " n03970156 - plunger      43.41%\n",
            " n03250847 - drumstick    17.14%\n",
            " n02786058 - Band_Aid     7.59%\n",
            "\n",
            "Image #936\n",
            " n02786058 - Band_Aid     12.53%\n",
            " n04357314 - sunscreen    7.16%\n",
            " n02808440 - bathtub      5.32%\n",
            "\n",
            "Image #937\n",
            " n04591157 - Windsor_tie  15.60%\n",
            " n04023962 - punching_bag 9.92%\n",
            " n03970156 - plunger      6.88%\n",
            "\n",
            "Image #938\n",
            " n03250847 - drumstick    31.69%\n",
            " n02786058 - Band_Aid     12.68%\n",
            " n04357314 - sunscreen    9.96%\n",
            "\n",
            "Image #939\n",
            " n04372370 - switch       33.74%\n",
            " n04591157 - Windsor_tie  17.41%\n",
            " n02786058 - Band_Aid     12.92%\n",
            "\n",
            "Image #940\n",
            " n04357314 - sunscreen    19.80%\n",
            " n02786058 - Band_Aid     15.68%\n",
            " n02951585 - can_opener   10.84%\n",
            "\n",
            "Image #941\n",
            " n03250847 - drumstick    18.61%\n",
            " n03970156 - plunger      12.75%\n",
            " n02786058 - Band_Aid     10.03%\n",
            "\n",
            "Image #942\n",
            " n03970156 - plunger      18.75%\n",
            " n02786058 - Band_Aid     14.49%\n",
            " n03250847 - drumstick    8.15%\n",
            "\n",
            "Image #943\n",
            " n03970156 - plunger      29.77%\n",
            " n03250847 - drumstick    11.54%\n",
            " n02786058 - Band_Aid     10.02%\n",
            "\n",
            "Image #944\n",
            " n02786058 - Band_Aid     13.32%\n",
            " n03250847 - drumstick    11.64%\n",
            " n04357314 - sunscreen    10.42%\n",
            "\n",
            "Image #945\n",
            " n04591157 - Windsor_tie  34.04%\n",
            " n02951585 - can_opener   12.90%\n",
            " n02786058 - Band_Aid     8.06%\n",
            "\n",
            "Image #946\n",
            " n03250847 - drumstick    28.53%\n",
            " n02786058 - Band_Aid     16.80%\n",
            " n04591157 - Windsor_tie  5.25%\n",
            "\n",
            "Image #947\n",
            " n03970156 - plunger      30.74%\n",
            " n03250847 - drumstick    21.33%\n",
            " n04591157 - Windsor_tie  6.21%\n",
            "\n",
            "Image #948\n",
            " n04591157 - Windsor_tie  39.51%\n",
            " n04254120 - soap_dispenser 18.05%\n",
            " n04357314 - sunscreen    6.76%\n",
            "\n",
            "Image #949\n",
            " n03970156 - plunger      28.53%\n",
            " n03250847 - drumstick    8.91%\n",
            " n02786058 - Band_Aid     7.28%\n",
            "\n",
            "Image #950\n",
            " n02786058 - Band_Aid     17.46%\n",
            " n04357314 - sunscreen    10.54%\n",
            " n03250847 - drumstick    5.12%\n",
            "\n",
            "Image #951\n",
            " n04357314 - sunscreen    40.68%\n",
            " n02786058 - Band_Aid     14.33%\n",
            " n04254120 - soap_dispenser 10.21%\n",
            "\n",
            "Image #952\n",
            " n04591157 - Windsor_tie  35.20%\n",
            " n02786058 - Band_Aid     19.14%\n",
            " n03720891 - maraca       4.18%\n",
            "\n",
            "Image #953\n",
            " n04591157 - Windsor_tie  27.69%\n",
            " n02786058 - Band_Aid     16.48%\n",
            " n04357314 - sunscreen    6.16%\n",
            "\n",
            "Image #954\n",
            " n03250847 - drumstick    27.15%\n",
            " n03970156 - plunger      18.12%\n",
            " n04591157 - Windsor_tie  13.52%\n",
            "\n",
            "Image #955\n",
            " n03970156 - plunger      17.39%\n",
            " n04270147 - spatula      15.31%\n",
            " n03720891 - maraca       6.74%\n",
            "\n",
            "Image #956\n",
            " n04357314 - sunscreen    26.06%\n",
            " n02786058 - Band_Aid     7.68%\n",
            " n04591157 - Windsor_tie  6.56%\n",
            "\n",
            "Image #957\n",
            " n04591157 - Windsor_tie  28.33%\n",
            " n04254120 - soap_dispenser 15.34%\n",
            " n02951585 - can_opener   13.98%\n",
            "\n",
            "Image #958\n",
            " n03970156 - plunger      54.68%\n",
            " n03250847 - drumstick    9.68%\n",
            " n04597913 - wooden_spoon 5.83%\n",
            "\n",
            "Image #959\n",
            " n04357314 - sunscreen    17.58%\n",
            " n02786058 - Band_Aid     13.19%\n",
            " n04591157 - Windsor_tie  10.12%\n",
            "\n",
            "Image #960\n",
            " n03250847 - drumstick    30.29%\n",
            " n03970156 - plunger      16.25%\n",
            " n02786058 - Band_Aid     13.64%\n",
            "\n",
            "Image #961\n",
            " n04591157 - Windsor_tie  22.23%\n",
            " n04254120 - soap_dispenser 11.18%\n",
            " n02786058 - Band_Aid     5.73%\n",
            "\n",
            "Image #962\n",
            " n04357314 - sunscreen    22.19%\n",
            " n02786058 - Band_Aid     16.24%\n",
            " n04591157 - Windsor_tie  8.77%\n",
            "\n",
            "Image #963\n",
            " n04591157 - Windsor_tie  58.28%\n",
            " n03970156 - plunger      8.09%\n",
            " n03250847 - drumstick    4.47%\n",
            "\n",
            "Image #964\n",
            " n02786058 - Band_Aid     62.72%\n",
            " n03970156 - plunger      3.86%\n",
            " n04357314 - sunscreen    3.74%\n",
            "\n",
            "Image #965\n",
            " n03970156 - plunger      32.28%\n",
            " n03720891 - maraca       11.47%\n",
            " n04270147 - spatula      9.79%\n",
            "\n",
            "Image #966\n",
            " n02786058 - Band_Aid     39.62%\n",
            " n04357314 - sunscreen    31.95%\n",
            " n04372370 - switch       4.34%\n",
            "\n",
            "Image #967\n",
            " n04591157 - Windsor_tie  31.08%\n",
            " n02786058 - Band_Aid     29.64%\n",
            " n03970156 - plunger      8.30%\n",
            "\n",
            "Image #968\n",
            " n03250847 - drumstick    36.20%\n",
            " n04591157 - Windsor_tie  16.23%\n",
            " n02786058 - Band_Aid     9.88%\n",
            "\n",
            "Image #969\n",
            " n03250847 - drumstick    35.08%\n",
            " n03658185 - letter_opener 10.84%\n",
            " n01704323 - triceratops  7.31%\n",
            "\n",
            "Image #970\n",
            " n04357314 - sunscreen    19.78%\n",
            " n02786058 - Band_Aid     15.26%\n",
            " n04540053 - volleyball   3.93%\n",
            "\n",
            "Image #971\n",
            " n02786058 - Band_Aid     24.87%\n",
            " n03250847 - drumstick    11.66%\n",
            " n04357314 - sunscreen    9.05%\n",
            "\n",
            "Image #972\n",
            " n03825788 - nipple       25.04%\n",
            " n03476991 - hair_spray   6.58%\n",
            " n03676483 - lipstick     5.02%\n",
            "\n",
            "Image #973\n",
            " n02786058 - Band_Aid     37.31%\n",
            " n04357314 - sunscreen    12.32%\n",
            " n04254120 - soap_dispenser 10.47%\n",
            "\n",
            "Image #974\n",
            " n03970156 - plunger      23.38%\n",
            " n04357314 - sunscreen    11.55%\n",
            " n02786058 - Band_Aid     6.32%\n",
            "\n",
            "Image #975\n",
            " n04591157 - Windsor_tie  26.71%\n",
            " n03250847 - drumstick    21.03%\n",
            " n03970156 - plunger      12.31%\n",
            "\n",
            "Image #976\n",
            " n02786058 - Band_Aid     47.90%\n",
            " n04357314 - sunscreen    27.83%\n",
            " n04372370 - switch       2.82%\n",
            "\n",
            "Image #977\n",
            " n04591157 - Windsor_tie  59.56%\n",
            " n03970156 - plunger      4.80%\n",
            " n03250847 - drumstick    4.39%\n",
            "\n",
            "Image #978\n",
            " n03250847 - drumstick    21.76%\n",
            " n04357314 - sunscreen    12.03%\n",
            " n02786058 - Band_Aid     11.94%\n",
            "\n",
            "Image #979\n",
            " n04357314 - sunscreen    19.68%\n",
            " n02786058 - Band_Aid     19.23%\n",
            " n04591157 - Windsor_tie  6.05%\n",
            "\n",
            "Image #980\n",
            " n04372370 - switch       17.82%\n",
            " n04591157 - Windsor_tie  17.28%\n",
            " n02786058 - Band_Aid     10.32%\n",
            "\n",
            "Image #981\n",
            " n04357314 - sunscreen    31.46%\n",
            " n02786058 - Band_Aid     13.07%\n",
            " n02951585 - can_opener   4.43%\n",
            "\n",
            "Image #982\n",
            " n04357314 - sunscreen    24.80%\n",
            " n02786058 - Band_Aid     10.36%\n",
            " n04591157 - Windsor_tie  7.82%\n",
            "\n",
            "Image #983\n",
            " n02786058 - Band_Aid     29.29%\n",
            " n04357314 - sunscreen    17.82%\n",
            " n04591157 - Windsor_tie  3.42%\n",
            "\n",
            "Image #984\n",
            " n02786058 - Band_Aid     17.24%\n",
            " n04357314 - sunscreen    13.01%\n",
            " n03188531 - diaper       6.04%\n",
            "\n",
            "Image #985\n",
            " n04372370 - switch       57.61%\n",
            " n02951585 - can_opener   5.22%\n",
            " n03476991 - hair_spray   5.01%\n",
            "\n",
            "Image #986\n",
            " n02786058 - Band_Aid     25.93%\n",
            " n03970156 - plunger      12.82%\n",
            " n04597913 - wooden_spoon 5.28%\n",
            "\n",
            "Image #987\n",
            " n04591157 - Windsor_tie  10.96%\n",
            " n03250847 - drumstick    9.61%\n",
            " n04357314 - sunscreen    8.45%\n",
            "\n",
            "Image #988\n",
            " n03250847 - drumstick    21.59%\n",
            " n03970156 - plunger      13.13%\n",
            " n04357314 - sunscreen    11.62%\n",
            "\n",
            "Image #989\n",
            " n02786058 - Band_Aid     23.58%\n",
            " n03970156 - plunger      14.95%\n",
            " n03250847 - drumstick    12.49%\n",
            "\n",
            "Image #990\n",
            " n02786058 - Band_Aid     21.57%\n",
            " n04357314 - sunscreen    12.17%\n",
            " n04591157 - Windsor_tie  5.49%\n",
            "\n",
            "Image #991\n",
            " n03250847 - drumstick    34.44%\n",
            " n02951585 - can_opener   8.84%\n",
            " n04254120 - soap_dispenser 4.10%\n",
            "\n",
            "Image #992\n",
            " n04357314 - sunscreen    10.97%\n",
            " n04591157 - Windsor_tie  8.71%\n",
            " n02786058 - Band_Aid     7.00%\n",
            "\n",
            "Image #993\n",
            " n04357314 - sunscreen    19.19%\n",
            " n02786058 - Band_Aid     13.91%\n",
            " n04591157 - Windsor_tie  12.11%\n",
            "\n",
            "Image #994\n",
            " n04270147 - spatula      13.08%\n",
            " n03970156 - plunger      8.84%\n",
            " n02786058 - Band_Aid     8.78%\n",
            "\n",
            "Image #995\n",
            " n04591157 - Windsor_tie  11.98%\n",
            " n03250847 - drumstick    8.69%\n",
            " n04357314 - sunscreen    6.94%\n",
            "\n",
            "Image #996\n",
            " n02786058 - Band_Aid     30.80%\n",
            " n04591157 - Windsor_tie  28.70%\n",
            " n04357314 - sunscreen    9.00%\n",
            "\n",
            "Image #997\n",
            " n04591157 - Windsor_tie  33.39%\n",
            " n03970156 - plunger      10.53%\n",
            " n04270147 - spatula      5.45%\n",
            "\n",
            "Image #998\n",
            " n03970156 - plunger      21.01%\n",
            " n02786058 - Band_Aid     12.17%\n",
            " n04270147 - spatula      7.45%\n",
            "\n",
            "Image #999\n",
            " n04357314 - sunscreen    11.89%\n",
            " n04591157 - Windsor_tie  10.69%\n",
            " n03970156 - plunger      8.35%\n",
            "\n",
            "Image #1000\n",
            " n04591157 - Windsor_tie  14.15%\n",
            " n03970156 - plunger      12.84%\n",
            " n03876231 - paintbrush   10.24%\n",
            "\n",
            "Image #1001\n",
            " n03250847 - drumstick    69.80%\n",
            " n02786058 - Band_Aid     6.50%\n",
            " n03970156 - plunger      1.86%\n",
            "\n",
            "Image #1002\n",
            " n04357314 - sunscreen    17.27%\n",
            " n04591157 - Windsor_tie  15.21%\n",
            " n02786058 - Band_Aid     7.92%\n",
            "\n",
            "Image #1003\n",
            " n04357314 - sunscreen    17.13%\n",
            " n02786058 - Band_Aid     11.27%\n",
            " n04209239 - shower_curtain 8.68%\n",
            "\n",
            "Image #1004\n",
            " n03970156 - plunger      18.85%\n",
            " n04591157 - Windsor_tie  15.70%\n",
            " n02786058 - Band_Aid     14.32%\n",
            "\n",
            "Image #1005\n",
            " n03970156 - plunger      55.43%\n",
            " n03250847 - drumstick    6.92%\n",
            " n04591157 - Windsor_tie  4.65%\n",
            "\n",
            "Image #1006\n",
            " n04357314 - sunscreen    42.50%\n",
            " n02786058 - Band_Aid     20.46%\n",
            " n04591157 - Windsor_tie  7.82%\n",
            "\n",
            "Image #1007\n",
            " n02786058 - Band_Aid     18.85%\n",
            " n03970156 - plunger      13.94%\n",
            " n03250847 - drumstick    12.89%\n",
            "\n",
            "Image #1008\n",
            " n04357314 - sunscreen    11.99%\n",
            " n02786058 - Band_Aid     8.59%\n",
            " n04591157 - Windsor_tie  8.38%\n",
            "\n",
            "Image #1009\n",
            " n04591157 - Windsor_tie  38.60%\n",
            " n02786058 - Band_Aid     19.25%\n",
            " n04254120 - soap_dispenser 3.91%\n",
            "\n",
            "Image #1010\n",
            " n02786058 - Band_Aid     32.64%\n",
            " n04357314 - sunscreen    15.71%\n",
            " n03250847 - drumstick    5.34%\n",
            "\n",
            "Image #1011\n",
            " n03250847 - drumstick    22.94%\n",
            " n04591157 - Windsor_tie  19.49%\n",
            " n03970156 - plunger      18.70%\n",
            "\n",
            "Image #1012\n",
            " n04372370 - switch       19.57%\n",
            " n02951585 - can_opener   11.99%\n",
            " n04591157 - Windsor_tie  6.28%\n",
            "\n",
            "Image #1013\n",
            " n04372370 - switch       31.61%\n",
            " n04254120 - soap_dispenser 13.99%\n",
            " n03476991 - hair_spray   7.11%\n",
            "\n",
            "Image #1014\n",
            " n04591157 - Windsor_tie  22.72%\n",
            " n02786058 - Band_Aid     12.95%\n",
            " n03250847 - drumstick    12.24%\n",
            "\n",
            "Image #1015\n",
            " n03970156 - plunger      24.44%\n",
            " n04591157 - Windsor_tie  11.35%\n",
            " n03250847 - drumstick    10.15%\n",
            "\n",
            "Image #1016\n",
            " n04357314 - sunscreen    20.19%\n",
            " n02786058 - Band_Aid     16.15%\n",
            " n04591157 - Windsor_tie  12.86%\n",
            "\n",
            "Image #1017\n",
            " n04591157 - Windsor_tie  19.68%\n",
            " n04357314 - sunscreen    13.17%\n",
            " n02786058 - Band_Aid     7.02%\n",
            "\n",
            "Image #1018\n",
            " n03825788 - nipple       12.22%\n",
            " n02786058 - Band_Aid     10.43%\n",
            " n04254120 - soap_dispenser 8.80%\n",
            "\n",
            "Image #1019\n",
            " n02786058 - Band_Aid     9.91%\n",
            " n04328186 - stopwatch    9.56%\n",
            " n03929660 - pick         7.28%\n",
            "\n",
            "Image #1020\n",
            " n04591157 - Windsor_tie  56.83%\n",
            " n02786058 - Band_Aid     12.30%\n",
            " n04372370 - switch       4.11%\n",
            "\n",
            "Image #1021\n",
            " n03970156 - plunger      18.86%\n",
            " n03250847 - drumstick    15.52%\n",
            " n04270147 - spatula      13.69%\n",
            "\n",
            "Image #1022\n",
            " n04357314 - sunscreen    12.31%\n",
            " n04254120 - soap_dispenser 8.47%\n",
            " n04591157 - Windsor_tie  8.34%\n",
            "\n",
            "Image #1023\n",
            " n02786058 - Band_Aid     59.58%\n",
            " n04357314 - sunscreen    23.61%\n",
            " n03690938 - lotion       1.88%\n",
            "\n",
            "Image #1024\n",
            " n02786058 - Band_Aid     43.02%\n",
            " n04357314 - sunscreen    11.26%\n",
            " n04591157 - Windsor_tie  3.93%\n",
            "\n",
            "Image #1025\n",
            " n02786058 - Band_Aid     17.03%\n",
            " n04357314 - sunscreen    16.01%\n",
            " n04591157 - Windsor_tie  8.56%\n",
            "\n",
            "Image #1026\n",
            " n02786058 - Band_Aid     24.39%\n",
            " n04357314 - sunscreen    16.05%\n",
            " n03314780 - face_powder  6.45%\n",
            "\n",
            "Image #1027\n",
            " n02786058 - Band_Aid     14.70%\n",
            " n04357314 - sunscreen    11.98%\n",
            " n04591157 - Windsor_tie  5.79%\n",
            "\n",
            "Image #1028\n",
            " n04591157 - Windsor_tie  20.56%\n",
            " n02786058 - Band_Aid     15.27%\n",
            " n04357314 - sunscreen    6.73%\n",
            "\n",
            "Image #1029\n",
            " n03476991 - hair_spray   6.38%\n",
            " n02951585 - can_opener   6.15%\n",
            " n04328186 - stopwatch    5.27%\n",
            "\n",
            "Image #1030\n",
            " n02786058 - Band_Aid     27.99%\n",
            " n04357314 - sunscreen    16.76%\n",
            " n03314780 - face_powder  4.48%\n",
            "\n",
            "Image #1031\n",
            " n02786058 - Band_Aid     22.31%\n",
            " n04357314 - sunscreen    14.19%\n",
            " n04372370 - switch       5.66%\n",
            "\n",
            "Image #1032\n",
            " n03250847 - drumstick    15.22%\n",
            " n02786058 - Band_Aid     11.54%\n",
            " n03970156 - plunger      8.39%\n",
            "\n",
            "Image #1033\n",
            " n02786058 - Band_Aid     17.45%\n",
            " n04357314 - sunscreen    5.95%\n",
            " n04591157 - Windsor_tie  5.61%\n",
            "\n",
            "Image #1034\n",
            " n02786058 - Band_Aid     16.45%\n",
            " n04591157 - Windsor_tie  11.02%\n",
            " n03970156 - plunger      10.46%\n",
            "\n",
            "Image #1035\n",
            " n02951585 - can_opener   14.82%\n",
            " n03476991 - hair_spray   7.85%\n",
            " n04591157 - Windsor_tie  6.11%\n",
            "\n",
            "Image #1036\n",
            " n02786058 - Band_Aid     33.94%\n",
            " n04591157 - Windsor_tie  8.72%\n",
            " n04357314 - sunscreen    6.54%\n",
            "\n",
            "Image #1037\n",
            " n03250847 - drumstick    17.23%\n",
            " n04254120 - soap_dispenser 14.42%\n",
            " n04372370 - switch       9.87%\n",
            "\n",
            "Image #1038\n",
            " n02786058 - Band_Aid     17.82%\n",
            " n04372370 - switch       14.24%\n",
            " n04591157 - Windsor_tie  12.74%\n",
            "\n",
            "Image #1039\n",
            " n02786058 - Band_Aid     90.91%\n",
            " n04591157 - Windsor_tie  4.10%\n",
            " n03970156 - plunger      0.65%\n",
            "\n",
            "Image #1040\n",
            " n03970156 - plunger      35.02%\n",
            " n03250847 - drumstick    15.87%\n",
            " n02786058 - Band_Aid     5.76%\n",
            "\n",
            "Image #1041\n",
            " n03970156 - plunger      12.99%\n",
            " n02786058 - Band_Aid     9.97%\n",
            " n04591157 - Windsor_tie  5.81%\n",
            "\n",
            "Image #1042\n",
            " n04357314 - sunscreen    21.67%\n",
            " n04372370 - switch       19.10%\n",
            " n02786058 - Band_Aid     8.20%\n",
            "\n",
            "Image #1043\n",
            " n02786058 - Band_Aid     19.43%\n",
            " n04357314 - sunscreen    17.42%\n",
            " n03314780 - face_powder  3.69%\n",
            "\n",
            "Image #1044\n",
            " n04372370 - switch       44.19%\n",
            " n02951585 - can_opener   10.73%\n",
            " n04254120 - soap_dispenser 8.75%\n",
            "\n",
            "Image #1045\n",
            " n04591157 - Windsor_tie  30.84%\n",
            " n02786058 - Band_Aid     17.87%\n",
            " n04357314 - sunscreen    9.00%\n",
            "\n",
            "Image #1046\n",
            " n04357314 - sunscreen    23.45%\n",
            " n02786058 - Band_Aid     13.34%\n",
            " n04254120 - soap_dispenser 4.84%\n",
            "\n",
            "Image #1047\n",
            " n02786058 - Band_Aid     21.31%\n",
            " n04591157 - Windsor_tie  8.46%\n",
            " n04372370 - switch       8.34%\n",
            "\n",
            "Image #1048\n",
            " n02786058 - Band_Aid     25.30%\n",
            " n04254120 - soap_dispenser 8.77%\n",
            " n03250847 - drumstick    7.34%\n",
            "\n",
            "Image #1049\n",
            " n04357314 - sunscreen    39.70%\n",
            " n02786058 - Band_Aid     12.30%\n",
            " n04372370 - switch       7.04%\n",
            "\n",
            "Image #1050\n",
            " n02786058 - Band_Aid     13.41%\n",
            " n03250847 - drumstick    12.01%\n",
            " n03970156 - plunger      9.95%\n",
            "\n",
            "Image #1051\n",
            " n03476991 - hair_spray   10.61%\n",
            " n03314780 - face_powder  8.22%\n",
            " n03814639 - neck_brace   6.23%\n",
            "\n",
            "Image #1052\n",
            " n02786058 - Band_Aid     26.62%\n",
            " n03970156 - plunger      23.97%\n",
            " n04591157 - Windsor_tie  13.08%\n",
            "\n",
            "Image #1053\n",
            " n02786058 - Band_Aid     22.74%\n",
            " n04357314 - sunscreen    21.18%\n",
            " n04591157 - Windsor_tie  8.54%\n",
            "\n",
            "Image #1054\n",
            " n04591157 - Windsor_tie  28.35%\n",
            " n04270147 - spatula      6.86%\n",
            " n02951585 - can_opener   5.60%\n",
            "\n",
            "Image #1055\n",
            " n04591157 - Windsor_tie  17.76%\n",
            " n02786058 - Band_Aid     14.05%\n",
            " n02951585 - can_opener   10.78%\n",
            "\n",
            "Image #1056\n",
            " n02786058 - Band_Aid     40.42%\n",
            " n04357314 - sunscreen    15.56%\n",
            " n04591157 - Windsor_tie  7.05%\n",
            "\n",
            "Image #1057\n",
            " n04357314 - sunscreen    20.18%\n",
            " n02786058 - Band_Aid     12.65%\n",
            " n04591157 - Windsor_tie  9.29%\n",
            "\n",
            "Image #1058\n",
            " n04357314 - sunscreen    13.10%\n",
            " n02786058 - Band_Aid     8.89%\n",
            " n04591157 - Windsor_tie  8.01%\n",
            "\n",
            "Image #1059\n",
            " n03970156 - plunger      20.55%\n",
            " n03250847 - drumstick    12.58%\n",
            " n04591157 - Windsor_tie  9.52%\n",
            "\n",
            "Image #1060\n",
            " n02786058 - Band_Aid     15.44%\n",
            " n04357314 - sunscreen    14.39%\n",
            " n04591157 - Windsor_tie  10.95%\n",
            "\n",
            "Image #1061\n",
            " n02786058 - Band_Aid     34.55%\n",
            " n04357314 - sunscreen    16.71%\n",
            " n04254120 - soap_dispenser 4.73%\n",
            "\n",
            "Image #1062\n",
            " n03250847 - drumstick    20.57%\n",
            " n02786058 - Band_Aid     17.36%\n",
            " n03970156 - plunger      14.98%\n",
            "\n",
            "Image #1063\n",
            " n03250847 - drumstick    48.02%\n",
            " n03970156 - plunger      9.84%\n",
            " n02951585 - can_opener   4.51%\n",
            "\n",
            "Image #1064\n",
            " n04357314 - sunscreen    42.90%\n",
            " n02786058 - Band_Aid     22.66%\n",
            " n04591157 - Windsor_tie  2.70%\n",
            "\n",
            "Image #1065\n",
            " n02786058 - Band_Aid     47.04%\n",
            " n04357314 - sunscreen    20.41%\n",
            " n04591157 - Windsor_tie  5.17%\n",
            "\n",
            "Image #1066\n",
            " n04591157 - Windsor_tie  19.38%\n",
            " n02786058 - Band_Aid     17.50%\n",
            " n04357314 - sunscreen    10.80%\n",
            "\n",
            "Image #1067\n",
            " n04591157 - Windsor_tie  22.93%\n",
            " n02951585 - can_opener   10.35%\n",
            " n03476991 - hair_spray   9.55%\n",
            "\n",
            "Image #1068\n",
            " n04357314 - sunscreen    26.16%\n",
            " n02786058 - Band_Aid     22.28%\n",
            " n03314780 - face_powder  3.64%\n",
            "\n",
            "Image #1069\n",
            " n02786058 - Band_Aid     29.28%\n",
            " n04357314 - sunscreen    18.66%\n",
            " n03314780 - face_powder  5.61%\n",
            "\n",
            "Image #1070\n",
            " n02786058 - Band_Aid     28.10%\n",
            " n04357314 - sunscreen    12.80%\n",
            " n03250847 - drumstick    5.22%\n",
            "\n",
            "Image #1071\n",
            " n02786058 - Band_Aid     13.65%\n",
            " n04357314 - sunscreen    11.58%\n",
            " n03250847 - drumstick    6.93%\n",
            "\n",
            "Image #1072\n",
            " n02786058 - Band_Aid     23.99%\n",
            " n03970156 - plunger      11.48%\n",
            " n04591157 - Windsor_tie  9.53%\n",
            "\n",
            "Image #1073\n",
            " n04372370 - switch       24.91%\n",
            " n04591157 - Windsor_tie  10.97%\n",
            " n02786058 - Band_Aid     9.44%\n",
            "\n",
            "Image #1074\n",
            " n02786058 - Band_Aid     34.10%\n",
            " n04357314 - sunscreen    12.50%\n",
            " n03970156 - plunger      6.50%\n",
            "\n",
            "Image #1075\n",
            " n04372370 - switch       13.36%\n",
            " n04357314 - sunscreen    13.17%\n",
            " n04125021 - safe         10.73%\n",
            "\n",
            "Image #1076\n",
            " n03970156 - plunger      24.19%\n",
            " n03250847 - drumstick    23.73%\n",
            " n04591157 - Windsor_tie  7.80%\n",
            "\n",
            "Image #1077\n",
            " n03250847 - drumstick    20.89%\n",
            " n04357314 - sunscreen    11.67%\n",
            " n03970156 - plunger      11.26%\n",
            "\n",
            "Image #1078\n",
            " n04357314 - sunscreen    24.10%\n",
            " n02786058 - Band_Aid     22.70%\n",
            " n04254120 - soap_dispenser 5.99%\n",
            "\n",
            "Image #1079\n",
            " n02786058 - Band_Aid     15.71%\n",
            " n04357314 - sunscreen    12.77%\n",
            " n04591157 - Windsor_tie  11.65%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIDbMC8803Oa"
      },
      "source": [
        "### Tal como avanzaba no es la más adecuada para la resolución de este problema y vemos que no acierta nada. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icRUz0lAEpba"
      },
      "source": [
        "# 4. CNN con Transfer Learning - cargamos Xception\n",
        "\n",
        "En este caso realizo los siguientes cambios sobre el código utilizado en clase:\n",
        "- No utilizo el preprocesado de las imagenes.\n",
        "- Cambio input  y output con los datos de este ejemplo.\n",
        "- Cambio la función de perdida: loss a categorical_crossentropy. \n",
        "- El entrenamiento lo realizo al igual que en los anteriores modelos sin utilizar el 75% de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCh5OhCJJ8c_"
      },
      "source": [
        "#SOLUCION\n",
        "#Importemos modelo base (Xception) sin las capas de clasificación (include_top=False)\n",
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
        "#Añadimos nuestras capas para hacer clasificación\n",
        "# Aplicar Global Average Pooling\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output) \n",
        "# Crear capa de salida para clasificación con n_classes mutuamente excluyentes\n",
        "output = keras.layers.Dense(6, activation=\"softmax\")(avg) \n",
        "#Definimos el modelo\n",
        "model = keras.Model(inputs=base_model.input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjsjcJInLTC0"
      },
      "source": [
        "Preparemos y entrenemos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvi3KeQOLQHM",
        "outputId": "ac666620-c306-4ff9-8333-a08008e42ed3"
      },
      "source": [
        "# Congelamos los peso del modelo base para sacar el máximo partido del modelo preentrenado\n",
        "for layer in base_model.layers: \n",
        "    layer.trainable = False\n",
        "    \n",
        "#Comencemos con una constante de aprendizaje un poco más agresiva\n",
        "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Para datasets muy grandes es conveniente definir el número de paso por epoca (steps_per_epoch)\n",
        "history = model.fit(X_train, y_train, validation_split = 0.2, epochs=20)\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "27/27 [==============================] - 3s 52ms/step - loss: 0.0763 - accuracy: 0.9923 - val_loss: 0.2662 - val_accuracy: 0.9167\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0617 - accuracy: 0.9933 - val_loss: 0.2700 - val_accuracy: 0.9120\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0730 - accuracy: 0.9940 - val_loss: 0.2656 - val_accuracy: 0.9167\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0632 - accuracy: 0.9955 - val_loss: 0.2652 - val_accuracy: 0.9120\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0676 - accuracy: 0.9934 - val_loss: 0.2642 - val_accuracy: 0.9167\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0825 - accuracy: 0.9922 - val_loss: 0.2654 - val_accuracy: 0.9120\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0619 - accuracy: 0.9961 - val_loss: 0.2667 - val_accuracy: 0.9120\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.0649 - accuracy: 0.9938 - val_loss: 0.2652 - val_accuracy: 0.9120\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0615 - accuracy: 0.9960 - val_loss: 0.2650 - val_accuracy: 0.9120\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0666 - accuracy: 0.9921 - val_loss: 0.2656 - val_accuracy: 0.9120\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0620 - accuracy: 0.9911 - val_loss: 0.2657 - val_accuracy: 0.9120\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0723 - accuracy: 0.9968 - val_loss: 0.2656 - val_accuracy: 0.9120\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0684 - accuracy: 0.9946 - val_loss: 0.2650 - val_accuracy: 0.9120\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0664 - accuracy: 0.9966 - val_loss: 0.2652 - val_accuracy: 0.9120\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.0537 - accuracy: 0.9942 - val_loss: 0.2664 - val_accuracy: 0.9120\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0716 - accuracy: 0.9945 - val_loss: 0.2653 - val_accuracy: 0.9120\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0627 - accuracy: 0.9920 - val_loss: 0.2660 - val_accuracy: 0.9120\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0660 - accuracy: 0.9935 - val_loss: 0.2649 - val_accuracy: 0.9120\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0577 - accuracy: 0.9936 - val_loss: 0.2650 - val_accuracy: 0.9120\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0584 - accuracy: 0.9918 - val_loss: 0.2647 - val_accuracy: 0.9120\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2942 - accuracy: 0.9250\n",
            "CNN básica:  [0.29416993260383606, 0.925000011920929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_qDrRYrOPa8"
      },
      "source": [
        "Lo más normal es que el entrenamiento se quede **atascado** en un nivel de precisión no satisfactorio. Si esto sucede, continuamos el entrenamiento incluyendo los pesos del modelo preentrenado y aplicando una constande de aprendizaje menor.\n",
        "\n",
        "*  Ya lo hemos aplicado con una constante de parendizaje: $0.01$\n",
        "\n",
        "Ahora lo hacemos:\n",
        "\n",
        "* Decaimiento de la constante de aprendizaje $0.001$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgyU3HT_ORQl",
        "outputId": "10df5f6b-9c77-4a6c-b09b-8331c65ec923"
      },
      "source": [
        "# Congelamos los peso del modelo base para sacar el máximo partido del modelo preentrenado\n",
        "for layer in base_model.layers: \n",
        "    layer.trainable = False\n",
        "    \n",
        "#Comencemos con una constante de aprendizaje un menos agresiva\n",
        "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Para datasets muy grandes es conveniente definir el número de paso por epoca (steps_per_epoch)\n",
        "history = model.fit(X_train, y_train, validation_split = 0.2, epochs=20)\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "27/27 [==============================] - 3s 54ms/step - loss: 0.0658 - accuracy: 0.9934 - val_loss: 0.2653 - val_accuracy: 0.9167\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0583 - accuracy: 0.9949 - val_loss: 0.2623 - val_accuracy: 0.9167\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0613 - accuracy: 0.9939 - val_loss: 0.2678 - val_accuracy: 0.9120\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0598 - accuracy: 0.9953 - val_loss: 0.2689 - val_accuracy: 0.9120\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0562 - accuracy: 0.9964 - val_loss: 0.2665 - val_accuracy: 0.9120\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0525 - accuracy: 0.9979 - val_loss: 0.2673 - val_accuracy: 0.9120\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0496 - accuracy: 0.9989 - val_loss: 0.2668 - val_accuracy: 0.9074\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0614 - accuracy: 0.9961 - val_loss: 0.2664 - val_accuracy: 0.9120\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0492 - accuracy: 0.9951 - val_loss: 0.2664 - val_accuracy: 0.9120\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.0498 - accuracy: 0.9959 - val_loss: 0.2660 - val_accuracy: 0.9120\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 1s 50ms/step - loss: 0.0611 - accuracy: 0.9927 - val_loss: 0.2668 - val_accuracy: 0.9120\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0501 - accuracy: 0.9951 - val_loss: 0.2682 - val_accuracy: 0.9120\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.0491 - accuracy: 0.9974 - val_loss: 0.2696 - val_accuracy: 0.9120\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0448 - accuracy: 0.9982 - val_loss: 0.2685 - val_accuracy: 0.9074\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0484 - accuracy: 0.9935 - val_loss: 0.2670 - val_accuracy: 0.9120\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0463 - accuracy: 0.9958 - val_loss: 0.2667 - val_accuracy: 0.9120\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.0484 - accuracy: 0.9983 - val_loss: 0.2668 - val_accuracy: 0.9074\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0522 - accuracy: 0.9951 - val_loss: 0.2677 - val_accuracy: 0.9120\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.0454 - accuracy: 0.9971 - val_loss: 0.2681 - val_accuracy: 0.9120\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 1s 26ms/step - loss: 0.0480 - accuracy: 0.9977 - val_loss: 0.2683 - val_accuracy: 0.9120\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3022 - accuracy: 0.9250\n",
            "CNN básica:  [0.3022349774837494, 0.925000011920929]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGFWm94k7lZ6"
      },
      "source": [
        "### El accuracy obtenido en los datos de test y aplicando el decamiento de la constante de aprendizaje a 0,001 es de **92,50%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9VQsvdn9VvK"
      },
      "source": [
        "### Como me indicaste que este caso habías obtenido un 96%, intento mejorarlo para ello pienso lo siguiente: \n",
        "- Teniendo en cuenta que es una transfer Learning he de centrarme en las capas  que añado de salida. La red que mejor resultado me ha dado es la RestNet (Punto 2) por lo que veo cuales han sido las últimas capas de salida, tenemos la Global Average Poling pero antes de la Dense de salida también usamos una Flatten, por lo que la incorporo en el modelo: \"flatten=keras.layers.Flatten()(avg)\".\n",
        "- Introduzco también EarlyStopping de Keras y aumento el entrenamiento a 50 epochs.\n",
        "- Utilizo la constante de aprendizaje menor de 0,001.\n",
        "- Cambio el learning rate a 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlSlYOtr3sz-",
        "outputId": "0d9f893f-de67-484b-aded-1d50e125d4c8"
      },
      "source": [
        "#SOLUCION\n",
        "#Importemos modelo base (Xception) sin las capas de clasificación (include_top=False)\n",
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
        "#Añadimos nuestras capas para hacer clasificación\n",
        "# Aplicar Global Average Pooling\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "#Convertimos las entradas en un vector para poder pasarlo a una capa Dense\n",
        "flatten=keras.layers.Flatten()(avg)\n",
        "# Crear capa de salida para clasificación con n_classes mutuamente excluyentes\n",
        "output = keras.layers.Dense(6, activation=\"softmax\")(flatten) \n",
        "#Definimos el modelo\n",
        "model = keras.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "\n",
        "# Congelamos los peso del modelo base para sacar el máximo partido del modelo preentrenado\n",
        "for layer in base_model.layers: \n",
        "    layer.trainable = False\n",
        "\n",
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    \n",
        "#Comencemos con una constante de aprendizaje menos agresiva\n",
        "optimizer = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Para datasets muy grandes es conveniente definir el número de paso por epoca (steps_per_epoch)\n",
        "history = model.fit(X_train,y_train, validation_split = 0.2, epochs=50,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 2s 0us/step\n",
            "Epoch 1/50\n",
            "27/27 [==============================] - 33s 57ms/step - loss: 1.3586 - accuracy: 0.4700 - val_loss: 0.6825 - val_accuracy: 0.7870\n",
            "Epoch 2/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.6608 - accuracy: 0.7848 - val_loss: 0.6315 - val_accuracy: 0.7639\n",
            "Epoch 3/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.4701 - accuracy: 0.8684 - val_loss: 0.5977 - val_accuracy: 0.8472\n",
            "Epoch 4/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.4107 - accuracy: 0.8773 - val_loss: 0.4033 - val_accuracy: 0.8843\n",
            "Epoch 5/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.3817 - accuracy: 0.8906 - val_loss: 0.3632 - val_accuracy: 0.9028\n",
            "Epoch 6/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.2907 - accuracy: 0.9232 - val_loss: 0.3867 - val_accuracy: 0.8889\n",
            "Epoch 7/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.2677 - accuracy: 0.9469 - val_loss: 0.3697 - val_accuracy: 0.8796\n",
            "Epoch 8/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.2463 - accuracy: 0.9664 - val_loss: 0.3288 - val_accuracy: 0.9028\n",
            "Epoch 9/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.2079 - accuracy: 0.9624 - val_loss: 0.3216 - val_accuracy: 0.9167\n",
            "Epoch 10/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.2283 - accuracy: 0.9674 - val_loss: 0.3219 - val_accuracy: 0.8935\n",
            "Epoch 11/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.2187 - accuracy: 0.9627 - val_loss: 0.3047 - val_accuracy: 0.9028\n",
            "Epoch 12/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1988 - accuracy: 0.9657 - val_loss: 0.3047 - val_accuracy: 0.8981\n",
            "Epoch 13/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.2016 - accuracy: 0.9645 - val_loss: 0.3054 - val_accuracy: 0.8981\n",
            "Epoch 14/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1718 - accuracy: 0.9765 - val_loss: 0.2985 - val_accuracy: 0.8981\n",
            "Epoch 15/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1681 - accuracy: 0.9723 - val_loss: 0.2959 - val_accuracy: 0.9074\n",
            "Epoch 16/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1676 - accuracy: 0.9827 - val_loss: 0.2898 - val_accuracy: 0.9120\n",
            "Epoch 17/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1781 - accuracy: 0.9701 - val_loss: 0.2976 - val_accuracy: 0.8981\n",
            "Epoch 18/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1495 - accuracy: 0.9841 - val_loss: 0.2855 - val_accuracy: 0.8981\n",
            "Epoch 19/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1693 - accuracy: 0.9742 - val_loss: 0.2869 - val_accuracy: 0.9028\n",
            "Epoch 20/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1669 - accuracy: 0.9729 - val_loss: 0.2854 - val_accuracy: 0.9028\n",
            "Epoch 21/50\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.1553 - accuracy: 0.9706 - val_loss: 0.2850 - val_accuracy: 0.8981\n",
            "Epoch 22/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1493 - accuracy: 0.9773 - val_loss: 0.2773 - val_accuracy: 0.9028\n",
            "Epoch 23/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1420 - accuracy: 0.9752 - val_loss: 0.2767 - val_accuracy: 0.9028\n",
            "Epoch 24/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1424 - accuracy: 0.9757 - val_loss: 0.2746 - val_accuracy: 0.8981\n",
            "Epoch 25/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1332 - accuracy: 0.9840 - val_loss: 0.2797 - val_accuracy: 0.9120\n",
            "Epoch 26/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1462 - accuracy: 0.9726 - val_loss: 0.2721 - val_accuracy: 0.9028\n",
            "Epoch 27/50\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.1602 - accuracy: 0.9727 - val_loss: 0.2738 - val_accuracy: 0.9028\n",
            "Epoch 28/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1310 - accuracy: 0.9799 - val_loss: 0.2730 - val_accuracy: 0.9028\n",
            "Epoch 29/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1443 - accuracy: 0.9778 - val_loss: 0.2722 - val_accuracy: 0.9074\n",
            "Epoch 30/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1309 - accuracy: 0.9772 - val_loss: 0.2727 - val_accuracy: 0.9074\n",
            "Epoch 31/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1287 - accuracy: 0.9792 - val_loss: 0.2674 - val_accuracy: 0.9120\n",
            "Epoch 32/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1310 - accuracy: 0.9788 - val_loss: 0.2693 - val_accuracy: 0.9074\n",
            "Epoch 33/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1335 - accuracy: 0.9780 - val_loss: 0.2686 - val_accuracy: 0.9074\n",
            "Epoch 34/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1148 - accuracy: 0.9845 - val_loss: 0.2674 - val_accuracy: 0.9028\n",
            "Epoch 35/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1293 - accuracy: 0.9763 - val_loss: 0.2662 - val_accuracy: 0.9074\n",
            "Epoch 36/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1222 - accuracy: 0.9805 - val_loss: 0.2671 - val_accuracy: 0.9120\n",
            "Epoch 37/50\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.1121 - accuracy: 0.9835 - val_loss: 0.2680 - val_accuracy: 0.9074\n",
            "Epoch 38/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1023 - accuracy: 0.9883 - val_loss: 0.2637 - val_accuracy: 0.9074\n",
            "Epoch 39/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1162 - accuracy: 0.9795 - val_loss: 0.2651 - val_accuracy: 0.9120\n",
            "Epoch 40/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1134 - accuracy: 0.9784 - val_loss: 0.2661 - val_accuracy: 0.9074\n",
            "Epoch 41/50\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.1068 - accuracy: 0.9878 - val_loss: 0.2619 - val_accuracy: 0.9028\n",
            "Epoch 42/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1000 - accuracy: 0.9839 - val_loss: 0.2626 - val_accuracy: 0.9120\n",
            "Epoch 43/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1020 - accuracy: 0.9914 - val_loss: 0.2670 - val_accuracy: 0.9074\n",
            "Epoch 44/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.0966 - accuracy: 0.9910 - val_loss: 0.2633 - val_accuracy: 0.9028\n",
            "Epoch 45/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1004 - accuracy: 0.9842 - val_loss: 0.2622 - val_accuracy: 0.9074\n",
            "Epoch 46/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1103 - accuracy: 0.9812 - val_loss: 0.2616 - val_accuracy: 0.9074\n",
            "Epoch 47/50\n",
            "27/27 [==============================] - 0s 19ms/step - loss: 0.1063 - accuracy: 0.9873 - val_loss: 0.2626 - val_accuracy: 0.9028\n",
            "Epoch 48/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1028 - accuracy: 0.9851 - val_loss: 0.2614 - val_accuracy: 0.9028\n",
            "Epoch 49/50\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.1089 - accuracy: 0.9815 - val_loss: 0.2630 - val_accuracy: 0.9074\n",
            "Epoch 50/50\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.1115 - accuracy: 0.9803 - val_loss: 0.2590 - val_accuracy: 0.9074\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.2757 - accuracy: 0.9333\n",
            "CNN básica:  [0.27569910883903503, 0.9333333373069763]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6TLGyOg_MHR"
      },
      "source": [
        "###Consigo subir un poco el accuracy a **93,33%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8yM4mjYNjoQ"
      },
      "source": [
        "#5. Realizo un último intento de una CNN propia para ver si mejoro el resultado.\n",
        "\n",
        "  - 1 capa con 64 filtros de dimensión 3 y max pool de dimensión 2.\n",
        "  - 1 capa con convolucion de 128 filtros de dimensión 3 y max pool de dimensión 2.\n",
        "  - 1 capa con convolucion de 256 filtros de dimensión 3 y max pool de dimensión 2.\n",
        "  - 1 capa LeakyRelu con alpha=0.1 y max pool de dimensión 2.\n",
        "  - Dropout a 0.5\n",
        "  - Flatten\n",
        "  - 1 capa de tipo Dense de dimensión 128\n",
        "  - Dropout a 0.5\n",
        "  - 1 capa de tipo Dense de dimensión 64\n",
        "  - 1 capa LeakyRelu con alpha=0.1\n",
        "  - Dropout a 0.5\n",
        "  - 1 capa de tipo de Dense de dimensión 6\n",
        "  - La función de activación es 'relu' a excepción de la salida 'softmax'(también he probado con kernel_initializer='lecun_normal',activation='selu' y da el mismo resultado), también he hecho pruebas cambiando la función de activación.\n",
        "  - Aplico también EarlyStopping de Keras y entreno todo lo que puedo.\n",
        "  - He probado con todos los tipos de optimizers y el que mejor funciona es: **optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_oWkFLuRH6s",
        "outputId": "ef8c6bf0-8d2e-4eca-cee3-038b9a4615cd"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "epochs=1000\n",
        "\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3, activation='relu', padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3, activation='relu', padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, activation='relu'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64,activation='relu'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adamax(\n",
        "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n",
        "mano_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 903,878\n",
            "Trainable params: 903,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 1.8307 - accuracy: 0.1602 - val_loss: 1.7939 - val_accuracy: 0.1713\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7976 - accuracy: 0.1849 - val_loss: 1.7930 - val_accuracy: 0.2083\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7898 - accuracy: 0.2062 - val_loss: 1.7929 - val_accuracy: 0.1296\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7861 - accuracy: 0.2411 - val_loss: 1.7942 - val_accuracy: 0.1296\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7937 - accuracy: 0.1456 - val_loss: 1.7907 - val_accuracy: 0.1296\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7876 - accuracy: 0.2007 - val_loss: 1.7834 - val_accuracy: 0.1296\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7835 - accuracy: 0.1930 - val_loss: 1.7662 - val_accuracy: 0.1620\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7618 - accuracy: 0.2044 - val_loss: 1.7328 - val_accuracy: 0.2037\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7269 - accuracy: 0.2346 - val_loss: 1.6541 - val_accuracy: 0.3056\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6974 - accuracy: 0.2503 - val_loss: 1.6023 - val_accuracy: 0.2963\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6411 - accuracy: 0.2934 - val_loss: 1.5570 - val_accuracy: 0.4491\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5923 - accuracy: 0.3507 - val_loss: 1.4250 - val_accuracy: 0.4907\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5608 - accuracy: 0.3362 - val_loss: 1.4677 - val_accuracy: 0.4583\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5090 - accuracy: 0.3839 - val_loss: 1.3128 - val_accuracy: 0.5926\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.3827 - accuracy: 0.4344 - val_loss: 1.2465 - val_accuracy: 0.6343\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.3241 - accuracy: 0.4528 - val_loss: 1.1935 - val_accuracy: 0.6157\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.3702 - accuracy: 0.4505 - val_loss: 1.1299 - val_accuracy: 0.6806\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.2899 - accuracy: 0.5057 - val_loss: 1.0102 - val_accuracy: 0.6806\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1709 - accuracy: 0.5296 - val_loss: 1.0159 - val_accuracy: 0.6759\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1195 - accuracy: 0.5673 - val_loss: 0.9069 - val_accuracy: 0.7361\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0660 - accuracy: 0.5779 - val_loss: 0.8544 - val_accuracy: 0.7593\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0434 - accuracy: 0.6112 - val_loss: 0.7760 - val_accuracy: 0.7407\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9714 - accuracy: 0.5985 - val_loss: 0.8116 - val_accuracy: 0.7778\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0295 - accuracy: 0.5985 - val_loss: 0.7810 - val_accuracy: 0.7824\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9304 - accuracy: 0.6380 - val_loss: 0.7032 - val_accuracy: 0.8009\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9287 - accuracy: 0.6322 - val_loss: 0.6922 - val_accuracy: 0.8194\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.8279 - accuracy: 0.6694 - val_loss: 0.6576 - val_accuracy: 0.8102\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.8574 - accuracy: 0.6441 - val_loss: 0.6726 - val_accuracy: 0.8426\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.8176 - accuracy: 0.6850 - val_loss: 0.5628 - val_accuracy: 0.8472\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.7551 - accuracy: 0.6694 - val_loss: 0.5592 - val_accuracy: 0.8565\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.7576 - accuracy: 0.7043 - val_loss: 0.6098 - val_accuracy: 0.8056\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.7344 - accuracy: 0.7012 - val_loss: 0.5528 - val_accuracy: 0.8657\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6793 - accuracy: 0.7287 - val_loss: 0.5009 - val_accuracy: 0.8611\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6665 - accuracy: 0.7597 - val_loss: 0.4988 - val_accuracy: 0.8472\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6692 - accuracy: 0.7395 - val_loss: 0.4510 - val_accuracy: 0.8704\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5988 - accuracy: 0.7758 - val_loss: 0.4242 - val_accuracy: 0.8796\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.6469 - accuracy: 0.7413 - val_loss: 0.4108 - val_accuracy: 0.8796\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5889 - accuracy: 0.7858 - val_loss: 0.4121 - val_accuracy: 0.9028\n",
            "Epoch 39/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5580 - accuracy: 0.7795 - val_loss: 0.3851 - val_accuracy: 0.8981\n",
            "Epoch 40/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5868 - accuracy: 0.7923 - val_loss: 0.3684 - val_accuracy: 0.9028\n",
            "Epoch 41/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5575 - accuracy: 0.7858 - val_loss: 0.3930 - val_accuracy: 0.8889\n",
            "Epoch 42/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.5025 - accuracy: 0.7948 - val_loss: 0.3691 - val_accuracy: 0.8981\n",
            "Epoch 43/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5122 - accuracy: 0.7977 - val_loss: 0.3428 - val_accuracy: 0.9120\n",
            "Epoch 44/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5038 - accuracy: 0.8122 - val_loss: 0.3172 - val_accuracy: 0.9167\n",
            "Epoch 45/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4870 - accuracy: 0.8186 - val_loss: 0.3583 - val_accuracy: 0.8935\n",
            "Epoch 46/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5422 - accuracy: 0.7997 - val_loss: 0.3256 - val_accuracy: 0.9074\n",
            "Epoch 47/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4459 - accuracy: 0.8252 - val_loss: 0.2957 - val_accuracy: 0.9213\n",
            "Epoch 48/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4321 - accuracy: 0.8303 - val_loss: 0.3085 - val_accuracy: 0.9120\n",
            "Epoch 49/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3986 - accuracy: 0.8380 - val_loss: 0.2748 - val_accuracy: 0.9306\n",
            "Epoch 50/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3739 - accuracy: 0.8743 - val_loss: 0.2945 - val_accuracy: 0.9306\n",
            "Epoch 51/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3710 - accuracy: 0.8749 - val_loss: 0.2934 - val_accuracy: 0.9259\n",
            "Epoch 52/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3945 - accuracy: 0.8547 - val_loss: 0.2719 - val_accuracy: 0.9259\n",
            "Epoch 53/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3926 - accuracy: 0.8761 - val_loss: 0.2477 - val_accuracy: 0.9444\n",
            "Epoch 54/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3056 - accuracy: 0.9067 - val_loss: 0.2461 - val_accuracy: 0.9352\n",
            "Epoch 55/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3720 - accuracy: 0.8787 - val_loss: 0.2516 - val_accuracy: 0.9120\n",
            "Epoch 56/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3392 - accuracy: 0.8669 - val_loss: 0.2428 - val_accuracy: 0.9259\n",
            "Epoch 57/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3597 - accuracy: 0.8714 - val_loss: 0.2505 - val_accuracy: 0.9306\n",
            "Epoch 58/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4046 - accuracy: 0.8626 - val_loss: 0.2797 - val_accuracy: 0.9120\n",
            "Epoch 59/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2993 - accuracy: 0.8844 - val_loss: 0.2606 - val_accuracy: 0.9259\n",
            "Epoch 60/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2908 - accuracy: 0.8829 - val_loss: 0.1980 - val_accuracy: 0.9398\n",
            "Epoch 61/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2961 - accuracy: 0.8802 - val_loss: 0.2580 - val_accuracy: 0.9398\n",
            "Epoch 62/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2631 - accuracy: 0.9251 - val_loss: 0.2023 - val_accuracy: 0.9444\n",
            "Epoch 63/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2943 - accuracy: 0.8934 - val_loss: 0.2114 - val_accuracy: 0.9444\n",
            "Epoch 64/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2481 - accuracy: 0.9125 - val_loss: 0.1973 - val_accuracy: 0.9491\n",
            "Epoch 65/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2458 - accuracy: 0.9091 - val_loss: 0.1790 - val_accuracy: 0.9491\n",
            "Epoch 66/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2531 - accuracy: 0.9216 - val_loss: 0.1647 - val_accuracy: 0.9398\n",
            "Epoch 67/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3118 - accuracy: 0.8980 - val_loss: 0.1796 - val_accuracy: 0.9537\n",
            "Epoch 68/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2435 - accuracy: 0.9214 - val_loss: 0.1790 - val_accuracy: 0.9398\n",
            "Epoch 69/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2626 - accuracy: 0.9109 - val_loss: 0.1589 - val_accuracy: 0.9583\n",
            "Epoch 70/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2094 - accuracy: 0.9410 - val_loss: 0.1743 - val_accuracy: 0.9583\n",
            "Epoch 71/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2343 - accuracy: 0.9296 - val_loss: 0.1567 - val_accuracy: 0.9537\n",
            "Epoch 72/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2481 - accuracy: 0.9079 - val_loss: 0.1470 - val_accuracy: 0.9583\n",
            "Epoch 73/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2382 - accuracy: 0.9278 - val_loss: 0.1686 - val_accuracy: 0.9583\n",
            "Epoch 74/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2085 - accuracy: 0.9375 - val_loss: 0.1671 - val_accuracy: 0.9537\n",
            "Epoch 75/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2246 - accuracy: 0.9203 - val_loss: 0.1325 - val_accuracy: 0.9630\n",
            "Epoch 76/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2364 - accuracy: 0.9202 - val_loss: 0.1592 - val_accuracy: 0.9491\n",
            "Epoch 77/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2096 - accuracy: 0.9273 - val_loss: 0.1243 - val_accuracy: 0.9630\n",
            "Epoch 78/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2537 - accuracy: 0.9103 - val_loss: 0.1490 - val_accuracy: 0.9491\n",
            "Epoch 79/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1980 - accuracy: 0.9186 - val_loss: 0.1409 - val_accuracy: 0.9583\n",
            "Epoch 80/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1825 - accuracy: 0.9382 - val_loss: 0.1497 - val_accuracy: 0.9583\n",
            "Epoch 81/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1895 - accuracy: 0.9279 - val_loss: 0.1638 - val_accuracy: 0.9491\n",
            "Epoch 82/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1790 - accuracy: 0.9402 - val_loss: 0.1322 - val_accuracy: 0.9583\n",
            "Epoch 83/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2055 - accuracy: 0.9370 - val_loss: 0.1468 - val_accuracy: 0.9630\n",
            "Epoch 84/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1714 - accuracy: 0.9508 - val_loss: 0.0953 - val_accuracy: 0.9722\n",
            "Epoch 85/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1525 - accuracy: 0.9409 - val_loss: 0.1219 - val_accuracy: 0.9630\n",
            "Epoch 86/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1553 - accuracy: 0.9562 - val_loss: 0.1610 - val_accuracy: 0.9537\n",
            "Epoch 87/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1534 - accuracy: 0.9500 - val_loss: 0.1275 - val_accuracy: 0.9630\n",
            "Epoch 88/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1559 - accuracy: 0.9433 - val_loss: 0.1314 - val_accuracy: 0.9583\n",
            "Epoch 89/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1564 - accuracy: 0.9508 - val_loss: 0.1121 - val_accuracy: 0.9630\n",
            "Epoch 90/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1664 - accuracy: 0.9316 - val_loss: 0.1209 - val_accuracy: 0.9722\n",
            "Epoch 91/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1597 - accuracy: 0.9544 - val_loss: 0.1489 - val_accuracy: 0.9676\n",
            "Epoch 92/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1643 - accuracy: 0.9396 - val_loss: 0.1560 - val_accuracy: 0.9630\n",
            "Epoch 93/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1749 - accuracy: 0.9379 - val_loss: 0.1052 - val_accuracy: 0.9676\n",
            "Epoch 94/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1060 - accuracy: 0.9569 - val_loss: 0.1490 - val_accuracy: 0.9630\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9667\n",
            "CNN básica:  [0.12956592440605164, 0.9666666388511658]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5x_EDIZXbLM"
      },
      "source": [
        "## Finalmente obtengo con una CNN propia un accuracy de 96,67%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy2Cq7O4Y7md"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nknWDNrYs2C"
      },
      "source": [
        "###Otros intentos para realizar la última CNN propia presentada en el punto 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkH_m8JiSO3N",
        "outputId": "b7cfd580-bc2c-49d3-ec59-9f80d951fa60"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "epochs=1000\n",
        "\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='lecun_normal',activation='selu',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3, kernel_initializer='lecun_normal',activation='selu', padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3,kernel_initializer='lecun_normal',activation='selu', padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, kernel_initializer='lecun_normal',activation='selu'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='selu'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adamax(\n",
        "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\")\n",
        "mano_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 903,878\n",
            "Trainable params: 903,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 16ms/step - loss: 2.7901 - accuracy: 0.1623 - val_loss: 1.7983 - val_accuracy: 0.1806\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.9473 - accuracy: 0.1767 - val_loss: 1.8067 - val_accuracy: 0.1296\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.9338 - accuracy: 0.1882 - val_loss: 1.7874 - val_accuracy: 0.2361\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.8565 - accuracy: 0.1464 - val_loss: 1.7854 - val_accuracy: 0.1852\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.8041 - accuracy: 0.2074 - val_loss: 1.7842 - val_accuracy: 0.2546\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7984 - accuracy: 0.2263 - val_loss: 1.7522 - val_accuracy: 0.3704\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.8003 - accuracy: 0.2283 - val_loss: 1.7244 - val_accuracy: 0.3565\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.7122 - accuracy: 0.2450 - val_loss: 1.5194 - val_accuracy: 0.4537\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.5634 - accuracy: 0.3498 - val_loss: 1.3890 - val_accuracy: 0.4769\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.4942 - accuracy: 0.3951 - val_loss: 1.2615 - val_accuracy: 0.5417\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.3727 - accuracy: 0.4851 - val_loss: 1.2355 - val_accuracy: 0.5648\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.3227 - accuracy: 0.4819 - val_loss: 1.0534 - val_accuracy: 0.6713\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0820 - accuracy: 0.5986 - val_loss: 0.9533 - val_accuracy: 0.6852\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0932 - accuracy: 0.6003 - val_loss: 0.8108 - val_accuracy: 0.7593\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9794 - accuracy: 0.6347 - val_loss: 0.7764 - val_accuracy: 0.7731\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9276 - accuracy: 0.6698 - val_loss: 0.7425 - val_accuracy: 0.7870\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.6878 - val_loss: 0.7720 - val_accuracy: 0.7824\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.9015 - accuracy: 0.6582 - val_loss: 0.6545 - val_accuracy: 0.8148\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.8197 - accuracy: 0.7136 - val_loss: 0.5736 - val_accuracy: 0.8472\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.7567 - accuracy: 0.7417 - val_loss: 0.5927 - val_accuracy: 0.8148\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.7485 - accuracy: 0.7393 - val_loss: 0.5101 - val_accuracy: 0.8333\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.6707 - accuracy: 0.7668 - val_loss: 0.4381 - val_accuracy: 0.8750\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.5945 - accuracy: 0.7789 - val_loss: 0.4787 - val_accuracy: 0.8380\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.5896 - accuracy: 0.8030 - val_loss: 0.4430 - val_accuracy: 0.8472\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.5494 - accuracy: 0.8072 - val_loss: 0.3670 - val_accuracy: 0.8935\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.5320 - accuracy: 0.8145 - val_loss: 0.3582 - val_accuracy: 0.8889\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.4732 - accuracy: 0.8291 - val_loss: 0.4155 - val_accuracy: 0.8796\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.4511 - accuracy: 0.8321 - val_loss: 0.3032 - val_accuracy: 0.9028\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.4113 - accuracy: 0.8661 - val_loss: 0.3234 - val_accuracy: 0.8889\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3629 - accuracy: 0.8715 - val_loss: 0.3058 - val_accuracy: 0.9074\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.4420 - accuracy: 0.8392 - val_loss: 0.3485 - val_accuracy: 0.8611\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3795 - accuracy: 0.8630 - val_loss: 0.3083 - val_accuracy: 0.8981\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3158 - accuracy: 0.8915 - val_loss: 0.2652 - val_accuracy: 0.9120\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3392 - accuracy: 0.8733 - val_loss: 0.2843 - val_accuracy: 0.9120\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3817 - accuracy: 0.8837 - val_loss: 0.2798 - val_accuracy: 0.9213\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2966 - accuracy: 0.8977 - val_loss: 0.2574 - val_accuracy: 0.9167\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2821 - accuracy: 0.8980 - val_loss: 0.2361 - val_accuracy: 0.9306\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2887 - accuracy: 0.9011 - val_loss: 0.1963 - val_accuracy: 0.9352\n",
            "Epoch 39/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2069 - accuracy: 0.9324 - val_loss: 0.2039 - val_accuracy: 0.9398\n",
            "Epoch 40/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2188 - accuracy: 0.9219 - val_loss: 0.2496 - val_accuracy: 0.9167\n",
            "Epoch 41/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2527 - accuracy: 0.9277 - val_loss: 0.2294 - val_accuracy: 0.9213\n",
            "Epoch 42/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1758 - accuracy: 0.9489 - val_loss: 0.1983 - val_accuracy: 0.9352\n",
            "Epoch 43/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2227 - accuracy: 0.9198 - val_loss: 0.2587 - val_accuracy: 0.9120\n",
            "Epoch 44/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.2033 - accuracy: 0.9264 - val_loss: 0.1938 - val_accuracy: 0.9398\n",
            "Epoch 45/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1594 - accuracy: 0.9412 - val_loss: 0.2120 - val_accuracy: 0.9259\n",
            "Epoch 46/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1858 - accuracy: 0.9397 - val_loss: 0.2045 - val_accuracy: 0.9306\n",
            "Epoch 47/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1607 - accuracy: 0.9547 - val_loss: 0.1743 - val_accuracy: 0.9444\n",
            "Epoch 48/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1606 - accuracy: 0.9482 - val_loss: 0.1707 - val_accuracy: 0.9491\n",
            "Epoch 49/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1244 - accuracy: 0.9615 - val_loss: 0.1717 - val_accuracy: 0.9444\n",
            "Epoch 50/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1687 - accuracy: 0.9417 - val_loss: 0.1875 - val_accuracy: 0.9352\n",
            "Epoch 51/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1830 - accuracy: 0.9380 - val_loss: 0.1921 - val_accuracy: 0.9444\n",
            "Epoch 52/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1641 - accuracy: 0.9519 - val_loss: 0.1594 - val_accuracy: 0.9444\n",
            "Epoch 53/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1438 - accuracy: 0.9544 - val_loss: 0.1732 - val_accuracy: 0.9398\n",
            "Epoch 54/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.1435 - accuracy: 0.9589 - val_loss: 0.1727 - val_accuracy: 0.9444\n",
            "Epoch 55/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1193 - accuracy: 0.9696 - val_loss: 0.1233 - val_accuracy: 0.9630\n",
            "Epoch 56/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0921 - accuracy: 0.9747 - val_loss: 0.1136 - val_accuracy: 0.9630\n",
            "Epoch 57/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1122 - accuracy: 0.9654 - val_loss: 0.1192 - val_accuracy: 0.9583\n",
            "Epoch 58/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0993 - accuracy: 0.9635 - val_loss: 0.1212 - val_accuracy: 0.9491\n",
            "Epoch 59/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1326 - accuracy: 0.9589 - val_loss: 0.1590 - val_accuracy: 0.9398\n",
            "Epoch 60/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0956 - accuracy: 0.9691 - val_loss: 0.1387 - val_accuracy: 0.9583\n",
            "Epoch 61/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0841 - accuracy: 0.9786 - val_loss: 0.1355 - val_accuracy: 0.9491\n",
            "Epoch 62/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1195 - accuracy: 0.9569 - val_loss: 0.1172 - val_accuracy: 0.9630\n",
            "Epoch 63/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0993 - accuracy: 0.9640 - val_loss: 0.1246 - val_accuracy: 0.9630\n",
            "Epoch 64/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0705 - accuracy: 0.9741 - val_loss: 0.1494 - val_accuracy: 0.9537\n",
            "Epoch 65/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0667 - accuracy: 0.9795 - val_loss: 0.1106 - val_accuracy: 0.9722\n",
            "Epoch 66/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0620 - accuracy: 0.9815 - val_loss: 0.1798 - val_accuracy: 0.9444\n",
            "Epoch 67/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0706 - accuracy: 0.9761 - val_loss: 0.1668 - val_accuracy: 0.9537\n",
            "Epoch 68/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0635 - accuracy: 0.9810 - val_loss: 0.1473 - val_accuracy: 0.9444\n",
            "Epoch 69/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0807 - accuracy: 0.9792 - val_loss: 0.1372 - val_accuracy: 0.9537\n",
            "Epoch 70/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0805 - accuracy: 0.9647 - val_loss: 0.1143 - val_accuracy: 0.9722\n",
            "Epoch 71/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0775 - accuracy: 0.9778 - val_loss: 0.1889 - val_accuracy: 0.9491\n",
            "Epoch 72/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0633 - accuracy: 0.9850 - val_loss: 0.1662 - val_accuracy: 0.9491\n",
            "Epoch 73/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0732 - accuracy: 0.9781 - val_loss: 0.1755 - val_accuracy: 0.9537\n",
            "Epoch 74/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0906 - accuracy: 0.9674 - val_loss: 0.1341 - val_accuracy: 0.9630\n",
            "Epoch 75/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.0636 - accuracy: 0.9806 - val_loss: 0.1292 - val_accuracy: 0.9722\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9667\n",
            "CNN básica:  [0.08877992630004883, 0.9666666388511658]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnBAvfdmFQfV",
        "outputId": "785b0256-a173-460a-b201-1b5237f0f170"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "epochs=1000\n",
        "\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='lecun_normal',activation='selu',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3, kernel_initializer='lecun_normal',activation='selu', padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3,kernel_initializer='lecun_normal',activation='selu', padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, kernel_initializer='lecun_normal',activation='selu'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='selu'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "\n",
        "#Comencemos con una constante de aprendizaje menos agresiva\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\")\n",
        "mano_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Para datasets muy grandes es conveniente definir el número de paso por epoca (steps_per_epoch)\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_112 (Conv2D)          (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_124 (MaxPoolin (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_113 (Conv2D)          (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_125 (MaxPoolin (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_114 (Conv2D)          (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_126 (MaxPoolin (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_84 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_127 (MaxPoolin (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_98 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_42 (Flatten)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "dropout_99 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_85 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_100 (Dropout)        (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 903,878\n",
            "Trainable params: 903,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 2.9549 - accuracy: 0.1763 - val_loss: 1.8147 - val_accuracy: 0.1852\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.9304 - accuracy: 0.2094 - val_loss: 1.8046 - val_accuracy: 0.1667\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.8363 - accuracy: 0.2188 - val_loss: 1.6506 - val_accuracy: 0.2963\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.6199 - accuracy: 0.3632 - val_loss: 1.2371 - val_accuracy: 0.5556\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.4142 - accuracy: 0.4460 - val_loss: 1.1463 - val_accuracy: 0.6065\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.2012 - accuracy: 0.5336 - val_loss: 1.0218 - val_accuracy: 0.6759\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0251 - accuracy: 0.6068 - val_loss: 0.8084 - val_accuracy: 0.7037\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 1.0318 - accuracy: 0.6277 - val_loss: 0.6211 - val_accuracy: 0.8241\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.7726 - accuracy: 0.6966 - val_loss: 0.4790 - val_accuracy: 0.8889\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.6696 - accuracy: 0.7647 - val_loss: 0.4748 - val_accuracy: 0.8611\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6089 - accuracy: 0.7786 - val_loss: 0.4254 - val_accuracy: 0.8657\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6178 - accuracy: 0.7730 - val_loss: 0.5001 - val_accuracy: 0.8380\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5813 - accuracy: 0.7798 - val_loss: 0.3188 - val_accuracy: 0.8889\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4467 - accuracy: 0.8264 - val_loss: 0.2941 - val_accuracy: 0.9074\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4330 - accuracy: 0.8491 - val_loss: 0.2849 - val_accuracy: 0.9028\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3270 - accuracy: 0.8880 - val_loss: 0.2950 - val_accuracy: 0.8981\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3156 - accuracy: 0.8834 - val_loss: 0.3514 - val_accuracy: 0.8935\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3307 - accuracy: 0.8971 - val_loss: 0.2765 - val_accuracy: 0.9167\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2246 - accuracy: 0.9240 - val_loss: 0.4035 - val_accuracy: 0.8889\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2784 - accuracy: 0.9035 - val_loss: 0.2216 - val_accuracy: 0.9352\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2166 - accuracy: 0.9313 - val_loss: 0.2860 - val_accuracy: 0.9213\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2546 - accuracy: 0.8935 - val_loss: 0.2235 - val_accuracy: 0.9352\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.1970 - accuracy: 0.9411 - val_loss: 0.3673 - val_accuracy: 0.8843\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2082 - accuracy: 0.9347 - val_loss: 0.3470 - val_accuracy: 0.9074\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1959 - accuracy: 0.9240 - val_loss: 0.2867 - val_accuracy: 0.9167\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1477 - accuracy: 0.9512 - val_loss: 0.2524 - val_accuracy: 0.9074\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2529 - accuracy: 0.9116 - val_loss: 0.3016 - val_accuracy: 0.9213\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1708 - accuracy: 0.9393 - val_loss: 0.1643 - val_accuracy: 0.9491\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1475 - accuracy: 0.9471 - val_loss: 0.2953 - val_accuracy: 0.9167\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.2508 - accuracy: 0.9308 - val_loss: 0.2161 - val_accuracy: 0.9120\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1459 - accuracy: 0.9611 - val_loss: 0.2644 - val_accuracy: 0.9306\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1500 - accuracy: 0.9526 - val_loss: 0.2398 - val_accuracy: 0.9398\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.0859 - accuracy: 0.9691 - val_loss: 0.2732 - val_accuracy: 0.9398\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.0741 - accuracy: 0.9678 - val_loss: 0.2233 - val_accuracy: 0.9259\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1246 - accuracy: 0.9539 - val_loss: 0.2892 - val_accuracy: 0.9259\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1069 - accuracy: 0.9611 - val_loss: 0.2304 - val_accuracy: 0.9213\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1720 - accuracy: 0.9537 - val_loss: 0.2601 - val_accuracy: 0.9352\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.1092 - accuracy: 0.9654 - val_loss: 0.2672 - val_accuracy: 0.9167\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9500\n",
            "CNN básica:  [0.14879046380519867, 0.949999988079071]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJd8g7JIKS1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a498360-a71e-48f9-cd96-0c828af6fee7"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "epochs=1000\n",
        "\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3, activation='relu', padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3, activation='relu', padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, activation='relu'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64,activation='relu'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "\n",
        "#Comencemos con una constante de aprendizaje menos agresiva\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\")\n",
        "mano_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Para datasets muy grandes es conveniente definir el número de paso por epoca (steps_per_epoch)\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 903,878\n",
            "Trainable params: 903,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 17ms/step - loss: 1.8341 - accuracy: 0.1714 - val_loss: 1.7922 - val_accuracy: 0.1296\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7935 - accuracy: 0.1547 - val_loss: 1.7927 - val_accuracy: 0.1296\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7921 - accuracy: 0.1746 - val_loss: 1.7920 - val_accuracy: 0.1296\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7904 - accuracy: 0.1585 - val_loss: 1.7758 - val_accuracy: 0.1574\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7714 - accuracy: 0.1851 - val_loss: 1.6575 - val_accuracy: 0.2407\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.6268 - accuracy: 0.3053 - val_loss: 1.5361 - val_accuracy: 0.2778\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.5398 - accuracy: 0.3549 - val_loss: 1.5188 - val_accuracy: 0.3426\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.4480 - accuracy: 0.3656 - val_loss: 1.4021 - val_accuracy: 0.3704\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.3665 - accuracy: 0.4209 - val_loss: 1.2788 - val_accuracy: 0.5370\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.2620 - accuracy: 0.4804 - val_loss: 1.2699 - val_accuracy: 0.5509\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.2087 - accuracy: 0.5008 - val_loss: 1.0786 - val_accuracy: 0.6296\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.0865 - accuracy: 0.5549 - val_loss: 0.9935 - val_accuracy: 0.6759\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0736 - accuracy: 0.6065 - val_loss: 0.9584 - val_accuracy: 0.6759\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9593 - accuracy: 0.6193 - val_loss: 0.9299 - val_accuracy: 0.6898\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9823 - accuracy: 0.6472 - val_loss: 0.7830 - val_accuracy: 0.7639\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.7988 - accuracy: 0.7185 - val_loss: 0.7224 - val_accuracy: 0.7870\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.8194 - accuracy: 0.6939 - val_loss: 0.6291 - val_accuracy: 0.8241\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6940 - accuracy: 0.7565 - val_loss: 0.5397 - val_accuracy: 0.8241\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6571 - accuracy: 0.7547 - val_loss: 0.4888 - val_accuracy: 0.8796\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5961 - accuracy: 0.7624 - val_loss: 0.4108 - val_accuracy: 0.8843\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5672 - accuracy: 0.7843 - val_loss: 0.3654 - val_accuracy: 0.8843\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4513 - accuracy: 0.8487 - val_loss: 0.3474 - val_accuracy: 0.9074\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4409 - accuracy: 0.8228 - val_loss: 0.3255 - val_accuracy: 0.8889\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4199 - accuracy: 0.8499 - val_loss: 0.3214 - val_accuracy: 0.8935\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3316 - accuracy: 0.8846 - val_loss: 0.3247 - val_accuracy: 0.8981\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4107 - accuracy: 0.8607 - val_loss: 0.3436 - val_accuracy: 0.8935\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2842 - accuracy: 0.9015 - val_loss: 0.2792 - val_accuracy: 0.9120\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2819 - accuracy: 0.9044 - val_loss: 0.2466 - val_accuracy: 0.9213\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2601 - accuracy: 0.9007 - val_loss: 0.2566 - val_accuracy: 0.9074\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3293 - accuracy: 0.8969 - val_loss: 0.3277 - val_accuracy: 0.9213\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2209 - accuracy: 0.9316 - val_loss: 0.3191 - val_accuracy: 0.9120\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2225 - accuracy: 0.9060 - val_loss: 0.2950 - val_accuracy: 0.9213\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2332 - accuracy: 0.9202 - val_loss: 0.2987 - val_accuracy: 0.8889\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2448 - accuracy: 0.9287 - val_loss: 0.2450 - val_accuracy: 0.9167\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1998 - accuracy: 0.9402 - val_loss: 0.2023 - val_accuracy: 0.9120\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1844 - accuracy: 0.9292 - val_loss: 0.2308 - val_accuracy: 0.9213\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1701 - accuracy: 0.9441 - val_loss: 0.2191 - val_accuracy: 0.9259\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1863 - accuracy: 0.9458 - val_loss: 0.2371 - val_accuracy: 0.9213\n",
            "Epoch 39/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1423 - accuracy: 0.9507 - val_loss: 0.2345 - val_accuracy: 0.9213\n",
            "Epoch 40/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1747 - accuracy: 0.9445 - val_loss: 0.2165 - val_accuracy: 0.9306\n",
            "Epoch 41/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1696 - accuracy: 0.9377 - val_loss: 0.2588 - val_accuracy: 0.9259\n",
            "Epoch 42/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1610 - accuracy: 0.9521 - val_loss: 0.2076 - val_accuracy: 0.9398\n",
            "Epoch 43/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1077 - accuracy: 0.9627 - val_loss: 0.2442 - val_accuracy: 0.9306\n",
            "Epoch 44/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1145 - accuracy: 0.9626 - val_loss: 0.2387 - val_accuracy: 0.9259\n",
            "Epoch 45/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1320 - accuracy: 0.9569 - val_loss: 0.2054 - val_accuracy: 0.9444\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1468 - accuracy: 0.9417\n",
            "CNN básica:  [0.14682422578334808, 0.9416666626930237]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18El2tgWPeyx",
        "outputId": "36415214-51e4-49d9-9b80-e3a4ca07dced"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "epochs=1000\n",
        "\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(3, 3),activation='relu',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3, activation='relu', padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3, activation='relu', padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, activation='relu'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64,activation='relu'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "\n",
        "#Comencemos con una constante de aprendizaje menos agresiva\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=False,\n",
        "    name=\"Adam\")\n",
        "mano_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "# Para datasets muy grandes es conveniente definir el número de paso por epoca (steps_per_epoch)\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               524416    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 903,878\n",
            "Trainable params: 903,878\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 16ms/step - loss: 1.8256 - accuracy: 0.1560 - val_loss: 1.7980 - val_accuracy: 0.1296\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7924 - accuracy: 0.1652 - val_loss: 1.7942 - val_accuracy: 0.1296\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7942 - accuracy: 0.1472 - val_loss: 1.7917 - val_accuracy: 0.1296\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7908 - accuracy: 0.1621 - val_loss: 1.7888 - val_accuracy: 0.1296\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.7830 - accuracy: 0.1930 - val_loss: 1.7382 - val_accuracy: 0.3148\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.7149 - accuracy: 0.2657 - val_loss: 1.5684 - val_accuracy: 0.3796\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 1.5814 - accuracy: 0.3408 - val_loss: 1.3437 - val_accuracy: 0.4769\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.3830 - accuracy: 0.4485 - val_loss: 1.1647 - val_accuracy: 0.5139\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.2447 - accuracy: 0.5226 - val_loss: 1.1370 - val_accuracy: 0.6065\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.1586 - accuracy: 0.5419 - val_loss: 1.0394 - val_accuracy: 0.5972\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 1.0552 - accuracy: 0.5628 - val_loss: 0.8234 - val_accuracy: 0.6991\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.9252 - accuracy: 0.6404 - val_loss: 0.7813 - val_accuracy: 0.7315\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.8942 - accuracy: 0.6377 - val_loss: 0.6233 - val_accuracy: 0.7963\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.7598 - accuracy: 0.7172 - val_loss: 0.5836 - val_accuracy: 0.7778\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.7342 - accuracy: 0.7265 - val_loss: 0.4796 - val_accuracy: 0.8287\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6578 - accuracy: 0.7616 - val_loss: 0.5488 - val_accuracy: 0.8056\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6379 - accuracy: 0.7633 - val_loss: 0.4741 - val_accuracy: 0.8333\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5348 - accuracy: 0.8092 - val_loss: 0.4130 - val_accuracy: 0.8843\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5392 - accuracy: 0.7839 - val_loss: 0.3732 - val_accuracy: 0.8704\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5171 - accuracy: 0.8129 - val_loss: 0.3814 - val_accuracy: 0.8380\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4346 - accuracy: 0.8491 - val_loss: 0.3297 - val_accuracy: 0.8935\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4421 - accuracy: 0.8560 - val_loss: 0.3390 - val_accuracy: 0.9074\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4034 - accuracy: 0.8475 - val_loss: 0.3424 - val_accuracy: 0.8750\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2719 - accuracy: 0.8902 - val_loss: 0.2663 - val_accuracy: 0.8935\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3166 - accuracy: 0.8766 - val_loss: 0.3417 - val_accuracy: 0.8981\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3813 - accuracy: 0.8512 - val_loss: 0.2756 - val_accuracy: 0.9213\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3148 - accuracy: 0.8952 - val_loss: 0.3333 - val_accuracy: 0.8889\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3344 - accuracy: 0.8801 - val_loss: 0.2735 - val_accuracy: 0.8981\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3014 - accuracy: 0.9053 - val_loss: 0.2745 - val_accuracy: 0.9167\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3098 - accuracy: 0.9076 - val_loss: 0.2359 - val_accuracy: 0.9259\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2591 - accuracy: 0.9265 - val_loss: 0.2164 - val_accuracy: 0.9259\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1863 - accuracy: 0.9414 - val_loss: 0.3837 - val_accuracy: 0.8981\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9231 - val_loss: 0.2738 - val_accuracy: 0.9259\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2653 - accuracy: 0.9119 - val_loss: 0.2753 - val_accuracy: 0.9120\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1971 - accuracy: 0.9227 - val_loss: 0.2168 - val_accuracy: 0.9306\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2036 - accuracy: 0.9351 - val_loss: 0.2391 - val_accuracy: 0.9213\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9248 - val_loss: 0.3248 - val_accuracy: 0.9167\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2765 - accuracy: 0.9158 - val_loss: 0.3499 - val_accuracy: 0.8981\n",
            "Epoch 39/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2458 - accuracy: 0.9206 - val_loss: 0.2061 - val_accuracy: 0.9306\n",
            "Epoch 40/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1965 - accuracy: 0.9328 - val_loss: 0.2460 - val_accuracy: 0.9259\n",
            "Epoch 41/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2208 - accuracy: 0.9239 - val_loss: 0.2314 - val_accuracy: 0.9259\n",
            "Epoch 42/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.2159 - accuracy: 0.9332 - val_loss: 0.2100 - val_accuracy: 0.9213\n",
            "Epoch 43/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1426 - accuracy: 0.9449 - val_loss: 0.1809 - val_accuracy: 0.9537\n",
            "Epoch 44/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1610 - accuracy: 0.9360 - val_loss: 0.2296 - val_accuracy: 0.9352\n",
            "Epoch 45/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1793 - accuracy: 0.9464 - val_loss: 0.2479 - val_accuracy: 0.9259\n",
            "Epoch 46/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1264 - accuracy: 0.9615 - val_loss: 0.1880 - val_accuracy: 0.9444\n",
            "Epoch 47/1000\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.1563 - accuracy: 0.9486 - val_loss: 0.2543 - val_accuracy: 0.9306\n",
            "Epoch 48/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1216 - accuracy: 0.9557 - val_loss: 0.2581 - val_accuracy: 0.9306\n",
            "Epoch 49/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.0907 - accuracy: 0.9707 - val_loss: 0.2910 - val_accuracy: 0.9306\n",
            "Epoch 50/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1559 - accuracy: 0.9481 - val_loss: 0.2251 - val_accuracy: 0.9259\n",
            "Epoch 51/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1683 - accuracy: 0.9479 - val_loss: 0.2756 - val_accuracy: 0.9074\n",
            "Epoch 52/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1677 - accuracy: 0.9456 - val_loss: 0.2584 - val_accuracy: 0.9398\n",
            "Epoch 53/1000\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.1313 - accuracy: 0.9595 - val_loss: 0.2624 - val_accuracy: 0.9398\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1945 - accuracy: 0.9417\n",
            "CNN básica:  [0.1944514960050583, 0.9416666626930237]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCz2g-sMuQk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f739fb-71ee-414a-bc3e-e83eb29530d2"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "\n",
        "INIT_LR = 1e-3\n",
        "epochs = 1000\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3,activation=\"linear\", padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3,activation=\"linear\", padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "##mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "##mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, activation='linear'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64, activation='linear'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "mano_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
        "\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_67 (Conv2D)           (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_71 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_69 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_54 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 128)               2097280   \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_55 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 2,476,742\n",
            "Trainable params: 2,476,742\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 1.8229 - accuracy: 0.1657 - val_loss: 1.7917 - val_accuracy: 0.2407\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.8025 - accuracy: 0.1630 - val_loss: 1.7900 - val_accuracy: 0.1296\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.8017 - accuracy: 0.1960 - val_loss: 1.7872 - val_accuracy: 0.2500\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7965 - accuracy: 0.1671 - val_loss: 1.7821 - val_accuracy: 0.1944\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7954 - accuracy: 0.1888 - val_loss: 1.7834 - val_accuracy: 0.2130\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7861 - accuracy: 0.1951 - val_loss: 1.7809 - val_accuracy: 0.2639\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7783 - accuracy: 0.2528 - val_loss: 1.7768 - val_accuracy: 0.2546\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7775 - accuracy: 0.1974 - val_loss: 1.7748 - val_accuracy: 0.2315\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7813 - accuracy: 0.1991 - val_loss: 1.7783 - val_accuracy: 0.1806\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7750 - accuracy: 0.2185 - val_loss: 1.7796 - val_accuracy: 0.2083\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7698 - accuracy: 0.2069 - val_loss: 1.7713 - val_accuracy: 0.2963\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7950 - accuracy: 0.1814 - val_loss: 1.7709 - val_accuracy: 0.2037\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7754 - accuracy: 0.2016 - val_loss: 1.7761 - val_accuracy: 0.2176\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7755 - accuracy: 0.2306 - val_loss: 1.7548 - val_accuracy: 0.3750\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7643 - accuracy: 0.2479 - val_loss: 1.7462 - val_accuracy: 0.3704\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7567 - accuracy: 0.2511 - val_loss: 1.7386 - val_accuracy: 0.3472\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7426 - accuracy: 0.2584 - val_loss: 1.7270 - val_accuracy: 0.3333\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7348 - accuracy: 0.2913 - val_loss: 1.7214 - val_accuracy: 0.3426\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7374 - accuracy: 0.2912 - val_loss: 1.7266 - val_accuracy: 0.3241\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.7476 - accuracy: 0.2450 - val_loss: 1.7112 - val_accuracy: 0.4213\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7176 - accuracy: 0.2561 - val_loss: 1.6980 - val_accuracy: 0.4583\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7161 - accuracy: 0.2988 - val_loss: 1.6955 - val_accuracy: 0.2963\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7023 - accuracy: 0.2976 - val_loss: 1.6844 - val_accuracy: 0.4074\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.7085 - accuracy: 0.3017 - val_loss: 1.6713 - val_accuracy: 0.3611\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.6772 - accuracy: 0.2955 - val_loss: 1.6627 - val_accuracy: 0.3750\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6606 - accuracy: 0.3532 - val_loss: 1.6503 - val_accuracy: 0.4306\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.6735 - accuracy: 0.3159 - val_loss: 1.6415 - val_accuracy: 0.4120\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6402 - accuracy: 0.3340 - val_loss: 1.6242 - val_accuracy: 0.4259\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.6299 - accuracy: 0.3384 - val_loss: 1.5968 - val_accuracy: 0.4861\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.6298 - accuracy: 0.3614 - val_loss: 1.5979 - val_accuracy: 0.4167\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5860 - accuracy: 0.3839 - val_loss: 1.5775 - val_accuracy: 0.5093\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5877 - accuracy: 0.3656 - val_loss: 1.5471 - val_accuracy: 0.4491\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5872 - accuracy: 0.3728 - val_loss: 1.5337 - val_accuracy: 0.4815\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5710 - accuracy: 0.3692 - val_loss: 1.5492 - val_accuracy: 0.3657\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5454 - accuracy: 0.3860 - val_loss: 1.5184 - val_accuracy: 0.4722\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5544 - accuracy: 0.3666 - val_loss: 1.4696 - val_accuracy: 0.4722\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.5032 - accuracy: 0.4254 - val_loss: 1.4504 - val_accuracy: 0.5278\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4923 - accuracy: 0.4193 - val_loss: 1.4176 - val_accuracy: 0.5463\n",
            "Epoch 39/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4671 - accuracy: 0.4244 - val_loss: 1.4381 - val_accuracy: 0.4815\n",
            "Epoch 40/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4433 - accuracy: 0.4185 - val_loss: 1.3894 - val_accuracy: 0.5694\n",
            "Epoch 41/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4468 - accuracy: 0.4126 - val_loss: 1.3708 - val_accuracy: 0.5324\n",
            "Epoch 42/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.4065 - accuracy: 0.4732 - val_loss: 1.3880 - val_accuracy: 0.4861\n",
            "Epoch 43/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.4080 - accuracy: 0.4794 - val_loss: 1.3495 - val_accuracy: 0.5556\n",
            "Epoch 44/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.3921 - accuracy: 0.4773 - val_loss: 1.2986 - val_accuracy: 0.5417\n",
            "Epoch 45/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.3566 - accuracy: 0.4667 - val_loss: 1.3174 - val_accuracy: 0.5139\n",
            "Epoch 46/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.3659 - accuracy: 0.4632 - val_loss: 1.3231 - val_accuracy: 0.4907\n",
            "Epoch 47/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.3670 - accuracy: 0.4654 - val_loss: 1.3094 - val_accuracy: 0.5046\n",
            "Epoch 48/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.3324 - accuracy: 0.4915 - val_loss: 1.2410 - val_accuracy: 0.6019\n",
            "Epoch 49/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.2626 - accuracy: 0.5449 - val_loss: 1.2078 - val_accuracy: 0.6111\n",
            "Epoch 50/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.3013 - accuracy: 0.5260 - val_loss: 1.1891 - val_accuracy: 0.6343\n",
            "Epoch 51/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.2891 - accuracy: 0.5447 - val_loss: 1.1821 - val_accuracy: 0.6250\n",
            "Epoch 52/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.2663 - accuracy: 0.5377 - val_loss: 1.1640 - val_accuracy: 0.5880\n",
            "Epoch 53/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.1802 - accuracy: 0.5656 - val_loss: 1.1542 - val_accuracy: 0.5602\n",
            "Epoch 54/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.1976 - accuracy: 0.5528 - val_loss: 1.1854 - val_accuracy: 0.5509\n",
            "Epoch 55/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.2026 - accuracy: 0.5352 - val_loss: 1.1191 - val_accuracy: 0.6991\n",
            "Epoch 56/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.1247 - accuracy: 0.5926 - val_loss: 1.1133 - val_accuracy: 0.6574\n",
            "Epoch 57/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.1298 - accuracy: 0.5814 - val_loss: 1.0647 - val_accuracy: 0.6389\n",
            "Epoch 58/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.1110 - accuracy: 0.5923 - val_loss: 1.0781 - val_accuracy: 0.6713\n",
            "Epoch 59/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.1223 - accuracy: 0.6105 - val_loss: 1.0660 - val_accuracy: 0.6667\n",
            "Epoch 60/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.1444 - accuracy: 0.5949 - val_loss: 1.0326 - val_accuracy: 0.6574\n",
            "Epoch 61/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.1974 - accuracy: 0.5625 - val_loss: 1.0334 - val_accuracy: 0.6759\n",
            "Epoch 62/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0835 - accuracy: 0.5990 - val_loss: 0.9926 - val_accuracy: 0.6806\n",
            "Epoch 63/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0676 - accuracy: 0.5970 - val_loss: 0.9963 - val_accuracy: 0.7130\n",
            "Epoch 64/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0635 - accuracy: 0.5846 - val_loss: 0.9926 - val_accuracy: 0.7037\n",
            "Epoch 65/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.1001 - accuracy: 0.5776 - val_loss: 0.9847 - val_accuracy: 0.6852\n",
            "Epoch 66/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0596 - accuracy: 0.5850 - val_loss: 0.9526 - val_accuracy: 0.6944\n",
            "Epoch 67/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0833 - accuracy: 0.5845 - val_loss: 0.9413 - val_accuracy: 0.7222\n",
            "Epoch 68/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.1091 - accuracy: 0.6307 - val_loss: 0.9478 - val_accuracy: 0.7037\n",
            "Epoch 69/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0110 - accuracy: 0.6230 - val_loss: 0.9530 - val_accuracy: 0.7222\n",
            "Epoch 70/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0316 - accuracy: 0.6070 - val_loss: 0.9314 - val_accuracy: 0.6806\n",
            "Epoch 71/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9962 - accuracy: 0.6508 - val_loss: 0.9020 - val_accuracy: 0.7130\n",
            "Epoch 72/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 1.0134 - accuracy: 0.6387 - val_loss: 0.9368 - val_accuracy: 0.6759\n",
            "Epoch 73/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 1.0058 - accuracy: 0.6452 - val_loss: 0.8921 - val_accuracy: 0.7130\n",
            "Epoch 74/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9466 - accuracy: 0.6836 - val_loss: 0.8566 - val_accuracy: 0.7361\n",
            "Epoch 75/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9934 - accuracy: 0.6351 - val_loss: 0.8807 - val_accuracy: 0.6898\n",
            "Epoch 76/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9507 - accuracy: 0.6346 - val_loss: 0.8859 - val_accuracy: 0.7130\n",
            "Epoch 77/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9372 - accuracy: 0.6695 - val_loss: 0.8584 - val_accuracy: 0.7546\n",
            "Epoch 78/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9636 - accuracy: 0.6486 - val_loss: 0.9279 - val_accuracy: 0.6852\n",
            "Epoch 79/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9576 - accuracy: 0.6473 - val_loss: 0.8322 - val_accuracy: 0.7130\n",
            "Epoch 80/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8865 - accuracy: 0.6713 - val_loss: 0.8112 - val_accuracy: 0.7500\n",
            "Epoch 81/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9393 - accuracy: 0.6513 - val_loss: 0.8009 - val_accuracy: 0.7546\n",
            "Epoch 82/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.9071 - accuracy: 0.6658 - val_loss: 0.8284 - val_accuracy: 0.7546\n",
            "Epoch 83/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.9455 - accuracy: 0.6477 - val_loss: 0.7910 - val_accuracy: 0.7639\n",
            "Epoch 84/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8729 - accuracy: 0.6892 - val_loss: 0.7798 - val_accuracy: 0.7870\n",
            "Epoch 85/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.8840 - accuracy: 0.6824 - val_loss: 0.7806 - val_accuracy: 0.7917\n",
            "Epoch 86/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8872 - accuracy: 0.6617 - val_loss: 0.7923 - val_accuracy: 0.7870\n",
            "Epoch 87/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.8402 - accuracy: 0.7064 - val_loss: 0.7487 - val_accuracy: 0.7778\n",
            "Epoch 88/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.8241 - accuracy: 0.7109 - val_loss: 0.7605 - val_accuracy: 0.7639\n",
            "Epoch 89/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8536 - accuracy: 0.6847 - val_loss: 0.7335 - val_accuracy: 0.7824\n",
            "Epoch 90/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8274 - accuracy: 0.7127 - val_loss: 0.7611 - val_accuracy: 0.7546\n",
            "Epoch 91/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8151 - accuracy: 0.7084 - val_loss: 0.7369 - val_accuracy: 0.7824\n",
            "Epoch 92/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8574 - accuracy: 0.6827 - val_loss: 0.7280 - val_accuracy: 0.7407\n",
            "Epoch 93/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8226 - accuracy: 0.7140 - val_loss: 0.7136 - val_accuracy: 0.8056\n",
            "Epoch 94/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8443 - accuracy: 0.6899 - val_loss: 0.7389 - val_accuracy: 0.7454\n",
            "Epoch 95/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7851 - accuracy: 0.7289 - val_loss: 0.7116 - val_accuracy: 0.7917\n",
            "Epoch 96/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7827 - accuracy: 0.7225 - val_loss: 0.7372 - val_accuracy: 0.7639\n",
            "Epoch 97/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7442 - accuracy: 0.7495 - val_loss: 0.6979 - val_accuracy: 0.7685\n",
            "Epoch 98/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7826 - accuracy: 0.7327 - val_loss: 0.6770 - val_accuracy: 0.8102\n",
            "Epoch 99/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8176 - accuracy: 0.6863 - val_loss: 0.7146 - val_accuracy: 0.7870\n",
            "Epoch 100/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.7627 - accuracy: 0.7423 - val_loss: 0.6790 - val_accuracy: 0.7593\n",
            "Epoch 101/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7906 - accuracy: 0.7276 - val_loss: 0.6754 - val_accuracy: 0.7870\n",
            "Epoch 102/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.8044 - accuracy: 0.7165 - val_loss: 0.6563 - val_accuracy: 0.8056\n",
            "Epoch 103/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7586 - accuracy: 0.7341 - val_loss: 0.6514 - val_accuracy: 0.8194\n",
            "Epoch 104/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7442 - accuracy: 0.7112 - val_loss: 0.6455 - val_accuracy: 0.8102\n",
            "Epoch 105/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7698 - accuracy: 0.7081 - val_loss: 0.6440 - val_accuracy: 0.7824\n",
            "Epoch 106/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7965 - accuracy: 0.7085 - val_loss: 0.6356 - val_accuracy: 0.8102\n",
            "Epoch 107/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.7272 - accuracy: 0.7298 - val_loss: 0.6403 - val_accuracy: 0.8194\n",
            "Epoch 108/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7144 - accuracy: 0.7444 - val_loss: 0.6271 - val_accuracy: 0.7870\n",
            "Epoch 109/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.7466 - accuracy: 0.7150 - val_loss: 0.6370 - val_accuracy: 0.7917\n",
            "Epoch 110/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6628 - accuracy: 0.7708 - val_loss: 0.6224 - val_accuracy: 0.8148\n",
            "Epoch 111/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7179 - accuracy: 0.7528 - val_loss: 0.6365 - val_accuracy: 0.7963\n",
            "Epoch 112/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6684 - accuracy: 0.7682 - val_loss: 0.6199 - val_accuracy: 0.8148\n",
            "Epoch 113/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7021 - accuracy: 0.7538 - val_loss: 0.5912 - val_accuracy: 0.8241\n",
            "Epoch 114/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6841 - accuracy: 0.7838 - val_loss: 0.5899 - val_accuracy: 0.8102\n",
            "Epoch 115/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7141 - accuracy: 0.7448 - val_loss: 0.5935 - val_accuracy: 0.8194\n",
            "Epoch 116/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6919 - accuracy: 0.7488 - val_loss: 0.5889 - val_accuracy: 0.8148\n",
            "Epoch 117/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6509 - accuracy: 0.7901 - val_loss: 0.6117 - val_accuracy: 0.8056\n",
            "Epoch 118/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6475 - accuracy: 0.8052 - val_loss: 0.6002 - val_accuracy: 0.8194\n",
            "Epoch 119/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6677 - accuracy: 0.7813 - val_loss: 0.5796 - val_accuracy: 0.8148\n",
            "Epoch 120/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6500 - accuracy: 0.7705 - val_loss: 0.6058 - val_accuracy: 0.8102\n",
            "Epoch 121/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6363 - accuracy: 0.7783 - val_loss: 0.5625 - val_accuracy: 0.8148\n",
            "Epoch 122/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6187 - accuracy: 0.8004 - val_loss: 0.5786 - val_accuracy: 0.8102\n",
            "Epoch 123/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6196 - accuracy: 0.7824 - val_loss: 0.5469 - val_accuracy: 0.8287\n",
            "Epoch 124/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6790 - accuracy: 0.7561 - val_loss: 0.5651 - val_accuracy: 0.7870\n",
            "Epoch 125/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.6501 - accuracy: 0.7826 - val_loss: 0.5754 - val_accuracy: 0.8102\n",
            "Epoch 126/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6564 - accuracy: 0.7567 - val_loss: 0.5665 - val_accuracy: 0.8241\n",
            "Epoch 127/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6235 - accuracy: 0.7730 - val_loss: 0.5448 - val_accuracy: 0.8519\n",
            "Epoch 128/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5768 - accuracy: 0.7841 - val_loss: 0.5278 - val_accuracy: 0.8333\n",
            "Epoch 129/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6253 - accuracy: 0.7823 - val_loss: 0.5414 - val_accuracy: 0.8611\n",
            "Epoch 130/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6199 - accuracy: 0.7938 - val_loss: 0.5379 - val_accuracy: 0.8148\n",
            "Epoch 131/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5644 - accuracy: 0.8300 - val_loss: 0.5429 - val_accuracy: 0.8426\n",
            "Epoch 132/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5569 - accuracy: 0.8011 - val_loss: 0.5182 - val_accuracy: 0.8380\n",
            "Epoch 133/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6143 - accuracy: 0.7859 - val_loss: 0.5653 - val_accuracy: 0.8426\n",
            "Epoch 134/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5994 - accuracy: 0.7899 - val_loss: 0.5432 - val_accuracy: 0.8426\n",
            "Epoch 135/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6102 - accuracy: 0.7795 - val_loss: 0.5107 - val_accuracy: 0.8380\n",
            "Epoch 136/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5900 - accuracy: 0.7922 - val_loss: 0.5110 - val_accuracy: 0.8148\n",
            "Epoch 137/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6068 - accuracy: 0.7991 - val_loss: 0.5157 - val_accuracy: 0.8241\n",
            "Epoch 138/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5855 - accuracy: 0.8032 - val_loss: 0.5098 - val_accuracy: 0.8426\n",
            "Epoch 139/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6149 - accuracy: 0.7821 - val_loss: 0.5096 - val_accuracy: 0.8241\n",
            "Epoch 140/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5453 - accuracy: 0.8198 - val_loss: 0.5164 - val_accuracy: 0.8333\n",
            "Epoch 141/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5777 - accuracy: 0.7992 - val_loss: 0.5082 - val_accuracy: 0.8287\n",
            "Epoch 142/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5482 - accuracy: 0.8239 - val_loss: 0.5249 - val_accuracy: 0.8241\n",
            "Epoch 143/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.5649 - accuracy: 0.8221 - val_loss: 0.5752 - val_accuracy: 0.8333\n",
            "Epoch 144/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.5596 - accuracy: 0.8037 - val_loss: 0.5128 - val_accuracy: 0.8472\n",
            "Epoch 145/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5574 - accuracy: 0.8086 - val_loss: 0.5394 - val_accuracy: 0.8380\n",
            "Epoch 146/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5687 - accuracy: 0.8178 - val_loss: 0.4711 - val_accuracy: 0.8380\n",
            "Epoch 147/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5497 - accuracy: 0.8111 - val_loss: 0.5000 - val_accuracy: 0.8333\n",
            "Epoch 148/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5106 - accuracy: 0.8372 - val_loss: 0.4672 - val_accuracy: 0.8380\n",
            "Epoch 149/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5199 - accuracy: 0.8052 - val_loss: 0.4686 - val_accuracy: 0.8519\n",
            "Epoch 150/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5557 - accuracy: 0.8154 - val_loss: 0.4819 - val_accuracy: 0.8333\n",
            "Epoch 151/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4845 - accuracy: 0.8328 - val_loss: 0.4752 - val_accuracy: 0.8380\n",
            "Epoch 152/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5346 - accuracy: 0.8051 - val_loss: 0.4898 - val_accuracy: 0.8565\n",
            "Epoch 153/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5222 - accuracy: 0.8180 - val_loss: 0.4907 - val_accuracy: 0.8565\n",
            "Epoch 154/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5054 - accuracy: 0.8308 - val_loss: 0.4641 - val_accuracy: 0.8519\n",
            "Epoch 155/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5040 - accuracy: 0.8472 - val_loss: 0.4690 - val_accuracy: 0.8472\n",
            "Epoch 156/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4447 - accuracy: 0.8565 - val_loss: 0.4632 - val_accuracy: 0.8380\n",
            "Epoch 157/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5662 - accuracy: 0.8038 - val_loss: 0.5206 - val_accuracy: 0.8056\n",
            "Epoch 158/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4806 - accuracy: 0.8385 - val_loss: 0.4616 - val_accuracy: 0.8194\n",
            "Epoch 159/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4814 - accuracy: 0.8252 - val_loss: 0.5023 - val_accuracy: 0.8426\n",
            "Epoch 160/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4987 - accuracy: 0.8335 - val_loss: 0.4485 - val_accuracy: 0.8380\n",
            "Epoch 161/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4418 - accuracy: 0.8565 - val_loss: 0.4534 - val_accuracy: 0.8333\n",
            "Epoch 162/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4617 - accuracy: 0.8397 - val_loss: 0.4493 - val_accuracy: 0.8519\n",
            "Epoch 163/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5109 - accuracy: 0.8208 - val_loss: 0.4464 - val_accuracy: 0.8287\n",
            "Epoch 164/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5278 - accuracy: 0.7957 - val_loss: 0.4600 - val_accuracy: 0.8472\n",
            "Epoch 165/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4561 - accuracy: 0.8578 - val_loss: 0.4243 - val_accuracy: 0.8611\n",
            "Epoch 166/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4506 - accuracy: 0.8318 - val_loss: 0.4317 - val_accuracy: 0.8519\n",
            "Epoch 167/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4630 - accuracy: 0.8338 - val_loss: 0.4282 - val_accuracy: 0.8472\n",
            "Epoch 168/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4376 - accuracy: 0.8427 - val_loss: 0.4356 - val_accuracy: 0.8472\n",
            "Epoch 169/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4593 - accuracy: 0.8477 - val_loss: 0.4439 - val_accuracy: 0.8472\n",
            "Epoch 170/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4688 - accuracy: 0.8442 - val_loss: 0.4266 - val_accuracy: 0.8565\n",
            "Epoch 171/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4895 - accuracy: 0.8287 - val_loss: 0.4267 - val_accuracy: 0.8611\n",
            "Epoch 172/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5085 - accuracy: 0.8234 - val_loss: 0.4204 - val_accuracy: 0.8426\n",
            "Epoch 173/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4638 - accuracy: 0.8437 - val_loss: 0.4302 - val_accuracy: 0.8380\n",
            "Epoch 174/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5022 - accuracy: 0.8211 - val_loss: 0.4484 - val_accuracy: 0.8426\n",
            "Epoch 175/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4414 - accuracy: 0.8394 - val_loss: 0.4678 - val_accuracy: 0.8426\n",
            "Epoch 176/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4953 - accuracy: 0.8292 - val_loss: 0.4304 - val_accuracy: 0.8472\n",
            "Epoch 177/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4331 - accuracy: 0.8560 - val_loss: 0.4215 - val_accuracy: 0.8565\n",
            "Epoch 178/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4766 - accuracy: 0.8247 - val_loss: 0.3980 - val_accuracy: 0.8565\n",
            "Epoch 179/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4230 - accuracy: 0.8544 - val_loss: 0.3998 - val_accuracy: 0.8611\n",
            "Epoch 180/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4435 - accuracy: 0.8603 - val_loss: 0.4475 - val_accuracy: 0.8657\n",
            "Epoch 181/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4045 - accuracy: 0.8773 - val_loss: 0.4168 - val_accuracy: 0.8565\n",
            "Epoch 182/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4133 - accuracy: 0.8696 - val_loss: 0.4088 - val_accuracy: 0.8657\n",
            "Epoch 183/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4357 - accuracy: 0.8533 - val_loss: 0.4255 - val_accuracy: 0.8704\n",
            "Epoch 184/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4098 - accuracy: 0.8591 - val_loss: 0.4418 - val_accuracy: 0.8426\n",
            "Epoch 185/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4135 - accuracy: 0.8605 - val_loss: 0.4250 - val_accuracy: 0.8472\n",
            "Epoch 186/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4708 - accuracy: 0.8365 - val_loss: 0.4172 - val_accuracy: 0.8657\n",
            "Epoch 187/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3995 - accuracy: 0.8773 - val_loss: 0.4183 - val_accuracy: 0.8750\n",
            "Epoch 188/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4098 - accuracy: 0.8577 - val_loss: 0.3824 - val_accuracy: 0.8472\n",
            "Epoch 189/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4171 - accuracy: 0.8699 - val_loss: 0.4394 - val_accuracy: 0.8472\n",
            "Epoch 190/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4706 - accuracy: 0.8536 - val_loss: 0.4069 - val_accuracy: 0.8565\n",
            "Epoch 191/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3882 - accuracy: 0.8830 - val_loss: 0.4159 - val_accuracy: 0.8565\n",
            "Epoch 192/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4352 - accuracy: 0.8576 - val_loss: 0.4001 - val_accuracy: 0.8657\n",
            "Epoch 193/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4081 - accuracy: 0.8561 - val_loss: 0.4095 - val_accuracy: 0.8704\n",
            "Epoch 194/1000\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4118 - accuracy: 0.8634 - val_loss: 0.4180 - val_accuracy: 0.8657\n",
            "Epoch 195/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3958 - accuracy: 0.8738 - val_loss: 0.4207 - val_accuracy: 0.8519\n",
            "Epoch 196/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3958 - accuracy: 0.8567 - val_loss: 0.3885 - val_accuracy: 0.8519\n",
            "Epoch 197/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3935 - accuracy: 0.8841 - val_loss: 0.4027 - val_accuracy: 0.8657\n",
            "Epoch 198/1000\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4127 - accuracy: 0.8631 - val_loss: 0.3934 - val_accuracy: 0.8565\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3415 - accuracy: 0.9000\n",
            "CNN básica:  [0.3414919972419739, 0.8999999761581421]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZAXmaL898_J",
        "outputId": "4572b5ff-58f0-4d58-e276-94c89047b0ac"
      },
      "source": [
        "#Importo EarlyStopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "\n",
        "INIT_LR = 1e-3\n",
        "epochs = 1000\n",
        " \n",
        "mano_model = keras.models.Sequential()\n",
        "mano_model.add(keras.layers.Conv2D(64, kernel_size=(5, 5),activation='elu',padding='same',input_shape=(64,64,3)))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(128,3,activation=\"elu\", padding=\"same\")) \n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.Conv2D(256,3,activation=\"elu\", padding=\"same\"))\n",
        "mano_model.add(keras.layers.MaxPool2D(pool_size=2))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "#mano_model.add(keras.layers.MaxPooling2D((2, 2),padding='same'))\n",
        "#mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Flatten())\n",
        "mano_model.add(keras.layers.Dense(units=128, activation='elu'))\n",
        "mano_model.add(keras.layers.Dropout(0.5))\n",
        "mano_model.add(keras.layers.Dense(64, activation='elu'))\n",
        "mano_model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "mano_model.add(keras.layers.Dropout(0.5)) \n",
        "mano_model.add(keras.layers.Dense(6, activation='softmax'))\n",
        " \n",
        "mano_model.summary()\n",
        " \n",
        "mano_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
        "\n",
        "history = mano_model.fit(X_train,y_train, validation_split = 0.2, epochs=epochs,callbacks=[callback],verbose=1)\n",
        "print('CNN básica: ', mano_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_55 (Conv2D)           (None, 64, 64, 64)        4864      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_59 (MaxPooling (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_60 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_61 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_46 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 128)               2097280   \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_47 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 2,479,814\n",
            "Trainable params: 2,479,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 1.8472 - accuracy: 0.1586 - val_loss: 1.7918 - val_accuracy: 0.1806\n",
            "Epoch 2/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7985 - accuracy: 0.1727 - val_loss: 1.7900 - val_accuracy: 0.1713\n",
            "Epoch 3/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7999 - accuracy: 0.1735 - val_loss: 1.7889 - val_accuracy: 0.1713\n",
            "Epoch 4/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7891 - accuracy: 0.1837 - val_loss: 1.7881 - val_accuracy: 0.1389\n",
            "Epoch 5/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7913 - accuracy: 0.1628 - val_loss: 1.7933 - val_accuracy: 0.1296\n",
            "Epoch 6/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7960 - accuracy: 0.1804 - val_loss: 1.7898 - val_accuracy: 0.3102\n",
            "Epoch 7/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7827 - accuracy: 0.2007 - val_loss: 1.7870 - val_accuracy: 0.1898\n",
            "Epoch 8/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7782 - accuracy: 0.1921 - val_loss: 1.7810 - val_accuracy: 0.2824\n",
            "Epoch 9/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7794 - accuracy: 0.2026 - val_loss: 1.7865 - val_accuracy: 0.1389\n",
            "Epoch 10/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7770 - accuracy: 0.2204 - val_loss: 1.7791 - val_accuracy: 0.1296\n",
            "Epoch 11/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7922 - accuracy: 0.1793 - val_loss: 1.7817 - val_accuracy: 0.2500\n",
            "Epoch 12/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7801 - accuracy: 0.2129 - val_loss: 1.7858 - val_accuracy: 0.2037\n",
            "Epoch 13/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7776 - accuracy: 0.2250 - val_loss: 1.7766 - val_accuracy: 0.2130\n",
            "Epoch 14/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7629 - accuracy: 0.2462 - val_loss: 1.7755 - val_accuracy: 0.2639\n",
            "Epoch 15/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7633 - accuracy: 0.2279 - val_loss: 1.7692 - val_accuracy: 0.3241\n",
            "Epoch 16/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7565 - accuracy: 0.2324 - val_loss: 1.7612 - val_accuracy: 0.3333\n",
            "Epoch 17/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7752 - accuracy: 0.2075 - val_loss: 1.7559 - val_accuracy: 0.3148\n",
            "Epoch 18/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7701 - accuracy: 0.1972 - val_loss: 1.7619 - val_accuracy: 0.3611\n",
            "Epoch 19/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7589 - accuracy: 0.2296 - val_loss: 1.7510 - val_accuracy: 0.3472\n",
            "Epoch 20/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7382 - accuracy: 0.2766 - val_loss: 1.7423 - val_accuracy: 0.3426\n",
            "Epoch 21/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7487 - accuracy: 0.2527 - val_loss: 1.7464 - val_accuracy: 0.3102\n",
            "Epoch 22/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7414 - accuracy: 0.2494 - val_loss: 1.7379 - val_accuracy: 0.3426\n",
            "Epoch 23/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7244 - accuracy: 0.2964 - val_loss: 1.7343 - val_accuracy: 0.3426\n",
            "Epoch 24/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7140 - accuracy: 0.2776 - val_loss: 1.7163 - val_accuracy: 0.3194\n",
            "Epoch 25/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.7213 - accuracy: 0.2596 - val_loss: 1.7048 - val_accuracy: 0.3657\n",
            "Epoch 26/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7112 - accuracy: 0.2320 - val_loss: 1.7094 - val_accuracy: 0.3750\n",
            "Epoch 27/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.7016 - accuracy: 0.2982 - val_loss: 1.6945 - val_accuracy: 0.3194\n",
            "Epoch 28/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6871 - accuracy: 0.3217 - val_loss: 1.6807 - val_accuracy: 0.3935\n",
            "Epoch 29/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6781 - accuracy: 0.3395 - val_loss: 1.6767 - val_accuracy: 0.4259\n",
            "Epoch 30/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6738 - accuracy: 0.3034 - val_loss: 1.6729 - val_accuracy: 0.3519\n",
            "Epoch 31/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6808 - accuracy: 0.3267 - val_loss: 1.6542 - val_accuracy: 0.3565\n",
            "Epoch 32/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6623 - accuracy: 0.3469 - val_loss: 1.6518 - val_accuracy: 0.4120\n",
            "Epoch 33/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6342 - accuracy: 0.3430 - val_loss: 1.6392 - val_accuracy: 0.3981\n",
            "Epoch 34/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6218 - accuracy: 0.3616 - val_loss: 1.6268 - val_accuracy: 0.4398\n",
            "Epoch 35/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.6207 - accuracy: 0.3691 - val_loss: 1.6131 - val_accuracy: 0.4213\n",
            "Epoch 36/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.5991 - accuracy: 0.3755 - val_loss: 1.5782 - val_accuracy: 0.4861\n",
            "Epoch 37/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.5848 - accuracy: 0.3857 - val_loss: 1.5882 - val_accuracy: 0.4120\n",
            "Epoch 38/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.5831 - accuracy: 0.3877 - val_loss: 1.5555 - val_accuracy: 0.4815\n",
            "Epoch 39/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.5485 - accuracy: 0.4285 - val_loss: 1.5289 - val_accuracy: 0.5694\n",
            "Epoch 40/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.5343 - accuracy: 0.4243 - val_loss: 1.5199 - val_accuracy: 0.4815\n",
            "Epoch 41/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.5218 - accuracy: 0.4243 - val_loss: 1.5160 - val_accuracy: 0.4491\n",
            "Epoch 42/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.4821 - accuracy: 0.4574 - val_loss: 1.4649 - val_accuracy: 0.5463\n",
            "Epoch 43/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.5011 - accuracy: 0.4278 - val_loss: 1.4638 - val_accuracy: 0.4861\n",
            "Epoch 44/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.4812 - accuracy: 0.4280 - val_loss: 1.4317 - val_accuracy: 0.5648\n",
            "Epoch 45/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.4663 - accuracy: 0.4544 - val_loss: 1.3976 - val_accuracy: 0.5370\n",
            "Epoch 46/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.4368 - accuracy: 0.4462 - val_loss: 1.3785 - val_accuracy: 0.6111\n",
            "Epoch 47/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.4262 - accuracy: 0.4750 - val_loss: 1.3928 - val_accuracy: 0.4954\n",
            "Epoch 48/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.4135 - accuracy: 0.4802 - val_loss: 1.3513 - val_accuracy: 0.5694\n",
            "Epoch 49/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.3953 - accuracy: 0.4682 - val_loss: 1.3351 - val_accuracy: 0.5926\n",
            "Epoch 50/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.3472 - accuracy: 0.5106 - val_loss: 1.3020 - val_accuracy: 0.6389\n",
            "Epoch 51/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.3468 - accuracy: 0.4979 - val_loss: 1.2976 - val_accuracy: 0.6620\n",
            "Epoch 52/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.3083 - accuracy: 0.5074 - val_loss: 1.2605 - val_accuracy: 0.6481\n",
            "Epoch 53/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.3082 - accuracy: 0.4835 - val_loss: 1.2484 - val_accuracy: 0.6250\n",
            "Epoch 54/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.2448 - accuracy: 0.5379 - val_loss: 1.1943 - val_accuracy: 0.6852\n",
            "Epoch 55/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.2622 - accuracy: 0.5502 - val_loss: 1.2026 - val_accuracy: 0.5787\n",
            "Epoch 56/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.2420 - accuracy: 0.5406 - val_loss: 1.1585 - val_accuracy: 0.6389\n",
            "Epoch 57/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.2658 - accuracy: 0.5101 - val_loss: 1.1477 - val_accuracy: 0.5972\n",
            "Epoch 58/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.1901 - accuracy: 0.5763 - val_loss: 1.1283 - val_accuracy: 0.6250\n",
            "Epoch 59/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.1555 - accuracy: 0.5497 - val_loss: 1.1007 - val_accuracy: 0.6898\n",
            "Epoch 60/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.1712 - accuracy: 0.5426 - val_loss: 1.1233 - val_accuracy: 0.6435\n",
            "Epoch 61/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 1.1892 - accuracy: 0.5786 - val_loss: 1.0951 - val_accuracy: 0.5926\n",
            "Epoch 62/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.1246 - accuracy: 0.5793 - val_loss: 1.0590 - val_accuracy: 0.6898\n",
            "Epoch 63/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.1229 - accuracy: 0.5884 - val_loss: 1.0624 - val_accuracy: 0.6296\n",
            "Epoch 64/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.1102 - accuracy: 0.5755 - val_loss: 1.0792 - val_accuracy: 0.6667\n",
            "Epoch 65/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0803 - accuracy: 0.6176 - val_loss: 1.0077 - val_accuracy: 0.7130\n",
            "Epoch 66/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0342 - accuracy: 0.6447 - val_loss: 1.0228 - val_accuracy: 0.6991\n",
            "Epoch 67/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0478 - accuracy: 0.6109 - val_loss: 0.9987 - val_accuracy: 0.7037\n",
            "Epoch 68/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0926 - accuracy: 0.6119 - val_loss: 0.9838 - val_accuracy: 0.7315\n",
            "Epoch 69/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0635 - accuracy: 0.6124 - val_loss: 0.9598 - val_accuracy: 0.7176\n",
            "Epoch 70/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0500 - accuracy: 0.6052 - val_loss: 0.9610 - val_accuracy: 0.6991\n",
            "Epoch 71/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0192 - accuracy: 0.6175 - val_loss: 1.0305 - val_accuracy: 0.6250\n",
            "Epoch 72/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0481 - accuracy: 0.5801 - val_loss: 0.9264 - val_accuracy: 0.6944\n",
            "Epoch 73/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9954 - accuracy: 0.6045 - val_loss: 0.9265 - val_accuracy: 0.7083\n",
            "Epoch 74/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 1.0066 - accuracy: 0.6247 - val_loss: 0.9145 - val_accuracy: 0.7315\n",
            "Epoch 75/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9139 - accuracy: 0.6754 - val_loss: 0.8827 - val_accuracy: 0.7176\n",
            "Epoch 76/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9424 - accuracy: 0.6599 - val_loss: 0.8864 - val_accuracy: 0.7130\n",
            "Epoch 77/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9673 - accuracy: 0.6533 - val_loss: 0.8780 - val_accuracy: 0.7083\n",
            "Epoch 78/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9362 - accuracy: 0.6547 - val_loss: 0.8786 - val_accuracy: 0.6759\n",
            "Epoch 79/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9169 - accuracy: 0.6290 - val_loss: 0.8448 - val_accuracy: 0.7546\n",
            "Epoch 80/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9764 - accuracy: 0.6317 - val_loss: 0.8556 - val_accuracy: 0.7176\n",
            "Epoch 81/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8844 - accuracy: 0.7004 - val_loss: 0.8177 - val_accuracy: 0.7685\n",
            "Epoch 82/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.8897 - accuracy: 0.6727 - val_loss: 0.8522 - val_accuracy: 0.7407\n",
            "Epoch 83/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.9302 - accuracy: 0.6531 - val_loss: 0.8290 - val_accuracy: 0.7315\n",
            "Epoch 84/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8676 - accuracy: 0.6813 - val_loss: 0.8141 - val_accuracy: 0.7639\n",
            "Epoch 85/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8714 - accuracy: 0.6739 - val_loss: 0.7893 - val_accuracy: 0.8009\n",
            "Epoch 86/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.8973 - accuracy: 0.6745 - val_loss: 0.7936 - val_accuracy: 0.7824\n",
            "Epoch 87/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8550 - accuracy: 0.6867 - val_loss: 0.8104 - val_accuracy: 0.7269\n",
            "Epoch 88/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.8807 - accuracy: 0.6694 - val_loss: 0.8174 - val_accuracy: 0.7269\n",
            "Epoch 89/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8630 - accuracy: 0.6834 - val_loss: 0.7752 - val_accuracy: 0.7639\n",
            "Epoch 90/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.8535 - accuracy: 0.7005 - val_loss: 0.7659 - val_accuracy: 0.7731\n",
            "Epoch 91/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8665 - accuracy: 0.6916 - val_loss: 0.7637 - val_accuracy: 0.7778\n",
            "Epoch 92/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8299 - accuracy: 0.7063 - val_loss: 0.7422 - val_accuracy: 0.7824\n",
            "Epoch 93/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8070 - accuracy: 0.7225 - val_loss: 0.7386 - val_accuracy: 0.7917\n",
            "Epoch 94/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7827 - accuracy: 0.7112 - val_loss: 0.7051 - val_accuracy: 0.8056\n",
            "Epoch 95/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.7816 - accuracy: 0.7266 - val_loss: 0.7217 - val_accuracy: 0.7454\n",
            "Epoch 96/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.8039 - accuracy: 0.7385 - val_loss: 0.7049 - val_accuracy: 0.8056\n",
            "Epoch 97/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7578 - accuracy: 0.6981 - val_loss: 0.6939 - val_accuracy: 0.8194\n",
            "Epoch 98/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.7715 - accuracy: 0.7146 - val_loss: 0.6794 - val_accuracy: 0.8102\n",
            "Epoch 99/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7997 - accuracy: 0.6945 - val_loss: 0.7237 - val_accuracy: 0.7731\n",
            "Epoch 100/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7779 - accuracy: 0.7222 - val_loss: 0.6732 - val_accuracy: 0.8056\n",
            "Epoch 101/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7100 - accuracy: 0.7436 - val_loss: 0.6692 - val_accuracy: 0.8148\n",
            "Epoch 102/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7189 - accuracy: 0.7628 - val_loss: 0.6781 - val_accuracy: 0.8148\n",
            "Epoch 103/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7264 - accuracy: 0.7415 - val_loss: 0.6930 - val_accuracy: 0.7361\n",
            "Epoch 104/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7323 - accuracy: 0.7305 - val_loss: 0.6556 - val_accuracy: 0.8056\n",
            "Epoch 105/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7776 - accuracy: 0.7262 - val_loss: 0.6650 - val_accuracy: 0.8241\n",
            "Epoch 106/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7357 - accuracy: 0.7145 - val_loss: 0.6467 - val_accuracy: 0.8380\n",
            "Epoch 107/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7274 - accuracy: 0.7557 - val_loss: 0.6572 - val_accuracy: 0.7917\n",
            "Epoch 108/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6881 - accuracy: 0.7573 - val_loss: 0.6362 - val_accuracy: 0.8194\n",
            "Epoch 109/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6666 - accuracy: 0.7739 - val_loss: 0.6217 - val_accuracy: 0.8102\n",
            "Epoch 110/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7642 - accuracy: 0.7253 - val_loss: 0.6638 - val_accuracy: 0.8241\n",
            "Epoch 111/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7178 - accuracy: 0.7439 - val_loss: 0.6337 - val_accuracy: 0.8287\n",
            "Epoch 112/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6566 - accuracy: 0.7772 - val_loss: 0.6085 - val_accuracy: 0.8194\n",
            "Epoch 113/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6672 - accuracy: 0.7535 - val_loss: 0.5951 - val_accuracy: 0.8519\n",
            "Epoch 114/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7078 - accuracy: 0.7716 - val_loss: 0.6061 - val_accuracy: 0.8241\n",
            "Epoch 115/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6911 - accuracy: 0.7480 - val_loss: 0.5901 - val_accuracy: 0.8380\n",
            "Epoch 116/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7105 - accuracy: 0.7385 - val_loss: 0.5975 - val_accuracy: 0.8102\n",
            "Epoch 117/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6804 - accuracy: 0.7587 - val_loss: 0.5870 - val_accuracy: 0.8565\n",
            "Epoch 118/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7122 - accuracy: 0.7403 - val_loss: 0.6027 - val_accuracy: 0.8472\n",
            "Epoch 119/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.6525 - accuracy: 0.7678 - val_loss: 0.5604 - val_accuracy: 0.8426\n",
            "Epoch 120/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6387 - accuracy: 0.7833 - val_loss: 0.6159 - val_accuracy: 0.8241\n",
            "Epoch 121/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6634 - accuracy: 0.7714 - val_loss: 0.5790 - val_accuracy: 0.8565\n",
            "Epoch 122/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7011 - accuracy: 0.7476 - val_loss: 0.5795 - val_accuracy: 0.8333\n",
            "Epoch 123/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.6420 - accuracy: 0.7669 - val_loss: 0.5699 - val_accuracy: 0.8519\n",
            "Epoch 124/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.6505 - accuracy: 0.7728 - val_loss: 0.5505 - val_accuracy: 0.8565\n",
            "Epoch 125/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6434 - accuracy: 0.7836 - val_loss: 0.5614 - val_accuracy: 0.8426\n",
            "Epoch 126/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6258 - accuracy: 0.7832 - val_loss: 0.5696 - val_accuracy: 0.8565\n",
            "Epoch 127/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6906 - accuracy: 0.7610 - val_loss: 0.5495 - val_accuracy: 0.8657\n",
            "Epoch 128/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6306 - accuracy: 0.7648 - val_loss: 0.5452 - val_accuracy: 0.8426\n",
            "Epoch 129/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5946 - accuracy: 0.8084 - val_loss: 0.5745 - val_accuracy: 0.8519\n",
            "Epoch 130/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6299 - accuracy: 0.8088 - val_loss: 0.5394 - val_accuracy: 0.8565\n",
            "Epoch 131/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5951 - accuracy: 0.7832 - val_loss: 0.5373 - val_accuracy: 0.8519\n",
            "Epoch 132/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5879 - accuracy: 0.8099 - val_loss: 0.5455 - val_accuracy: 0.8519\n",
            "Epoch 133/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6362 - accuracy: 0.8088 - val_loss: 0.5336 - val_accuracy: 0.8611\n",
            "Epoch 134/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.6458 - accuracy: 0.7552 - val_loss: 0.5232 - val_accuracy: 0.8565\n",
            "Epoch 135/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5590 - accuracy: 0.8311 - val_loss: 0.5213 - val_accuracy: 0.8565\n",
            "Epoch 136/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5709 - accuracy: 0.7985 - val_loss: 0.5409 - val_accuracy: 0.8657\n",
            "Epoch 137/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5637 - accuracy: 0.8034 - val_loss: 0.5208 - val_accuracy: 0.8426\n",
            "Epoch 138/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5679 - accuracy: 0.8012 - val_loss: 0.5168 - val_accuracy: 0.8519\n",
            "Epoch 139/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5696 - accuracy: 0.7962 - val_loss: 0.5056 - val_accuracy: 0.8657\n",
            "Epoch 140/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5933 - accuracy: 0.8092 - val_loss: 0.5082 - val_accuracy: 0.8565\n",
            "Epoch 141/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5905 - accuracy: 0.7998 - val_loss: 0.5378 - val_accuracy: 0.8380\n",
            "Epoch 142/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6150 - accuracy: 0.7839 - val_loss: 0.4982 - val_accuracy: 0.8796\n",
            "Epoch 143/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5768 - accuracy: 0.8019 - val_loss: 0.5016 - val_accuracy: 0.8472\n",
            "Epoch 144/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5344 - accuracy: 0.8324 - val_loss: 0.4916 - val_accuracy: 0.8565\n",
            "Epoch 145/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6312 - accuracy: 0.7904 - val_loss: 0.4874 - val_accuracy: 0.8657\n",
            "Epoch 146/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5367 - accuracy: 0.7978 - val_loss: 0.4815 - val_accuracy: 0.8750\n",
            "Epoch 147/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5253 - accuracy: 0.8181 - val_loss: 0.4880 - val_accuracy: 0.8750\n",
            "Epoch 148/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5388 - accuracy: 0.8053 - val_loss: 0.5228 - val_accuracy: 0.8426\n",
            "Epoch 149/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5664 - accuracy: 0.8056 - val_loss: 0.4697 - val_accuracy: 0.8796\n",
            "Epoch 150/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5046 - accuracy: 0.8231 - val_loss: 0.4697 - val_accuracy: 0.8565\n",
            "Epoch 151/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5074 - accuracy: 0.8091 - val_loss: 0.4926 - val_accuracy: 0.8472\n",
            "Epoch 152/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.4812 - accuracy: 0.8394 - val_loss: 0.4581 - val_accuracy: 0.8657\n",
            "Epoch 153/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5440 - accuracy: 0.8039 - val_loss: 0.4805 - val_accuracy: 0.8843\n",
            "Epoch 154/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5024 - accuracy: 0.8198 - val_loss: 0.4593 - val_accuracy: 0.8611\n",
            "Epoch 155/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5107 - accuracy: 0.8253 - val_loss: 0.4567 - val_accuracy: 0.8750\n",
            "Epoch 156/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4806 - accuracy: 0.8381 - val_loss: 0.4469 - val_accuracy: 0.8657\n",
            "Epoch 157/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5182 - accuracy: 0.8283 - val_loss: 0.4576 - val_accuracy: 0.8380\n",
            "Epoch 158/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5443 - accuracy: 0.8041 - val_loss: 0.4677 - val_accuracy: 0.8750\n",
            "Epoch 159/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4792 - accuracy: 0.8415 - val_loss: 0.4422 - val_accuracy: 0.8843\n",
            "Epoch 160/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5089 - accuracy: 0.8475 - val_loss: 0.4710 - val_accuracy: 0.8657\n",
            "Epoch 161/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4909 - accuracy: 0.8367 - val_loss: 0.4405 - val_accuracy: 0.8843\n",
            "Epoch 162/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5229 - accuracy: 0.8364 - val_loss: 0.4666 - val_accuracy: 0.8889\n",
            "Epoch 163/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5103 - accuracy: 0.8239 - val_loss: 0.4442 - val_accuracy: 0.8657\n",
            "Epoch 164/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4675 - accuracy: 0.8618 - val_loss: 0.4374 - val_accuracy: 0.8657\n",
            "Epoch 165/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5387 - accuracy: 0.8144 - val_loss: 0.4366 - val_accuracy: 0.8843\n",
            "Epoch 166/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5790 - accuracy: 0.8179 - val_loss: 0.4527 - val_accuracy: 0.8611\n",
            "Epoch 167/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4381 - accuracy: 0.8654 - val_loss: 0.4252 - val_accuracy: 0.8750\n",
            "Epoch 168/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4563 - accuracy: 0.8521 - val_loss: 0.4300 - val_accuracy: 0.8796\n",
            "Epoch 169/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4611 - accuracy: 0.8564 - val_loss: 0.4397 - val_accuracy: 0.8796\n",
            "Epoch 170/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5528 - accuracy: 0.8076 - val_loss: 0.4254 - val_accuracy: 0.8796\n",
            "Epoch 171/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5128 - accuracy: 0.8333 - val_loss: 0.4285 - val_accuracy: 0.8889\n",
            "Epoch 172/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4829 - accuracy: 0.8190 - val_loss: 0.4281 - val_accuracy: 0.8935\n",
            "Epoch 173/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4266 - accuracy: 0.8475 - val_loss: 0.4232 - val_accuracy: 0.8796\n",
            "Epoch 174/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5060 - accuracy: 0.8078 - val_loss: 0.4254 - val_accuracy: 0.8843\n",
            "Epoch 175/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.4537 - accuracy: 0.8348 - val_loss: 0.4242 - val_accuracy: 0.8750\n",
            "Epoch 176/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4587 - accuracy: 0.8533 - val_loss: 0.4137 - val_accuracy: 0.8750\n",
            "Epoch 177/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4535 - accuracy: 0.8251 - val_loss: 0.4223 - val_accuracy: 0.8704\n",
            "Epoch 178/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4005 - accuracy: 0.8719 - val_loss: 0.3989 - val_accuracy: 0.8704\n",
            "Epoch 179/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4345 - accuracy: 0.8477 - val_loss: 0.4047 - val_accuracy: 0.8796\n",
            "Epoch 180/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4438 - accuracy: 0.8452 - val_loss: 0.4331 - val_accuracy: 0.8657\n",
            "Epoch 181/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3748 - accuracy: 0.8835 - val_loss: 0.4103 - val_accuracy: 0.8843\n",
            "Epoch 182/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4333 - accuracy: 0.8473 - val_loss: 0.4014 - val_accuracy: 0.8750\n",
            "Epoch 183/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4618 - accuracy: 0.8461 - val_loss: 0.4098 - val_accuracy: 0.8704\n",
            "Epoch 184/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5140 - accuracy: 0.8200 - val_loss: 0.3974 - val_accuracy: 0.8704\n",
            "Epoch 185/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4520 - accuracy: 0.8305 - val_loss: 0.3948 - val_accuracy: 0.8796\n",
            "Epoch 186/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4060 - accuracy: 0.8598 - val_loss: 0.4056 - val_accuracy: 0.8889\n",
            "Epoch 187/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4361 - accuracy: 0.8476 - val_loss: 0.4084 - val_accuracy: 0.8843\n",
            "Epoch 188/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4378 - accuracy: 0.8520 - val_loss: 0.4110 - val_accuracy: 0.8889\n",
            "Epoch 189/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3975 - accuracy: 0.8819 - val_loss: 0.4014 - val_accuracy: 0.8843\n",
            "Epoch 190/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3531 - accuracy: 0.8902 - val_loss: 0.4010 - val_accuracy: 0.8704\n",
            "Epoch 191/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4786 - accuracy: 0.8434 - val_loss: 0.3772 - val_accuracy: 0.8889\n",
            "Epoch 192/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4014 - accuracy: 0.8623 - val_loss: 0.4005 - val_accuracy: 0.8889\n",
            "Epoch 193/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.4178 - accuracy: 0.8542 - val_loss: 0.3904 - val_accuracy: 0.8889\n",
            "Epoch 194/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3586 - accuracy: 0.8850 - val_loss: 0.3764 - val_accuracy: 0.8889\n",
            "Epoch 195/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.4374 - accuracy: 0.8535 - val_loss: 0.4220 - val_accuracy: 0.8796\n",
            "Epoch 196/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4140 - accuracy: 0.8508 - val_loss: 0.3740 - val_accuracy: 0.8935\n",
            "Epoch 197/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4350 - accuracy: 0.8440 - val_loss: 0.4138 - val_accuracy: 0.8843\n",
            "Epoch 198/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.4094 - accuracy: 0.8580 - val_loss: 0.4065 - val_accuracy: 0.8843\n",
            "Epoch 199/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3975 - accuracy: 0.8686 - val_loss: 0.3823 - val_accuracy: 0.8935\n",
            "Epoch 200/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3827 - accuracy: 0.8701 - val_loss: 0.3800 - val_accuracy: 0.8935\n",
            "Epoch 201/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4665 - accuracy: 0.8177 - val_loss: 0.3775 - val_accuracy: 0.8796\n",
            "Epoch 202/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3418 - accuracy: 0.8999 - val_loss: 0.3954 - val_accuracy: 0.8796\n",
            "Epoch 203/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4114 - accuracy: 0.8552 - val_loss: 0.3849 - val_accuracy: 0.8981\n",
            "Epoch 204/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4140 - accuracy: 0.8576 - val_loss: 0.3699 - val_accuracy: 0.8889\n",
            "Epoch 205/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4078 - accuracy: 0.8553 - val_loss: 0.3968 - val_accuracy: 0.8935\n",
            "Epoch 206/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3909 - accuracy: 0.8674 - val_loss: 0.3689 - val_accuracy: 0.8981\n",
            "Epoch 207/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3530 - accuracy: 0.8892 - val_loss: 0.3661 - val_accuracy: 0.8889\n",
            "Epoch 208/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3752 - accuracy: 0.8776 - val_loss: 0.3947 - val_accuracy: 0.8981\n",
            "Epoch 209/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3831 - accuracy: 0.8693 - val_loss: 0.3856 - val_accuracy: 0.8981\n",
            "Epoch 210/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3838 - accuracy: 0.8721 - val_loss: 0.3855 - val_accuracy: 0.8843\n",
            "Epoch 211/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3480 - accuracy: 0.9045 - val_loss: 0.3760 - val_accuracy: 0.8889\n",
            "Epoch 212/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3580 - accuracy: 0.8826 - val_loss: 0.3756 - val_accuracy: 0.8843\n",
            "Epoch 213/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3992 - accuracy: 0.8751 - val_loss: 0.3605 - val_accuracy: 0.8889\n",
            "Epoch 214/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3643 - accuracy: 0.8838 - val_loss: 0.3475 - val_accuracy: 0.9167\n",
            "Epoch 215/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3810 - accuracy: 0.8538 - val_loss: 0.3528 - val_accuracy: 0.9074\n",
            "Epoch 216/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3760 - accuracy: 0.8641 - val_loss: 0.3498 - val_accuracy: 0.8981\n",
            "Epoch 217/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3700 - accuracy: 0.8899 - val_loss: 0.3470 - val_accuracy: 0.8889\n",
            "Epoch 218/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3975 - accuracy: 0.8579 - val_loss: 0.3694 - val_accuracy: 0.9120\n",
            "Epoch 219/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3851 - accuracy: 0.8657 - val_loss: 0.3602 - val_accuracy: 0.9028\n",
            "Epoch 220/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3515 - accuracy: 0.8812 - val_loss: 0.3384 - val_accuracy: 0.9120\n",
            "Epoch 221/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3975 - accuracy: 0.8628 - val_loss: 0.3437 - val_accuracy: 0.9074\n",
            "Epoch 222/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3329 - accuracy: 0.8937 - val_loss: 0.3457 - val_accuracy: 0.9028\n",
            "Epoch 223/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2863 - accuracy: 0.9111 - val_loss: 0.3425 - val_accuracy: 0.9028\n",
            "Epoch 224/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3483 - accuracy: 0.8878 - val_loss: 0.3627 - val_accuracy: 0.8935\n",
            "Epoch 225/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3419 - accuracy: 0.8956 - val_loss: 0.3360 - val_accuracy: 0.9074\n",
            "Epoch 226/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3275 - accuracy: 0.8965 - val_loss: 0.3348 - val_accuracy: 0.9120\n",
            "Epoch 227/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3364 - accuracy: 0.8979 - val_loss: 0.3385 - val_accuracy: 0.9120\n",
            "Epoch 228/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3961 - accuracy: 0.8728 - val_loss: 0.3391 - val_accuracy: 0.9028\n",
            "Epoch 229/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3658 - accuracy: 0.8672 - val_loss: 0.3517 - val_accuracy: 0.9028\n",
            "Epoch 230/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3544 - accuracy: 0.8953 - val_loss: 0.3499 - val_accuracy: 0.9074\n",
            "Epoch 231/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3696 - accuracy: 0.8850 - val_loss: 0.3249 - val_accuracy: 0.9167\n",
            "Epoch 232/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3133 - accuracy: 0.9056 - val_loss: 0.3697 - val_accuracy: 0.9074\n",
            "Epoch 233/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3494 - accuracy: 0.8823 - val_loss: 0.3406 - val_accuracy: 0.9120\n",
            "Epoch 234/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2986 - accuracy: 0.9132 - val_loss: 0.3310 - val_accuracy: 0.9074\n",
            "Epoch 235/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3548 - accuracy: 0.8810 - val_loss: 0.3516 - val_accuracy: 0.9028\n",
            "Epoch 236/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3256 - accuracy: 0.8758 - val_loss: 0.3329 - val_accuracy: 0.9120\n",
            "Epoch 237/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3194 - accuracy: 0.9073 - val_loss: 0.3210 - val_accuracy: 0.9028\n",
            "Epoch 238/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3466 - accuracy: 0.8767 - val_loss: 0.3324 - val_accuracy: 0.9167\n",
            "Epoch 239/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2916 - accuracy: 0.8991 - val_loss: 0.3222 - val_accuracy: 0.9074\n",
            "Epoch 240/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2881 - accuracy: 0.9153 - val_loss: 0.3406 - val_accuracy: 0.9120\n",
            "Epoch 241/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3209 - accuracy: 0.8975 - val_loss: 0.3176 - val_accuracy: 0.9120\n",
            "Epoch 242/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2985 - accuracy: 0.9085 - val_loss: 0.3411 - val_accuracy: 0.8935\n",
            "Epoch 243/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2855 - accuracy: 0.9114 - val_loss: 0.3579 - val_accuracy: 0.8981\n",
            "Epoch 244/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3456 - accuracy: 0.8841 - val_loss: 0.3315 - val_accuracy: 0.9120\n",
            "Epoch 245/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2959 - accuracy: 0.9174 - val_loss: 0.3337 - val_accuracy: 0.9120\n",
            "Epoch 246/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3120 - accuracy: 0.8976 - val_loss: 0.3278 - val_accuracy: 0.9120\n",
            "Epoch 247/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2804 - accuracy: 0.9128 - val_loss: 0.3440 - val_accuracy: 0.9074\n",
            "Epoch 248/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3245 - accuracy: 0.8866 - val_loss: 0.3305 - val_accuracy: 0.9120\n",
            "Epoch 249/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2825 - accuracy: 0.9158 - val_loss: 0.3167 - val_accuracy: 0.9074\n",
            "Epoch 250/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3035 - accuracy: 0.8854 - val_loss: 0.3298 - val_accuracy: 0.9167\n",
            "Epoch 251/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3211 - accuracy: 0.8948 - val_loss: 0.3340 - val_accuracy: 0.9028\n",
            "Epoch 252/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3179 - accuracy: 0.8764 - val_loss: 0.3252 - val_accuracy: 0.9167\n",
            "Epoch 253/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2927 - accuracy: 0.9061 - val_loss: 0.3228 - val_accuracy: 0.9213\n",
            "Epoch 254/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2875 - accuracy: 0.8916 - val_loss: 0.3410 - val_accuracy: 0.9074\n",
            "Epoch 255/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2749 - accuracy: 0.9221 - val_loss: 0.3225 - val_accuracy: 0.9167\n",
            "Epoch 256/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2909 - accuracy: 0.9050 - val_loss: 0.3342 - val_accuracy: 0.9167\n",
            "Epoch 257/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3167 - accuracy: 0.9037 - val_loss: 0.3294 - val_accuracy: 0.9074\n",
            "Epoch 258/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2752 - accuracy: 0.9115 - val_loss: 0.3164 - val_accuracy: 0.9074\n",
            "Epoch 259/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2884 - accuracy: 0.9049 - val_loss: 0.3070 - val_accuracy: 0.9167\n",
            "Epoch 260/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2803 - accuracy: 0.9254 - val_loss: 0.3275 - val_accuracy: 0.9306\n",
            "Epoch 261/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2435 - accuracy: 0.9255 - val_loss: 0.3268 - val_accuracy: 0.9167\n",
            "Epoch 262/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2936 - accuracy: 0.9096 - val_loss: 0.3100 - val_accuracy: 0.9213\n",
            "Epoch 263/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2962 - accuracy: 0.8962 - val_loss: 0.3120 - val_accuracy: 0.9213\n",
            "Epoch 264/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3084 - accuracy: 0.8945 - val_loss: 0.3393 - val_accuracy: 0.8981\n",
            "Epoch 265/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2614 - accuracy: 0.9196 - val_loss: 0.3193 - val_accuracy: 0.9028\n",
            "Epoch 266/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2812 - accuracy: 0.9082 - val_loss: 0.3000 - val_accuracy: 0.9213\n",
            "Epoch 267/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2545 - accuracy: 0.9166 - val_loss: 0.3063 - val_accuracy: 0.9120\n",
            "Epoch 268/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2859 - accuracy: 0.9073 - val_loss: 0.3206 - val_accuracy: 0.9167\n",
            "Epoch 269/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2950 - accuracy: 0.8870 - val_loss: 0.3176 - val_accuracy: 0.9167\n",
            "Epoch 270/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3538 - accuracy: 0.8825 - val_loss: 0.3310 - val_accuracy: 0.9167\n",
            "Epoch 271/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2752 - accuracy: 0.9035 - val_loss: 0.3011 - val_accuracy: 0.9120\n",
            "Epoch 272/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2857 - accuracy: 0.8843 - val_loss: 0.3137 - val_accuracy: 0.9074\n",
            "Epoch 273/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2521 - accuracy: 0.9123 - val_loss: 0.3280 - val_accuracy: 0.9120\n",
            "Epoch 274/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2904 - accuracy: 0.9058 - val_loss: 0.2943 - val_accuracy: 0.9213\n",
            "Epoch 275/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2734 - accuracy: 0.9060 - val_loss: 0.2921 - val_accuracy: 0.9074\n",
            "Epoch 276/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2636 - accuracy: 0.9204 - val_loss: 0.2927 - val_accuracy: 0.9167\n",
            "Epoch 277/1000\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.2963 - accuracy: 0.8922 - val_loss: 0.3014 - val_accuracy: 0.9167\n",
            "Epoch 278/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3108 - accuracy: 0.9007 - val_loss: 0.3145 - val_accuracy: 0.9120\n",
            "Epoch 279/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2378 - accuracy: 0.9268 - val_loss: 0.3264 - val_accuracy: 0.9167\n",
            "Epoch 280/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2835 - accuracy: 0.9141 - val_loss: 0.3265 - val_accuracy: 0.9028\n",
            "Epoch 281/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2484 - accuracy: 0.9141 - val_loss: 0.2842 - val_accuracy: 0.9074\n",
            "Epoch 282/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2497 - accuracy: 0.9228 - val_loss: 0.3106 - val_accuracy: 0.9259\n",
            "Epoch 283/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2878 - accuracy: 0.9060 - val_loss: 0.3158 - val_accuracy: 0.9074\n",
            "Epoch 284/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2584 - accuracy: 0.9181 - val_loss: 0.2970 - val_accuracy: 0.9074\n",
            "Epoch 285/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2670 - accuracy: 0.9114 - val_loss: 0.2850 - val_accuracy: 0.9213\n",
            "Epoch 286/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2743 - accuracy: 0.9131 - val_loss: 0.2894 - val_accuracy: 0.9120\n",
            "Epoch 287/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2338 - accuracy: 0.9349 - val_loss: 0.3012 - val_accuracy: 0.9213\n",
            "Epoch 288/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2993 - accuracy: 0.9030 - val_loss: 0.2827 - val_accuracy: 0.9167\n",
            "Epoch 289/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2296 - accuracy: 0.9222 - val_loss: 0.2920 - val_accuracy: 0.9167\n",
            "Epoch 290/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2902 - accuracy: 0.8990 - val_loss: 0.3253 - val_accuracy: 0.9028\n",
            "Epoch 291/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2369 - accuracy: 0.9349 - val_loss: 0.2804 - val_accuracy: 0.9213\n",
            "Epoch 292/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2598 - accuracy: 0.9322 - val_loss: 0.3056 - val_accuracy: 0.9167\n",
            "Epoch 293/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2585 - accuracy: 0.9173 - val_loss: 0.2821 - val_accuracy: 0.9120\n",
            "Epoch 294/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2046 - accuracy: 0.9359 - val_loss: 0.2798 - val_accuracy: 0.9213\n",
            "Epoch 295/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2875 - accuracy: 0.9067 - val_loss: 0.3546 - val_accuracy: 0.8981\n",
            "Epoch 296/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2465 - accuracy: 0.9385 - val_loss: 0.2833 - val_accuracy: 0.9213\n",
            "Epoch 297/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2404 - accuracy: 0.9308 - val_loss: 0.2935 - val_accuracy: 0.9167\n",
            "Epoch 298/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2477 - accuracy: 0.9231 - val_loss: 0.3026 - val_accuracy: 0.9074\n",
            "Epoch 299/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2755 - accuracy: 0.8991 - val_loss: 0.3200 - val_accuracy: 0.9167\n",
            "Epoch 300/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2647 - accuracy: 0.9155 - val_loss: 0.3070 - val_accuracy: 0.9120\n",
            "Epoch 301/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2321 - accuracy: 0.9237 - val_loss: 0.2969 - val_accuracy: 0.9120\n",
            "Epoch 302/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2398 - accuracy: 0.9229 - val_loss: 0.2878 - val_accuracy: 0.9167\n",
            "Epoch 303/1000\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.2481 - accuracy: 0.9225 - val_loss: 0.2988 - val_accuracy: 0.8981\n",
            "Epoch 304/1000\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.2273 - accuracy: 0.9206 - val_loss: 0.2979 - val_accuracy: 0.9167\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2531 - accuracy: 0.9250\n",
            "CNN básica:  [0.253143846988678, 0.925000011920929]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}